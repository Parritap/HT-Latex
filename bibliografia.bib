#################################################################
#######################  GLOSARIO #############################
#################################################################

@misc{HTCondor,
	title        = {HTCondor Overview},
	author       = {{HTCondor Team}},
	organization = {HTCondor},
	year         = {s.f},
	url          = {https://htcondor.org/htcondor/overview/},
	note         = {Retrieved September 29, 2024}
}

@misc{AWS01,
	author       = {{Amazon Web Services, Inc.}},
	title        = {What is Distributed Computing?},
	howpublished = {\url{https://aws.amazon.com/what-is/distributed-computing/}},
	note         = {Accessed: 2024-09-28},
	year         = {s.f.}
}

@misc{HTCondor-what-is-HTCondor,
	author       = {{HTCondor Team}},
	title        = {{W}hat is {H}{T}{C}ondor? --- htcondor.org},
	howpublished = {\url{https://htcondor.org/description.html}},
	year         = {s.f.},
	note         = {[Accessed 28-09-2025]}
}

@article{Manzoor2020,
	title    = {Resource Allocation Techniques in Cloud Computing: A Review and Future Directions},
	author   = {Manzoor, Shahida and Ullah, Zahid and Jeon, Gwanggil},
	journal  = {Elektronika ir Elektrotechnika},
	volume   = {26},
	number   = {6},
	pages    = {40--51},
	year     = {2020},
	month    = {December},
	doi      = {10.5755/j01.eie.26.6.25865},
	url      = {https://eejournal.ktu.lt/index.php/elt/article/view/25865},
	abstract = {Cloud computing has become a very important computing model to process data and execute computationally concentrated applications in pay-per-use method. Resource allocation is a process in which the resources are allocated to consumers by cloud providers based on their flexible requirements. As the data is expanding every day, allocating resources efficiently according to the consumer demand has also become very important, keeping Service Level Agreement (SLA) between service providers and consumers in prospect. This task of resource allocation becomes more challenging due to finite available resources and increasing consumer demands. Therefore, many unique models and techniques have been proposed to allocate resources efficiently. In the light of the uniqueness of the models and techniques, the main aim of the resource allocation is to limit the overhead/expenses associated with it. This research aims to present a comprehensive, structured literature review on different aspects of resource allocation in cloud computing, including strategic, target resources, optimization, scheduling and power. More than 50 articles, between year 2007 and 2019, related to resource allocation in cloud computing have been shortlisted through a structured mechanism and they are reviewed under clearly defined objectives. It presents a topical taxonomy of resource allocation dimensions, and articles under each category are discussed and analysed. Lastly, salient future directions in this area are discussed.}
}


@article{Oldham1995,
	author   = {Chang, H.W.D. and Oldham, W.J.B.},
	journal  = {IEEE Transactions on Parallel and Distributed Systems},
	title    = {Dynamic task allocation models for large distributed computing systems},
	year     = {1995},
	volume   = {6},
	number   = {12},
	pages    = {1301-1315},
	keywords = {Distributed computing;Distributed control;Simulated annealing;Application software;Resource management;Hardware;Computer architecture;Transport protocols;Throughput;Computational modeling},
	doi      = {10.1109/71.476170}
}



@article{Thain2005,
	author     = {Thain, Douglas and Tannenbaum, Todd and Livny, Miron},
	title      = {Distributed computing in practice: the Condor experience: Research Articles},
	year       = {2005},
	issue_date = {February 2005},
	publisher  = {John Wiley and Sons Ltd.},
	address    = {GBR},
	volume     = {17},
	number     = {2–4},
	issn       = {1532-0626},
	abstract   = {Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational Grid. In this paper, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course travelled by research ideas as they grow into production systems. Copyright © 2005 John Wiley \& Sons, Ltd.},
	journal    = {Concurr. Comput.: Pract. Exper.},
	month      = feb,
	pages      = {323–356},
	numpages   = {34},
	keywords   = {split execution, scheduling, planning, history, community, Grid, Condor}
}


@inproceedings{Raman1998,
	author    = {Raman, R. and Livny, M. and Solomon, M.},
	booktitle = {Proceedings. The Seventh International Symposium on High Performance Distributed Computing (Cat. No.98TB100244)},
	title     = {Matchmaking: distributed resource management for high throughput computing},
	year      = {1998},
	volume    = {},
	number    = {},
	pages     = {140-146},
	keywords  = {Resource management;Throughput;Robustness;Processor scheduling;Centralized control;Distributed computing;Data models;Specification languages;Protocols;Computer architecture},
	doi       = {10.1109/HPDC.1998.709966}
}



@book{Landau2005,
	isbn      = {9780691121833},
	url       = {http://www.jstor.org/stable/j.ctvcm4grd},
	author    = {RUBIN H. Landu and Robyn Wangberg and Kyle Augustson and M. J. Páez and C. C. Bordeianu and C. Barnes},
	publisher = {Princeton University Press},
	title     = {A First Course in Scientific Computing: Symbolic, Graphic, and Numeric Modeling Using Maple, Java, Mathematica, and Fortran90},
	urldate   = {2025-07-29},
	year      = {2005}
}



@incollection{Lamport1990,
	title     = {CHAPTER 18 - Distributed Computing: Models and Methods},
	editor    = {JAN {VAN LEEUWEN}},
	booktitle = {Formal Models and Semantics},
	publisher = {Elsevier},
	address   = {Amsterdam},
	pages     = {1157-1199},
	year      = {1990},
	series    = {Handbook of Theoretical Computer Science},
	isbn      = {978-0-444-88074-1},
	doi       = {https://doi.org/10.1016/B978-0-444-88074-1.50023-8},
	url       = {https://www.sciencedirect.com/science/article/pii/B9780444880741500238},
	author    = {Leslie Lamport and Nancy Lynch},
	abstract  = {Publisher Summary
			Distributed computing is an activity that is performed on a spatially distributed system. An important problem in distributed computing is to provide a user with a non-distributed view of a distributed system to implement a distributed file system that allows the client programmer to ignore the physical location of his data. The models of computation generally considered to be distributed are process models in which the computational activity is represented as the concurrent execution of sequential processes. Different process models are distinguished by the mechanism employed for inter-process communication. The process models that are most distributed are the ones in which processes communicate by message passing. A process sends a message by adding it to a message queue and another process receives the message by removing it from the queue. There are two basic complexity measures for distributed algorithms: time and message complexity. The time complexity of an algorithm measures the time needed both for message transmission and for computation within the processes. The most common measure of message complexity is the total number of messages transmitted. If messages contain on the order of a few hundred bits or more, then the total number of bits sent might be a better measure of the cost than the number of messages.}
}


@inproceedings{Juve2015,
	author    = {Juve, Gideon and Tovar, Benjamin and Da Silva, Rafael Ferreira and Krol, Dariusz and Thain, Douglas and Deelman, Ewa and Allcock, William and Livny, Miron},
	booktitle = {2015 IEEE International Conference on Cluster Computing},
	title     = {Practical Resource Monitoring for Robust High Throughput Computing},
	year      = {2015},
	volume    = {},
	number    = {},
	pages     = {650-657},
	keywords  = {Monitoring;Libraries;Linux;Kernel;Probes;Radiation detectors;High-Throughput Computing;Profiling;Monitoring},
	doi       = {10.1109/CLUSTER.2015.115}
}


@incollection{Morgan2009,
	title     = {Chapter 8 - High-Throughput Computing in the Sciences},
	editor    = {Michael L. Johnson and Ludwig Brand},
	series    = {Methods in Enzymology},
	publisher = {Academic Press},
	volume    = {467},
	pages     = {197-227},
	year      = {2009},
	booktitle = {Computer Methods Part B},
	issn      = {0076-6879},
	doi       = {https://doi.org/10.1016/S0076-6879(09)67008-7},
	url       = {https://www.sciencedirect.com/science/article/pii/S0076687909670087},
	author    = {Mark Morgan and Andrew Grimshaw},
	abstract  = {While it is true that the modern computer is many orders of magnitude faster than that of yesteryear; this tremendous growth in CPU clock rates is now over. Unfortunately, however, the growth in demand for computational power has not abated; whereas researchers a decade ago could simply wait for computers to get faster, today the only solution to the growing need for more powerful computational resource lies in the exploitation of parallelism. Software parallelization falls generally into two broad categories—“true parallel” and high-throughput computing. This chapter focuses on the latter of these two types of parallelism. With high-throughput computing, users can run many copies of their software at the same time across many different computers. This technique for achieving parallelism is powerful in its ability to provide high degrees of parallelism, yet simple in its conceptual implementation. This chapter covers various patterns of high-throughput computing usage and the skills and techniques necessary to take full advantage of them. By utilizing numerous examples and sample codes and scripts, we hope to provide the reader not only with a deeper understanding of the principles behind high-throughput computing, but also with a set of tools and references that will prove invaluable as she explores software parallelism with her own software applications and research.}
}


@article{SK2023,
	title   = {Exploring the advancements in high-performance computing paradigm for remote sensing big data analytics},
	author  = {Sudha, SK and Aji, S},
	journal = {Cloud Computing and Data Science},
	pages   = {50--61},
	year    = {2024}
}

@incollection{Salama2017,
	title     = {Chapter 11 - Managing Trade-offs in Self-Adaptive Software Architectures: A Systematic Mapping Study},
	editor    = {Ivan Mistrik and Nour Ali and Rick Kazman and John Grundy and Bradley Schmerl},
	booktitle = {Managing Trade-Offs in Adaptable Software Architectures},
	publisher = {Morgan Kaufmann},
	address   = {Boston},
	pages     = {249-297},
	year      = {2017},
	isbn      = {978-0-12-802855-1},
	doi       = {https://doi.org/10.1016/B978-0-12-802855-1.00011-3},
	url       = {https://www.sciencedirect.com/science/article/pii/B9780128028551000113},
	author    = {M. Salama and R. Bahsoon and N. Bencomo},
	keywords  = {Self-adaptation, Self-adaptive architecture, Software architecture, Trade-offs management, Systematic mapping study, Self-awareness, Long-living software},
	abstract  = {Self-adaptation has been driven by the need to achieve and maintain quality attributes in the face of the continuously changing requirements, as well as the uncertain demand during run-time. Designing architectures that exhibit a good trade-off between multiple quality attributes is challenging, especially in the case of self-adaptive software systems, due to the complexity, heterogeneity, and ultra-large scale of modern software systems. This challenge increases with the dynamic, open, and uncertain operating environment, as well as the need for complying to environmental, regulatory, and sustainability requirements; such as energy consumption regulations. This study aims at analyzing the research landscape that have explicitly addressed trade-offs management for self-adaptive software architectures, to obtain a comprehensive overview on the current state of research on this specialized area. A systematic mapping study was conducted to identify and analyze research works related to analyzing and managing trade-offs to support decision-making for self-adaptive software architectures. Twenty primary studies were evidently selected and analyzed to classify software paradigms, quality attributes considered, and the self-* properties that drive trade-offs management. The results show constant interest in finding solutions for trade-offs management at design-time and run-time, as well as the success of research initiatives even when new research challenges are found. The findings call for foundational framework to analyze and manage trade-offs for self-adaptive software architectures that can explicitly consider specific multiple quality attributes, the run-time dynamics, the uncertainty of the environment and the complex challenges of modern, ultra-large scale systems in particular given software paradigms.}
}


@book{Tanenbaum2015,
	title     = {Modern Operating Systems},
	author    = {Tanenbaum, Andrew S. and Bos, Herbert},
	edition   = {4},
	year      = {2015},
	publisher = {Pearson},
	address   = {Boston},
	isbn      = {978-0133591620}
}



@article{PerezMiguel2013,
	title    = {High throughput computing over peer-to-peer networks},
	journal  = {Future Generation Computer Systems},
	volume   = {29},
	number   = {1},
	pages    = {352-360},
	year     = {2013},
	note     = {Including Special section: AIRCC-NetCoM 2009 and Special section: Clouds and Service-Oriented Architectures},
	issn     = {0167-739X},
	doi      = {https://doi.org/10.1016/j.future.2011.08.011},
	url      = {https://www.sciencedirect.com/science/article/pii/S0167739X11001506},
	author   = {Carlos Pérez-Miguel and Jose Miguel-Alonso and Alexander Mendiburu},
	keywords = {Peer-to-peer networks, High throughput computing, Distributed hash tables},
	abstract = {In this work, we present a proposal to build a high throughput computing system totally based upon the Peer-to-Peer (P2P) paradigm. We discuss the general characteristics of P2P systems, with focus on P2P storage, and the expected characteristics of the HTC system: totally decentralized, not requiring permanent connection, and able to implement scheduling policies such as running jobs in a (non-strict) FCFS order. We have selected Cassandra as the supporting P2P storage system for our purposes. We discuss the basic aspects of the system implementation, and carry out some experiments designed to verify that it works as expected.}
}

@inbook{Nielsen2016,
	author = {Nielsen, Frank},
	year = {2016},
	month = {02},
	pages = {21-62},
	title = {Introduction to MPI: The Message Passing Interface},
	isbn = {978-3-319-21902-8},
	doi = {10.1007/978-3-319-21903-5_2}
}


@inproceedings{Wilson2016,
	author = {Wilson, Lucas A. and Dey, S. Charlie},
	title = {Computational science education focused on future domain scientists},
	year = {2016},
	isbn = {9781509038275},
	publisher = {IEEE Press},
	abstract = {The majority of university courses which educate students in high performance, parallel, and distributed computing are located within computer science departments. This can potentially be a hurdle to students from other disciplines who need to acquire these critical skills.We discuss a sequence of application-driven courses designed to educate undergraduate and graduate students who do not necessarily have a computer science background on developing scientific research software, with an emphasis on using high performance, parallel, and distributed computational systems.},
	booktitle = {Proceedings of the Workshop on Education for High Performance Computing},
	pages = {19–24},
	numpages = {6},
	location = {Salt Lake City, Utah},
	series = {EduHPC '16}
}

#################################################################
#######################JUSTIFICACION ############################
#################################################################

@inproceedings{Bianchi2013,
	author    = {Bianchi, Oscar Martín and Ariznabarreta Fossati, José Ignacio and Repetto, Alejandro Juan Manuel},
	title     = {Construyendo un sistema de cómputo distribuido multipropósito},
	booktitle = {XV Workshop de Investigadores en Ciencias de la Computación},
	year      = {2013},
	month     = jun,
	pages     = {652--657},
	address   = {Red de Universidades con Carreras en Informática (RedUNCI)},
	note      = {Exposición: abril 2013},
	language  = {Spanish},
	url       = {http://sedici.unlp.edu.ar/handle/10915/27283}
}


@article{Tsai2015,
	author   = {Tsai, Chun-Wei
			and Lai, Chin-Feng
			and Chao, Han-Chieh
			and Vasilakos, Athanasios V.},
	title    = {Big data analytics: a survey},
	journal  = {Journal of Big Data},
	year     = {2015},
	month    = {Oct},
	day      = {01},
	volume   = {2},
	number   = {1},
	pages    = {21},
	abstract = {The age of big data is now coming. But the traditional data analytics may not be able to handle such large quantities of data. The question that arises now is, how to develop a high performance platform to efficiently analyze big data and how to design an appropriate mining algorithm to find the useful things from big data. To deeply discuss this issue, this paper begins with a brief introduction to data analytics, followed by the discussions of big data analytics. Some important open issues and further research directions will also be presented for the next step of big data analytics.},
	issn     = {2196-1115},
	doi      = {10.1186/s40537-015-0030-3},
	url      = {https://doi.org/10.1186/s40537-015-0030-3}
}


@inproceedings{Thomson2023,
	author       = {Thompson, Neil and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
	booktitle    = {Ninth {Computing} within {Limits} 2023},
	year         = {2023},
	month        = {jun 14},
	note         = {https://limits.pubpub.org/pub/wm1lwjce},
	organization = {LIMITS},
	title        = {The {Computational} {Limits} of {Deep} {Learning}}
}


@manual{HTCondor-what-is-htc,
	title        = {High-Throughput Computing (HTC) and its Requirements},
	author       = {{HTCondor Team }},
	organization = {HTCondor Project},
	url          = {https://htcondor.readthedocs.io/en/latest/overview/high-throughput-computing-requirements.html},
	year         = {2025},
	month        = {9},
	note         = {HTCondor Manual, accessed 2025-09-28},
	version      = {24.12.4}
}



#################################################################
W####################### MARCO CONCEPTUAL ########################
#################################################################

@article{Ali2015,
	title     = {Distributed computing: An overview},
	author    = {Ali, Md Firoj and Khan, Rafiqul Zaman},
	journal   = {International Journal of Advanced Networking and Applications},
	volume    = {7},
	number    = {1},
	pages     = {2630},
	year      = {2015},
	publisher = {Eswar Publications}
}


@article{Chang1995,
	author   = {Chang, H.W.D. and Oldham, W.J.B.},
	journal  = {IEEE Transactions on Parallel and Distributed Systems},
	title    = {Dynamic task allocation models for large distributed computing systems},
	year     = {1995},
	volume   = {6},
	number   = {12},
	pages    = {1301-1315},
	keywords = {Distributed computing;Distributed control;Simulated annealing;Application software;Resource management;Hardware;Computer architecture;Transport protocols;Throughput;Computational modeling},
	doi      = {10.1109/71.476170}
}

@misc{HTCondor-what-is-a-job,
	author       = {{HTCondor Team}},
	title        = {{HTCondor User Manual: Quick Start Guide -- What Is a Job?}},
	howpublished = {\url{https://htcondor.readthedocs.io/en/24.x/users-manual/quick-start-guide.html#what-is-a-job}},
	year         = {s.f.},
	note         = {[Accessed 28-09-2025]}
}

@misc{HTCondor-choosing-universe,
	title = {Choosing an {HTCondor} Universe},
	author = {{HTCondor Team}},
	howpublished = {HTCondor Manual},
	year = {2025},
	url = {https://htcondor.readthedocs.io/en/lts/users-manual/choosing-an-htcondor-universe.html},
	note = {Accedido el 2025-10-02},
	urldate = {2025-10-02}
}









#################################################################
####################### MARCO DE TEORICO ########################
#################################################################
@book{PMI2019,
	title     = {Guía de los fundamentos para la dirección de proyectos (Guía del PMBOK)},
	author    = {{Project Management Institute}},
	year      = {2019},
	publisher = {Project Management Institute},
	edition   = {6},
	address   = {Newtown Square, PA},
	url       = {https://books.google.com.co/books?id=MNRHAQAACAAJ},
	isbn      = {978-1628255848}
}


@misc{Spray2023,
	author       = {Spray, J. R.},
	title        = {Abstraction Layered Architecture},
	year         = {2023},
	howpublished = {\url{http://abstractionlayeredarchitecture.com/}},
	note         = {Accessed: 2025-09-25}
}


@inproceedings{Mumtaza2025,
	author    = {Mumtaza, Farisa Fikri and Mulyana, Rahmat and Mukti, Iqbal Yulizar},
	booktitle = {2025 International Conference on Advancement in Data Science, E-learning and Information System (ICADEIS)},
	title     = {Utilizing TOGAF 10 to Design an Enterprise Architecture for BPRBCo SME Digital Transformation},
	year      = {2025},
	volume    = {},
	number    = {},
	pages     = {1-7},
	keywords  = {Digital transformation;Standards organizations;Merging;Banking;Organizations;Planning;Interviews;Faces;Investment;Information systems;BPR;digital transformation;enterprise architecture;SMEs;TOGAF},
	doi       = {10.1109/ICADEIS65852.2025.10933402}
}

@misc{ISO25010,
	title        = {{ISO/IEC} 25010:2011, Systems and software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — System and software quality models},
	author       = {{International Organization for Standardization}},
	year         = {2011},
	howpublished = {Standard},
	address      = {Geneva, Switzerland}
}

% RAMAN 2018 is already in Glossary section.


#################################################################
####################### #########################################
#################################################################



@misc{SMSBuilder2020,
	author       = {Candela-Uribe, C.A. and Sepúlveda-Rodríguez, L.E. and Chavarro-Porras, J.C. and Sanabria-Ordoñez, J.A. and Garrido, J.L. and Rodríguez-Domínguez, C. and Guerrero-Contreras, G.},
	title        = {SMS-Builder Project},
	year         = {2020},
	howpublished = {\url{https://github.com/grid-uq/sms-builder}},
	note         = {Accessed: 2025-08-11}
}

@misc{sms-builder-own-container,
author = {Candela-Uribe, C.A. and J, L.E. and Chavarro-Porras, J.C. and Parra Parra, J.E. and Casta\~{n}o Osma, J.E.},
title = {SMS-builder project},
year = {2025},
url = {https://hub.docker.com/r/parritap/sms-htcondor-universes},
}


%########################## INTRO  ####################################


@book{landau01,
	isbn      = {9780691121833},
	url       = {http://www.jstor.org/stable/j.ctvcm4grd},
	author    = {Rubin H. Landu and Robyn Wangberg and Kyle Augustson and M. J. Páez and C. C. Bordeianu and C. Barnes},
	publisher = {Princeton University Press},
	title     = {A First Course in Scientific Computing: Symbolic, Graphic, and Numeric Modeling Using Maple, Java, Mathematica, and Fortran90},
	urldate   = {2025-07-29},
	year      = {2005}
}


@inproceedings{juve-01,
	author    = {Juve, Gideon and Tovar, Benjamin and Da Silva, Rafael Ferreira and Krol, Dariusz and Thain, Douglas and Deelman, Ewa and Allcock, William and Livny, Miron},
	booktitle = {2015 IEEE International Conference on Cluster Computing},
	title     = {Practical Resource Monitoring for Robust High Throughput Computing},
	year      = {2015},
	volume    = {},
	number    = {},
	pages     = {650-657},
	keywords  = {Monitoring;Libraries;Linux;Kernel;Probes;Radiation detectors;High-Throughput Computing;Profiling;Monitoring},
	doi       = {10.1109/CLUSTER.2015.115}
}


@article{Senol-01,
	title    = {The impact of distributed computing on education},
	journal  = {Computers \& Structures},
	volume   = {15},
	number   = {2},
	pages    = {149-156},
	year     = {1982},
	issn     = {0045-7949},
	doi      = {https://doi.org/10.1016/0045-7949(82)90062-1},
	url      = {https://www.sciencedirect.com/science/article/pii/0045794982900621},
	author   = {Senol Utku and Joseph Lestingi and Moktar Salama},
	abstract = {In this paper, developments in digital computer technology since the early Fifties are reviewed briefly, and the parallelism which exists between these developments and developments in analysis and design procedures of structural engineering is identified. The recent trends in digital computer technology are examined in order to establish the fact that distributed processing is now an accepted philosophy for further developments. The impact of this on the analysis and design practices of structural engineering is assessed by first examining these practices from a data processing standpoint to identify the key operations and data bases, and then fitting them to the characteristics of distributed processing. The merits and drawbacks of the present philosophy in educating structural engineers are discussed and projections are made for the industry-academia relations in the distributed processing environment of structural analysis and design. An ongoing experiment of distributed computing in a University environment is described.}
}



@inproceedings{chang-01,
	author    = {Liu, Chang and Zhao, Zhiwen and Liu, Fang},
	booktitle = {2009 International Symposium on Computer Network and Multimedia Technology},
	title     = {An Insight into the Architecture of Condor - A Distributed Scheduler},
	year      = {2009},
	volume    = {},
	number    = {},
	pages     = {1-4},
	keywords  = {Read-write memory;Random access memory;Resource management;Information science;Computer architecture;Batch production systems;Job shop scheduling;Processor scheduling;Computerized monitoring;Computer industry},
	doi       = {10.1109/CNMT.2009.5374622}
}


@misc{htcondor-description,
	title        = {What is {HTCondor}?},
	author       = {{Center for High Throughput Computing}},
	year         = {2025},
	url          = {https://htcondor.org/description.html},
	note         = {Accessed: July 30, 2025},
	institution  = {University of Wisconsin--Madison},
	howpublished = {\url{https://htcondor.org/description.html}}
}


###########%################### CARACTERIZACION UNIVERSOS #############################################


@manual{CERNBatchDocs,
	title        = {CERN Batch Service User Guide},
	author       = {{CERN IT Department}},
	organization = {CERN},
	howpublished = {\url{https://batchdocs.web.cern.ch/index.html}},
	note         = {Documentation for the CERN IT Batch Service based on HTCondor},
	year         = {2025},
	url          = {https://batchdocs.web.cern.ch/index.html}
}


@manual{HTCondor-roles,
	title = {HTCondor Administrator Manual},
	author = {{HTCondor Team}},
	organization = {University of Wisconsin–Madison},
	year = {2025},
	url = {https://htcondor.readthedocs.io/en/latest/admin-manual/introduction-admin-manual.html},
	note = {Accessed: 2025-10-07},
	urldate = {2025-10-07}
}


@misc{Emilio_DockerHTCondor,
	author = {Emilio, Carlo},
	title = {{Docker on HTCondor}},
	howpublished = {A guide published on the CERN ABP Computing website, available at \url{https://abpcomputing.web.cern.ch/guides/docker_on_htcondor/}},
	organization = {{ABP Computing @ CERN}},
	note = {Accessed: October 6, 2025},
	year = {s.f}
}

@manual{HTCondor_Parallel,
	title = {{Parallel Applications (Including MPI Applications)}},
	author = {{HTCondor Team}},
	organization = {{Center for High Throughput Computing, University of Wisconsin-Madison}},
	note = {HTCondor Manual, Users' Manual Chapter. Version 10.0.9 documentation.},
	url = {https://htcondor.readthedocs.io/en/v10_0/users-manual/parallel-applications.html},
	year = {2020}
}



@article{Thain2002,
	author    = {Douglas Thain and Todd Tannenbaum and Miron Livny},
	title     = {Condor and the Grid},
	journal   = {Concurrency and Computation: Practice and Experience},
	volume    = {0},
	issue     = {0},
	pages     = {0--20},
	year      = {2002},
	publisher = {John Wiley \& Sons, Ltd.},
	note      = {Chapter, Originally published as Concurrency: Pract. Exper. 2002},
	url       = {https://research.cs.wisc.edu/htcondor/doc/condorgrid.pdf}
}

@misc{HTCondor-env-services,
author = {{HTCondor Team}},
title = {{E}nvironment and services for a running job --- {H}{T}{C}ondor {M}anual 25.0.1 documentation --- htcondor.readthedocs.io},
howpublished = {\url{https://htcondor.readthedocs.io/en/lts/users-manual/env-of-job.html#parallel-jobs-including-mpi-jobs}},
year = {s.f.},
note = {[Accessed 07-10-2025]},
}


@online{HTCondor_vm_universe_wiki,
	title = {HTCondorWiki: Vm Universe},
	author = {{HTCondor Team}},
	organization = {University of Wisconsin--Madison},
	year = {2025},
	url = {https://htcondor-wiki.cs.wisc.edu/index.cgi/wiki?p=VmUniverse},
	urldate = {2025-10-08},
	note = {Wiki page; provides a comprehensive design outline for the vm-universe in HTCondor}
}


@misc{HTCondor-Grid-universe,
	author = {{HTCondor Team}},
	title = {The Grid Universe --- HTCondor Manual 25.3.0 documentation --- htcondor.readthedocs.io},
	howpublished = {\url{https://htcondor.readthedocs.io/en/main/grid-computing/grid-universe.html}},
	year = {s.f.},
	note = {[Accessed 08-10-2025]},
}

@inproceedings{Padmanabhan2011,
	author = {Padmanabhan, Anand and Wang, Shaowen and Navarro, John-Paul},
	title = {A CyberGIS gateway approach to interoperable access to the National Science Foundation TeraGrid and the Open Science Grid},
	year = {2011},
	isbn = {9781450308885},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2016741.2016786},
	doi = {10.1145/2016741.2016786},
	abstract = {The vision of creating a "virtual supercomputing" environment to solve large-scale scientific problems has largely been facilitated by the development and deployment of Grid middleware. However, with the deployment of multiple disconnected Grid environments, we are now faced with the problem of interoperable access to resources from multiple environments to meet the requirements of scientific applications. Within the U. S. cyberinfrastructure environments, two key elements: both the National Science Foundation TeraGrid and the Open Science Grid (OSG) provide varied but important capabilities and resources needed by diverse computational communities. Hence, it is critical to understand how these communities can benefit from bridging these different environments and utilize them when needed. In this paper we present a novel approach to interoperable access to both OSG and TeraGrid to users through the CyberGIS Gateway -- an online geographic information system. In particular, five key interoperability themes are addressed: authentication and authorization, information services, data management, and computation management and auditing. We take a scientific application use-case (viewshed analysis) on the CyberGIS Gateway to demonstrate how to exploit resources on both OSG and TeraGrid.},
	booktitle = {Proceedings of the 2011 TeraGrid Conference: Extreme Digital Discovery},
	articleno = {42},
	numpages = {8},
	keywords = {science gateway, distributed computing, cyberinfrastructure interoperability, CyberGIS},
	location = {Salt Lake City, Utah},
	series = {TG '11}
}

@ARTICLE{4163024,
	author={Jeffries, Ron and Melnik, Grigori},
	journal={IEEE Software},
	title={Guest Editors' Introduction: TDD--The Art of Fearless Programming},
	year={2007},
	volume={24},
	number={3},
	pages={24-30},
	keywords={Art;Programming profession;Software testing;Performance evaluation;Spatial databases;Embedded software;Graphical user interfaces;Safety;Writing;Automatic testing;program testing;software engineering;test-driven development;TDD},
	doi={10.1109/MS.2007.75}}

@INPROCEEDINGS{8974490,
	author={Singh, Manjeet},
	booktitle={2019 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)},
	title={An Overview of Grid Computing},
	year={2019},
	volume={},
	number={},
	pages={194-198},
	keywords={Cloud computing;Schedules;Processor scheduling;Scalability;Organizations;Parallel processing;Grid computing;Data processing;Software;Resource management;Grid Computing;Distributed Computing;Geographically Distributed;Heterogeneous Resources},
	doi={10.1109/ICCCIS48478.2019.8974490}}

@ARTICLE{159342,
	author={IEEE},
	journal={IEEE Std 610.12-1990},
	title={IEEE Standard Glossary of Software Engineering Terminology},
	year={1990},
	volume={},
	number={},
	pages={1-84},
	keywords={Terminology;Software engineering;Standards;glossary;terminology;dictionary;Software engineering;Definitions},
	doi={10.1109/IEEESTD.1990.101064}}

@INPROCEEDINGS{4384163,
	author={Glinz, Martin},
	booktitle={15th IEEE International Requirements Engineering Conference (RE 2007)},
	title={On Non-Functional Requirements},
	year={2007},
	volume={},
	number={},
	pages={21-26},
	keywords={Yarn;Jacobian matrices;Timing;Costs;Maintenance;Wikipedia;Informatics;Ergonomics;System testing;Security},
	doi={10.1109/RE.2007.45}}

@article{josey2016introduction,
	title={An introduction to the ArchiMate{\textregistered} 3.0 specification},
	author={Josey, Andrew and Lankhorst, Marc and Band, Iver and Jonkers, Henk and Quartel, Dick},
	journal={White Paper from The Open Group},
	pages={35},
	year={2016}
}

@InProceedings{5WAmitava,
	author="Das, Amitava
	and Bandyaopadhyay, Sivaji
	and Gamb{\"a}ck, Bj{\"o}rn",
	editor="Gelbukh, Alexander",
	title="The 5W Structure for Sentiment Summarization-Visualization-Tracking",
	booktitle="Computational Linguistics and Intelligent Text Processing",
	year="2012",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="540--555",
	abstract="In this paper we address the Sentiment Analysis problem from the end user's perspective. An end user might desire an automated at-a-glance presentation of the main points made in a single review or how opinion changes time to time over multiple documents. To meet the requirement we propose a relatively generic opinion 5Ws structurization, further used for textual and visual summary and tracking. The 5W task seeks to extract the semantic constituents in a natural language sentence by distilling it into the answers to the 5W questions: Who, What, When, Where and Why. The visualization system facilitates users to generate sentiment tracking with textual summary and sentiment polarity wise graph based on any dimension or combination of dimensions as they want i.e. ``Who'' are the actors and ``What'' are their sentiment regarding any topic, changes in sentiment during ``When'' and ``Where'' and the reasons for change in sentiment as ``Why''.",
	isbn="978-3-642-28604-9"
}

@article{10.1145/1125944.1125949,
	author = {Dobing, Brian and Parsons, Jeffrey},
	title = {How UML is used},
	year = {2006},
	issue_date = {May 2006},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {49},
	number = {5},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/1125944.1125949},
	doi = {10.1145/1125944.1125949},
	abstract = {Many UML projects are not Use Case driven.},
	journal = {Commun. ACM},
	month = may,
	pages = {109–113},
	numpages = {5}
}

@misc{xcpng_intro,
	title = {Requirements - XCP-ng Documentation},
	author = {{XCP-ng Team and Contributors}},
	organization = {XCP-ng},
	year = {2025},
	url = {https://docs.xcp-ng.org/installation/requirements/},
	note = {Accessed: 2025-10-09}
}

@misc{HTCondor_install,
	title = {Downloading and Installing - HTCondor Manual},
	author = {{HTCondor Team}},
	year = {2025},
	url = {https://htcondor.readthedocs.io/en/latest/getting-htcondor/},
	note = {HTCondor Version 25.3.0 documentation; provides instructions for installing HTCondor on single machines, clusters, and via Docker. Accessed: 2025-10-09}
}

@unpublished{Bunsic2025,
	author = {Buncic, Predrag and Peters, Andreas Joachim and Espinal, Xavier and others},
	title = {Notes 26th {RCS-ICT} Technical Committee},
	year = {2025},
	month = {9},
	url = {https://indico.cern.ch/event/1460900/attachments/3135176/5562729/Notes\%2026th\%20RCS-ICT\%20Technical\%20Committee-v0.pdf},
	note = {Meeting notes from the 26th RCS-ICT Technical Committee, CERN, September 5, 2025. Covers Linux strategy update and MLFlow as an IT service.},
	urldate = {2025-10-09}
}




@misc{HTCondor-linux-install,
	title        = {Linux (as root) — HTCondor Manual},
	author       = {{HTcondor Team}},
	year         = {2025},
	howpublished = {\url{https://htcondor.readthedocs.io/en/latest/getting-htcondor/install-linux-as-root.html}},
	note         = {Accessed: 2025-10-09}
}


@article{CMMIInstitute2010,
abstract = {El Marco CMMI es la estructura b{\'{a}}sica que organiza los componentes de CMMI y los combina en las constelaciones y modelos CMM},
author = {{CMMI Institute}},
file = {:Users/aariaz/Downloads/CMMI - DAR.pdf:pdf},
number = {1},
pages = {1--555},
title = {{CMMI {\textregistered} para Desarrollo, Versi{\'{o}}n 1.3 Equipo del Producto CMMI}},
url = {http://www.sei.cmu.edu},
volume = {1},
year = {2010}
}











% ---------------------------------------------------------------
% ---------------DESDE AQUI INICIAN LOS SPSs---------------------
% ---------------------------------------------------------------

@article{Selikhov2005,
abstract = {Fault tolerant message passing environments protect parallel applications against node failures. Very large scale computing systems, ranging from large clusters to worldwide Global Computing systems, require a high level of fault tolerance in order to efficiently run parallel applications. The Channel Memory approach provides the infrastructure for scalable tolerance to simultaneous faults. Along with a specially designed checkpointing system and recovery protocol, this approach has resulted in the MPICH-V architecture. In this paper, we describe CMDE – a stand-alone distributed program system based on MPICH-V architecture and implementing an approach to tolerate faults of Channel Memories.},
author = {Selikhov, A and Germain, C},
doi = {10.1016/j.future.2004.05.011},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Channel Memory,Fault tolerance,Global Computing,Grid,Message passing interface},
month = {may},
number = {5},
pages = {709--715},
title = {{A Channel Memory based fault tolerance for MPI applications}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X04000822 https://linkinghub.elsevier.com/retrieve/pii/S0167739X04000822},
volume = {21},
year = {2005}
}
@article{10.1016/j.future.2019.05.062,
abstract = {There are many scientific and commercial applications that require the execution of a large number of independent jobs resulting in significant overall execution time. Therefore, such applications typically require distributed computing infrastructures and science gateways to run efficiently and to be easily accessible for end-users. Optimising the execution of such applications in a cloud computing environment by keeping resource utilisation at minimum but still completing the experiment by a set deadline has paramount importance. As container-based technologies are becoming more widespread, support for job-queuing and auto-scaling in such environments is becoming important. Current container management technologies, such as Docker Swarm or Kubernetes, while provide auto-scaling based on resource consumption, do not support job queuing and deadline-based execution policies directly. This paper presents JQueuer, a cloud-agnostic queuing system that supports the scheduling of a large number of jobs in containerised cloud environments. The paper also demonstrates how JQueuer, when integrated with a cloud application-level orchestrator and auto-scaling framework, called MiCADO, can be used to implement deadline-based execution policies. This novel technical solution provides an important step towards the cost-optimisation of batch processing and job submission applications. In order to test and prove the effectiveness of the solution, the paper presents experimental results when executing an agent-based simulation application using the open source REPAST simulation framework.},
address = {NLD},
author = {Kiss, Tamas and DesLauriers, James and Gesmier, Gregoire and Terstyanszky, Gabor and Pierantoni, Gabriele and Oun, Osama Abu and Taylor, Simon J.E. and Anagnostou, Anastasia and Kovacs, Jozsef},
doi = {10.1016/j.future.2019.05.062},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Agent-based simulation,Cloud computing,Container technologies,Deadline-based auto-scaling,JQueuer,MiCADO},
number = {C},
pages = {99--111},
publisher = {Elsevier Science Publishers B. V.},
title = {{A cloud-agnostic queuing system to support the implementation of deadline-based application execution policies}},
url = {https://doi.org/10.1016/j.future.2019.05.062},
volume = {101},
year = {2019}
}
@inproceedings{10.1109/GCE.2014.8,
abstract = {Diffusion Tensor Imaging (DTI) may soon become the noninvasive tool of choice for diagnosing Traumatic Brain Injury (TBI) due to its ability to examine white matter changes in the living brain. While research has shown the value of DTI in differentiating affected and control populations, further work is required to develop algorithms and tools that enable reliable and reproducible methods for obtaining clinical biomarkers. Current approaches are computationally intensive, collaborative and require orchestrated invocation of many independently developed research tools. Thus, researchers are faced with challenges related to developing such complex analysis pipelines and executing them at scale on high performance compute resources and over large subject cohorts. To alleviate the need for both considerable technical expertise and dedicated local resources to set up and run those analyses, we have developed an always available cloud-based gateway service in which computational resources are provisioned and configured on demand. We build upon the Globus Galaxies platform which allows researchers to assemble and execute complex pipelines through their web browser. Our DTI gateway supports a range of different analyses and implements an execution model in which cloud compute instances are dynamically provisioned and configured.},
author = {Chard, Kyle and Madduri, Ravi and Jiang, Xia and Dahi, Farid and Vannier, Michael W. and Foster, Ian},
booktitle = {Proceedings of GCE 2014: 9th Gateway Computing Environments Workshop, held in conjunction with SC 2014: The International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/GCE.2014.8},
isbn = {9781479970308},
keywords = {Diffusion tensor imaging,Educational institutions,Logic gates,Pipelines,Sociology,Statistics,Tensile stress},
pages = {13--16},
publisher = {IEEE Press},
series = {GCE '14},
title = {{A cloud-based image analysis gateway for Traumatic Brain Injury research}},
url = {https://doi.org/10.1109/GCE.2014.8},
year = {2015}
}
@inproceedings{Quang2015,
abstract = {Traditional High-Throughput Computing (HTC) consists of running many loosely-coupled tasks that are independent but requires a large amount of computing power during significant period of time. However, recent emerging applications requiring millions or even billions of tasks to be processed within a relatively short period of time have expanded the traditional HTC into Many-Task Computing (MTC).In silico drug discovery offers an efficient alternative to reduce the cost of drug development and discovery process. For this purpose, virtual screening is used to select the most promising candidate drugs for in vitro testing from millions of chemical compounds. This process requires a substantial amount of computing resources and high-performance processing of docking simulations, which shows the typical characteristics of MTC applications. As the number of users performing this virtual screening process increases with limited available computing resources, it becomes crucial to devise an effective scheduling policy that can ensure a certain degree of fairness and user satisfaction. In this paper, we present a comparative analysis of scheduling mechanisms for the virtual screening workflow where multiple users in the system are sharing a common service infrastructure. To effectively support these multiple users, the underlying system should be able to consider fairness, user response time and overall system throughput. We have implemented two different scheduling algorithms which can address fairness and user response time respectively in a common middleware stack call edHTCaaS which is a pilot-job based multi-level scheduling system running on top of a dedicated production-level cluster. Throughout comparative analysis of two different scheduling mechanisms targeting different metrics on top of a single H/W and S/W system, we can give an insight to the research community on the design and implementation of a scheduling mechanism that can trade-off user fairness and overall system performance which's crucial to support challenging MTC applications.},
author = {Quang, Bui The and Kim, Jik Soo and Rho, Seungwoo and Kim, Seoyoung and Kim, Sangwan and Hwang, Soonwook and Medernach, Emmanuel and Breton, Vincent},
booktitle = {Proceedings - 2015 IEEE/ACM 15th International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2015},
doi = {10.1109/CCGrid.2015.123},
isbn = {9781479980062},
keywords = {Fairness,HTCaaS,Many-task computing,Protein-ligand docking,Scheduling mechanisms,User response time,Virtual screening workflow},
pages = {853--862},
publisher = {IEEE Press},
series = {CCGRID '15},
title = {{A comparative analysis of scheduling mechanisms for virtual screening workflow in a shared resource environment}},
url = {https://doi.org/10.1109/CCGrid.2015.123},
year = {2015}
}
@article{Abramson2002,
abstract = {Computational grids that couple geographically distributed resources such as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments continue to be a complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service (QoS). Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user-defined QoS requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength grids. We present the results of experiments using the Nimrod-G resource broker for scheduling parametric computations on the World Wide Grid (WWG) resources that span five continents.},
author = {Abramson, David and Buyya, Rajkumar and Giddy, Jonathan},
doi = {10.1016/S0167-739X(02)00085-7},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Computational economy,Grid computing,Grid scheduling,Nimrod-G broker,Resource management},
month = {oct},
number = {8},
pages = {1061--1074},
title = {{A computational economy for grid computing and its implementation in the Nimrod-G resource broker}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X02000857 https://linkinghub.elsevier.com/retrieve/pii/S0167739X02000857},
volume = {18},
year = {2002}
}
@article{Venkataraman2015,
abstract = {To harvest idle, unused computational resources in networked environments, researchers have proposed different architectures for desktop grid infrastructure. In this paper, we present one such infrastructure, called the Mini-Grid Framework for resource management in ad hoc grids using market-based scheduling and context-based resource and application modeling. The framework proposes peer-to-peer architecture that supports several futures: decentralized task distribution, small scale ad hoc grid formation, and symmetric resource. Furthermore, users can model and specify non-performance based parameters that influence resource allocation. Different types of resources can provide similar capabilities but with varying degrees of quality of service. Hence, the resource capabilities are required to be presented in such a way that resource providers can evaluate their capabilities against the requested capabilities for task execution, and the resource consumers can find optimal resources.},
author = {Venkataraman, Neelanarayanan},
doi = {10.1016/j.procs.2015.04.099},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {context information management,desktop grid computing,quality of service,resource description},
pages = {653--662},
title = {{A Context-aware Task Scheduling in Ad-hoc Desktop Grids}},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915006006 https://linkinghub.elsevier.com/retrieve/pii/S1877050915006006},
volume = {50},
year = {2015}
}

@article{10.1145/3594539,
abstract = {Human-centered robotic applications are becoming pervasive in the context of robotics and smart manufacturing, and such a pervasiveness is even more expected with the shift to Industry 5.0. The always increasing level of autonomy of modern robotic platforms requires the integration of software applications from different domains to implement artificial intelligence, cognition, and human-robot/robot-robot interaction. Developing and (re)configuring such a multi-domain software to meet functional constraints is a challenging task. Even more challenging is customizing the software to satisfy non-functional requirements such as real-time, reliability, and energy efficiency. In this context, the concept of Edge-Cloud continuum is gaining consensus as a solution to address functional and non-functional constraints in a seamless way. Containerization and orchestration are becoming a standard practice, as they allow for better information flow among different network levels as well as increased modularity in the use of multi-domain software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de facto development standards (e.g., ROS - Robotic Operating System) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The methodology aims at (i) integrating and verifying multi-domain components since early in the design flow, (ii) mapping software tasks to containers to minimize the performance and memory footprint overhead, (iii) clustering containers to efficiently distribute load across the edge-cloud architecture by minimizing resource utilization, and (iv) enabling multi-domain verification of functional and non-functional constraints before deployment. The article presents the results obtained with a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. We have obtained reduced load on the robot's HW with minimal performance and network overhead, thanks to the optimized distributed system.},
address = {New York, NY, USA},
annote = {Tags: Extension, Containerization, Kubernetes
CVI: 3

Segunda iteracion},
author = {Lumpp, Francesco and Panato, Marco and Bombieri, Nicola and Fummi, Franco},
doi = {10.1145/3594539},
issn = {1539-9087},
journal = {ACM Transactions on Embedded Computing Systems},
keywords = {Docker,Edge-Cloud computing,K3S,Kubernetes,ROS,robotic applications},
number = {5},
pages = {1--24},
publisher = {Association for Computing Machinery},
title = {{A Design Flow Based on Docker and Kubernetes for ROS-based Robotic Software Applications}},
url = {https://doi.org/10.1145/3594539},
volume = {23},
year = {2024}
}
@article{Wu2012,
abstract = {Next-generation scientific applications feature complex workflows comprised of many computing modules with intricate inter-module dependencies. Supporting such scientific workflows in wide-area networks especially Grids and optimizing their performance are crucial to the success of collaborative scientific discovery. We develop a Scientific Workflow Automation and Management Platform (SWAMP), which enables scientists to conveniently assemble, execute, monitor, control, and steer computing workflows in distributed environments via a unified web-based user interface. The SWAMP architecture is built entirely on a seamless composition of web services: the functionalities of its own are provided and its interactions with other tools or systems are enabled through web services for easy access over standard Internet protocols while being independent of different platforms and programming languages. SWAMP also incorporates a class of efficient workflow mapping schemes to achieve optimal end-to-end performance based on rigorous performance modeling and algorithm design. The performance superiority of SWAMP over existing workflow mapping schemes is justified by extensive simulations, and the system efficacy is illustrated by large-scale experiments on real-life scientific workflows for climate modeling through effective system implementation, deployment, and testing on the Open Science Grid. {\textcopyright} 2012 Springer Science+Business Media B.V.},
author = {Wu, Qishi and Zhu, Mengxia and Gu, Yi and Brown, Patrick and Lu, Xukang and Lin, Wuyin and Liu, Yangang},
doi = {10.1007/s10723-012-9222-7},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Climate modeling,Distributed computing,Open Science Grid,Scientific workflow},
number = {3},
pages = {367--393},
title = {{A Distributed Workflow Management System with Case Study of Real-life Scientific Applications on Grids}},
url = {https://doi.org/10.1007/s10723-012-9222-7},
volume = {10},
year = {2012}
}
@article{Korkhov2008,
abstract = {We address the problem of porting parallel distributed applications from static homogeneous cluster environments to dynamic heterogeneous Grid resources. We introduce a generic technique for adaptive load balancing of parallel applications on heterogeneous resources and evaluate it using a case study application: a Virtual Reactor for simulation of plasma chemical vapour deposition. This application has a modular architecture with a number of loosely coupled components suitable for distribution over the Grid. It requires large parameter space exploration that allows using Grid resources for high-throughput computing. The Virtual Reactor contains a number of parallel solvers originally designed for homogeneous computer clusters that needed adaptation to the heterogeneity of the Grid. In this paper we study the performance of one of the parallel solvers, apply the technique developed for adaptive load balancing, evaluate the efficiency of this approach and outline an automated procedure for optimal utilization of heterogeneous Grid resources for high-performance parallel computing. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Korkhov, Vladimir V. and Krzhizhanovskaya, Valeria V. and Sloot, P. M.A.},
doi = {10.1016/j.jpdc.2007.08.010},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Adaptive load balancing,Benchmarking,Grid,Heterogeneous resources,PECVD,Virtual Reactor},
number = {5},
pages = {596--608},
title = {{A Grid-based Virtual Reactor: Parallel performance and adaptive load balancing}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731507001621},
volume = {68},
year = {2008}
}
@article{Anand2011,
abstract = {The objective of this research is to convert ordinary idle PCs into virtual clusters for executing parallel applications. The paper presents VolpexMPI that is designed to enable seamless forward application progress in the presence of frequent node failures as well as dynamically changing networks and node execution speeds. Process replication is employed to provide robustness. The central challenge in the design of VolpexMPI is to efficiently and automatically manage dynamically varying number of process replicas in different states of execution progress. The key fault tolerance technique employed is fully distributed sender based logging. The paper presents the design and an implementation of VolpexMPI. Preliminary results validate that the overhead of providing robustness is modest for applications with a favorable ratio of communication to computation and a low degree of communication. {\textcopyright} 2010 Springer Science+Business Media B.V.},
author = {Anand, Rakhi and LeBlanc, Troy and Gabriel, Edgar and Subhlok, Jaspal},
doi = {10.1007/s10723-010-9172-x},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Message logging,Message passing interface,Process failures,Process replication,Volunteer computing},
number = {3},
pages = {325--344},
title = {{A Robust and Efficient Message Passing Library for Volunteer Computing Environments}},
url = {https://doi.org/10.1007/s10723-010-9172-x},
volume = {9},
year = {2011}
}
@article{Altunay2011,
abstract = {This article describes the Open Science Grid, a large distributed computational infrastructure in the United States which supports many different high-throughput scientific applications, and partners (federates) with other infrastructures nationally and internationally to form multi-domain integrated distributed systems for science. The Open Science Grid consortium not only provides services and software to an increasingly diverse set of scientific communities, but also fosters a collaborative team of practitioners and researchers who use, support and advance the state of the art in large-scale distributed computing. The scale of the infrastructure can be expressed by the daily throughput of around seven hundred thousand jobs, just under a million hours of computing, a million file transfers, and half a petabyte of data movement. In this paper we introduce and reflect on some of the OSG capabilities, usage and activities. {\textcopyright} 2010 Springer Science+Business Media B.V. (outside the USA).},
author = {Altunay, Mine and Avery, Paul and Blackburn, Kent and Bockelman, Brian and Ernst, Michael and Fraser, Dan and Quick, Robert and Gardner, Robert and Goasguen, Sebastien and Levshina, Tanya and Livny, Miron and McGee, John and Olson, Doug and Pordes, Ruth and Potekhin, Maxim and Rana, Abhishek and Roy, Alain and Sehgal, Chander and Sfiligoi, Igor and Wuerthwein, Frank},
doi = {10.1007/s10723-010-9176-6},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Data intensive computing,Distributed computing,Grids,High throughput computing,Scientific computing},
number = {2},
pages = {201--218},
title = {{A Science Driven Production Cyberinfrastructure-the Open Science Grid}},
url = {https://doi.org/10.1007/s10723-010-9176-6},
volume = {9},
year = {2011}
}
@article{Saad2016,
abstract = {Nowadays, the adoption of Cloud Computing platforms and Service Computing technologies are almost natural for the different e-Science communities. Cost benefits for data-intensive applications, ease of access, rich and varied offers for services are examples of positive returns by users. However, beyond this favorable welcome for the technology, some research problems remain and are still challenging. In this paper, we focus on the problems of automatically deploying IaaS for computing and for data management, using the SlapOS Cloud. The core of the system is a distributed protocol for orchestrating data and compute nodes. Using this interaction scheme, users are able to deploy, without any system administrator intervention, a PaaS inside the IaaS basically a Desktop Grid middleware. The aim of this paper is to demonstrate that the Desktop Grid and Cloud paradigms may merge and may be widely used by non-experts in the different areas of e-Science. We propose a fully self-organized volunteer Cloud for researchers where they can carry out e-Science experiments and process large amounts of data in a coherent way.},
author = {Saad, Walid and Abbes, Heithem and C{\'{e}}rin, Christophe and Jemni, Mohamed},
doi = {10.1007/s11227-015-1564-z},
issn = {15730484},
journal = {Journal of Supercomputing},
keywords = {Cloud Computing,Desktop Grid,Emerging platform technologies,Self-configuration of applications,Volunteer Cloud,e-Science applications as a service},
number = {4},
pages = {1271--1290},
title = {{A self-organized volunteer Cloud for e-Science}},
url = {https://doi.org/10.1007/s11227-015-1564-z},
volume = {72},
year = {2016}
}
@article{Butt2006,
abstract = {Condor enables high throughput computing using off-the-shelf cost-effective components. It also supports flocking, a mechanism for sharing resources among Condor pools. Since Condor pools distributed over a wide area can have dynamically changing availability and sharing preferences, the current flocking mechanism based on static configurations can limit the potential of sharing resources across Condor pools. This paper presents a technique for resource discovery in distributed Condor pools using peer-to-peer mechanisms that are self-organizing, fault-tolerant, scalable, and locality-aware. Locality-awareness guarantees that applications are not shipped across long distances when nearby resources are available. Measurements using a synthetic job trace show that self-organized flocking reduces the maximum job wait time in queue for a heavily loaded pool by a factor of 10 compared to without flocking. Simulations of 1000 Condor pools are also presented and the results confirm that our technique discovers and utilizes nearby resources in the physical network. {\textcopyright} 2005 Elsevier Inc. All righs reserved.},
author = {Butt, Ali R. and Zhang, Rongmei and Hu, Y. Charlie},
doi = {10.1016/j.jpdc.2005.06.022},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Condor,Distributed resource discovery,Flocking,Peer-to-peer},
number = {1},
pages = {145--161},
title = {{A self-organizing flock of Condors}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731505001747},
volume = {66},
year = {2006}
}
@inproceedings{10.1145/1566445.1566492,
abstract = {We present a performance study of a virtualized cluster based on the virtualization system KVM. We show bench-mark results from the High Performance Computing Challenge (HPCC) application suite including the High Performance Linpack (HPL) benchmark. We also present the mechanism by which this cluster is connected to the Open Science Grid (OSG). Our results show that jobs with low amounts of network communication will only suer moderate overhead (≈10%) due to virtualization, while MPI applications will suer from a considerable overhead in the 60% range. The KVM cluster under investigation does prove to be suitable for current High Throughput Computing (HTC) grid usage on OSG where the Condor middleware is used. {\textcopyright}2009 ACM.},
address = {New York, NY, USA},
author = {Fenn, Michael and Murphy, Michael A. and Goasgueny, Sebastien},
booktitle = {Proceedings of the 47th Annual Southeast Regional Conference, ACM-SE 47},
doi = {10.1145/1566445.1566492},
isbn = {9781605584218},
keywords = {Benchmark,Cluster,Condor,Grid,KVM},
publisher = {Association for Computing Machinery},
series = {ACMSE '09},
title = {{A study of a KVM-based cluster for grid computing}},
url = {https://doi.org/10.1145/1566445.1566492},
year = {2009}
}
@article{Liu2015,
abstract = {Nowadays, more and more computer-based scientific experiments need to handle massive amounts of data. Their data processing consists of multiple computational steps and dependencies within them. A data-intensive scientific workflow is useful for modeling such process. Since the sequential execution of data-intensive scientific workflows may take much time, Scientific Workflow Management Systems (SWfMSs) should enable the parallel execution of data-intensive scientific workflows and exploit the resources distributed in different infrastructures such as grid and cloud. This paper provides a survey of data-intensive scientific workflow management in SWfMSs and their parallelization techniques. Based on a SWfMS functional architecture, we give a comparative analysis of the existing solutions. Finally, we identify research issues for improving the execution of data-intensive scientific workflows in a multisite cloud.},
author = {Liu, Ji and Pacitti, Esther and Valduriez, Patrick and Mattoso, Marta},
doi = {10.1007/s10723-015-9329-8},
issn = {15729184},
journal = {Journal of Grid Computing},
keywords = {Cloud,Distributed and parallel data management,Grid,Multisite cloud,Parallelization,Scheduling,Scientific workflow,Scientific workflow management system},
number = {4},
pages = {457--493},
title = {{A Survey of Data-Intensive Scientific Workflow Management}},
url = {https://doi.org/10.1007/s10723-015-9329-8},
volume = {13},
year = {2015}
}
@inproceedings{Wang2020,
abstract = {In the rapidly expanding field of parallel processing, job schedulers act as the "operating systems"of the clusters, including modern big data architectures and supercomputing systems. Job schedulers manage and allocate system resources, dispatch the queued jobs, and control the execution of processes on the allocated resources. In this paper, we firstly make an introduction to the cluster schedulers. Then according to the scenarios, we make a comprehensive survey of schedulers for HPC and Big Data. We can conclude that most of these current schedulers are centralized, which means master assigns jobs to the slaves. We call this mode Push, which is different from our new idea that introduces Pull to the schedulers. We proposed a novel scheduling model that allow slaves to actively pull jobs from master to execute. By analyzing the execution time and resource requests of jobs in "Tianhe-II", we will clarify that scheduling based on Push & Pull is a direction worthy of in-depth study in the future.},
address = {New York, NY, USA},
author = {Wang, Bo and Chen, Zhiguang and Xiao, Nong},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3407947.3407977},
isbn = {9781450376914},
keywords = {Job scheduler,big data cluster,decentralized scheduling,high performance computing},
pages = {178--183},
publisher = {Association for Computing Machinery},
series = {HP3C 2020},
title = {{A Survey of System Scheduling for HPC and Big Data}},
url = {https://doi.org/10.1145/3407947.3407977},
year = {2020}
}
@article{Grehant2013,
abstract = {Grids designed for computationally demanding scientific applications started experimental phases ten years ago and have been continuously delivering computing power to a wide range of applications for more than half of this time. The observation of their emergence and evolution reveals actual constraints and successful approaches to task mapping across administrative boundaries. Beyond differences in distributions, services, protocols, and standards, a common architecture is outlined. Application-agnostic infrastructures built for resource registration, identification, and access control dispatch delegation to grid sites. Efficient task mapping is managed by large, autonomous applications or collaborations that temporarily infiltrate resources for their own benefits. {\textcopyright} 2013 ACM.},
address = {New York, NY, USA},
author = {Grehant, Xavier and Demeure, Isabelle and Jarp, Sverre},
doi = {10.1145/2480741.2480754},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Computing Grids,Grid Architecture,Late Binding,Meta-Scheduling,Pull Model,Resource Allocation,Resource Utilization,Task Mapping,Task Scheduling},
month = {jul},
number = {3},
publisher = {Association for Computing Machinery},
title = {{A survey of task mapping on production grids}},
url = {https://doi.org/10.1145/2480741.2480754},
volume = {45},
year = {2013}
}
@article{10.1016/j.future.2017.11.007,
abstract = {The increasing amount of free and open geospatial data of interest to major societal questions calls for the development of innovative data-intensive computing platforms for the efficient and effective extraction of information from these data. This paper proposes a versatile petabyte-scale platform based on commodity hardware and equipped with open-source software for the operating system, the distributed file system, and the task scheduler for batch processing as well as the containerization of user specific applications. Interactive visualization and processing based on deferred processing are also proposed. The versatility of the proposed platform is illustrated with a series of applications together with their performance metrics.},
address = {NLD},
annote = {No se encuentran las palabras clave},
author = {Soille, P. and Burger, A. and {De Marchi}, D. and Kempeneers, P. and Rodriguez, D. and Syrris, V. and Vasilev, V.},
doi = {10.1016/j.future.2017.11.007},
issn = {0167739X},
journal = {Future Generation Computer Systems},
number = {C},
pages = {30--40},
publisher = {Elsevier Science Publishers B. V.},
title = {{A versatile data-intensive computing platform for information retrieval from big geospatial data}},
url = {https://doi.org/10.1016/j.future.2017.11.007},
volume = {81},
year = {2018}
}
@article{Curran2009,
abstract = {This work presents the details of a model for fully decentralised scientific workflow management designed to support the efficient execution of applications in heterogeneous and unpredictable computing environments. The proposed model involves a novel application of the overlay metacomputer concept, built on a graph-oriented peer to peer framework. The use of peer to peer overlay on both batch and cycle scavenging systems simultaneously enables the aggregation and virtual homogenisation of heterogeneous resource collectives, and makes possible the execution of complex scientific workflow applications on otherwise batch oriented processors. Overlaying a peer to peer workflow management system on multiple independent sites decouples task expression from resource allocation, i.e., the host resource managers allocate processing elements to host the peers, allowing the peers execute multiple workflow tasks. This facilitates the efficient execution of very fine grained workflow expressions, which is shown to reduce both application makespan and wasted cycles when using processing elements allocated by a pre-emptive resource manager. Quantitative results are presented based on performance observed in a large scale heterogeneous multi-user environment. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Curran, Ois{\'{i}}n and Shearer, Andy},
doi = {10.1016/j.future.2008.09.010},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Condensed graphs,Fine grained workflow,Heterogeneous resource aggregation,Pre-emptive environments,Webcom},
number = {4},
pages = {414--425},
title = {{A workflow model for heterogeneous computing environments}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X08001532},
volume = {25},
year = {2009}
}
@article{Cai2023,
abstract = {We study the performance of a cloud-based GPU-accelerated inference server to speed up event reconstruction in neutrino data batch jobs. Using detector data from the ProtoDUNE experiment and employing the standard DUNE grid job submission tools, we attempt to reprocess the data by running several thousand concurrent grid jobs, a rate we expect to be typical of current and future neutrino physics experiments. We process most of the dataset with the GPU version of our processing algorithm and the remainder with the CPU version for timing comparisons. We find that a 100-GPU cloud-based server is able to easily meet the processing demand, and that using the GPU version of the event processing algorithm is two times faster than processing these data with the CPU version when comparing to the newest CPUs in our sample. The amount of data transferred to the inference server during the GPU runs can overwhelm even the highest-bandwidth network switches, however, unless care is taken to observe network facility limits or otherwise distribute the jobs to multiple sites. We discuss the lessons learned from this processing campaign and several avenues for future improvements.},
annote = {Tags: Cloud Computing, Heterogenenous Resources, Research, Machine Learning
CVI: 3

Primera iteracion},
archivePrefix = {arXiv},
arxivId = {2301.04633},
author = {Cai, Tejin and Herner, Kenneth and Yang, Tingjun and Wang, Michael and {Acosta Flechas}, Maria and Harris, Philip and Holzman, Burt and Pedro, Kevin and Tran, Nhan},
doi = {10.1007/s41781-023-00101-0},
eprint = {2301.04633},
issn = {25102044},
journal = {Computing and Software for Big Science},
keywords = {Cloud computing (SaaS),Distributed computing,GPU (graphics processing unit),Heterogeneous (CPU+GPU) computing,Machine learning,Neutrino physics,Particle physics},
number = {1},
pages = {11},
title = {{Accelerating Machine Learning Inference with GPUs in ProtoDUNE Data Processing}},
url = {https://doi.org/10.1007/s41781-023-00101-0},
volume = {7},
year = {2023}
}
@inproceedings{Chard2016,
abstract = {Cloud providers offer a diverse set of instance types with varying resource capacities, designed to meet the needs of a broad range of user requirements. While this flexibility is a major benefit of the cloud computing model, it also creates challenges when selecting the most suitable instance type for a given application. Sub-optimal instance selection can result in poor performance and/or increased cost, with significant impacts when applications are executed repeatedly. Yet selecting an optimal instance type is challenging, as each instance type can be configured differently, application performance is dependent on input data and configuration, and instance types and applications are frequently updated. We present a service that supports automatic profiling of application performance on different instance types to create rich application profiles that can be used for comparison, provisioning, and scheduling. This service can dynamically provision cloud instances, automatically deploy and contextualize applications, transfer input datasets, monitor execution performance, and create a composite profile with fine grained resource usage information. We use real usage data from four production genomics gateways and estimate the use of profiles in autonomic provisioning systems can decrease execution time by up to 15.7% and cost by up to 86.6%.},
author = {Chard, Ryan and Chard, Kyle and Ng, Bryan and Bubendorfer, Kris and Rodriguez, Alex and Madduri, Ravi and Foster, Ian},
booktitle = {Proceedings - 2016 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2016},
doi = {10.1109/CCGrid.2016.57},
isbn = {9781509024520},
keywords = {Cloud Computing,Cloud Provisioning,Profiling},
pages = {223--232},
publisher = {IEEE Press},
series = {CCGRID '16},
title = {{An Automated Tool Profiling Service for the Cloud}},
url = {https://doi.org/10.1109/CCGrid.2016.57},
year = {2016}
}
@article{DomizziSanchez-Gallegos2021,
abstract = {Workflow engines are commonly used to orchestrate large-scale scientific computations such as, but not limited to weather, climate, natural disasters, food safety, and territorial management. However, to implement, manage, and execute real-world scientific applications in the form of workflows on multiple infrastructures (servers, clusters, cloud) remains a challenge. In this paper, we present DagOnStar (Directed Acyclic Graph OnAnything), a lightweight Python library implementing a workflow paradigm based on parallel patterns that can be executed on any combination of local machines, on-premise high performance computing clusters, containers, and cloud-based virtual infrastructures. DagOnStar is designed to minimize data movement to reduce the application storage footprint. A case study based on a real-world application is explored to illustrate the use of this novel workflow engine: a containerized weather data collection application deployed on multiple infrastructures. An experimental comparison with other state-of-the-art workflow engines shows that DagOnStar can run workflows on multiple types of infrastructure with an improvement of 50.19% in run time when using a parallel pattern with eight task-level workers.},
annote = {Reviewed},
author = {Domizzi Sanchez-Gallegos, Dante and {Di Luccio}, Diana and Kosta, Sokol and Gonzalez-Compean, J. L. and Montella, Raffaele},
doi = {10.1016/j.future.2021.03.017},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud computing,Data intensive,Directed acyclic graph,Parallel processing,Workflow},
pages = {187--203},
title = {{An efficient pattern-based approach for workflow supporting large-scale science: The DagOnStar experience}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000984},
volume = {122},
year = {2021}
}
@article{Juve2012,
abstract = {Workflows are used to orchestrate data-intensive applications in many different scientific domains. Workflow applications typically communicate data between processing steps using intermediate files. When tasks are distributed, these files are either transferred from one computational node to another, or accessed through a shared storage system. As a result, the efficient management of data is a key factor in achieving good performance for workflow applications in distributed environments. In this paper we investigate some of the ways in which data can be managed for workflows in the cloud. We ran experiments using three typical workflow applications on Amazon's EC2 cloud computing platform. We discuss the various storage and file systems we used, describe the issues and problems we encountered deploying them on EC2, and analyze the resulting performance and cost of the workflows. {\textcopyright} 2012 Springer Science+Business Media B.V.},
author = {Juve, Gideon and Deelman, Ewa and Berriman, G. Bruce and Berman, Benjamin P. and Maechling, Philip},
doi = {10.1007/s10723-012-9207-6},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Cloud computing,Scientific workflows},
number = {1},
pages = {5--21},
title = {{An Evaluation of the Cost and Performance of Scientific Workflows on Amazon EC2}},
url = {https://doi.org/10.1007/s10723-012-9207-6},
volume = {10},
year = {2012}
}
@article{Zarza2012,
abstract = {Nowadays, the study of high-performance computing (HPC) is one of the essential aspects of postgraduate pro-grammes in Computational Science. However, university education in HPC often suffers from a significant gap between theoretical concepts and the practical experience of students. To face this challenge, we have implemented an innovative teaching strategy to provide students appropriate resources to ease the assimilation of theoretical con-cepts, while improving their practical experience through the use of teaching tools and resources specifically designed to promote active learning. We have used the proposed strategy to organize the module of Parallel Computers and Architectures of the Master's in High-Performance Computing, at the Universitat Aut‘onoma de Barcelona, obtaining very promising results. In particular, we have observed improvements of both the academic marks of students and the perception about their own expertise and skills in HPC, regarding the previous teaching approach.},
author = {Zarza, Gonzalo and Lugones, Diego and Franco, Daniel and Luque, Emilio},
doi = {10.1016/j.procs.2012.04.191},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {High-Performance Computing,OPNET Modeler,Simulation Management,University Education},
pages = {1733--1742},
title = {{An Innovative Teaching Strategy to Understand High-Performance Systems through Performance Evaluation}},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912003122 https://linkinghub.elsevier.com/retrieve/pii/S1877050912003122},
volume = {9},
year = {2012}
}
@inproceedings{Filgueira2017,
abstract = {We present Asterism, an open source data-intensive framework, which combines the strengths of traditional workflow management systems with new parallel stream-based dataflow systems to run data-intensive applications across multiple heterogeneous resources, without users having to: re-formulate their methods according to different enactment engines; manage the data distribution across systems; parallelize their methods; co-place and schedule their methods with computing resources; and store and transfer large/small volumes of data. We also present the Data-Intensive workflows as a Service (DIaaS) model, which enables easy dataintensive workow composition and deployment on clouds using containers. The feasibility of Asterism and DIaaS model have been evaluated using a real domain application on the NSF-Chameleon cloud. Experimental results shows how Asterism successfully and efficiently exploits combinations of diverse computational platforms, whereas DIaaS delivers specialized software to execute data-intensive applications in a scalable, efficient, and robust way reducing the engineering time and computational cost.},
author = {Filgueira, Rosa and Silva, Rafael Ferreira Da and Krause, Amrey and Deelman, Ewa and Atkinson, Malcolm},
booktitle = {Proceedings of DataCloud 2016: 7th International Workshop on Data-Intensive Computing in the Clouds - Held in conjunction with SC 2016: The International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/DataCloud.2016.004},
isbn = {9781509061587},
keywords = {Data-Intensive science,Deployment and reusability of execution environmen,scientific workows,stream-based system},
pages = {1--8},
publisher = {IEEE Press},
series = {DataCloud '16},
title = {{Asterism: Pegasus and Dispel4py Hybrid Workflows for Data-Intensive Science}},
year = {2017}
}
@inproceedings{Sfiligoi2022,
abstract = {HTCondor has been very successful in managing globally distributed, pleasantly parallel scientific workloads, especially as part of the Open Science Grid. HTCondor system design makes it ideal for integrating compute resources provisioned from anywhere, but it has very limited native support for autonomously provisioning resources managed by other solutions. This work presents a solution that allows for autonomous, demand-driven provisioning of Kubernetes-managed resources. A high-level overview of the employed architectures is presented, paired with the description of the setups used in both on-prem and Cloud deployments in support of several Open Science Grid communities. The experience suggests that the described solution should be generally suitable for contributing Kubernetes-based resources to existing HTCondor pools.},
address = {New York, NY, USA},
author = {Sfiligoi, Igor and Defanti, Thomas and W{\"{u}}rthwein, Frank},
booktitle = {PEARC 2022 Conference Series - Practice and Experience in Advanced Research Computing 2022 - Revolutionary: Computing, Connections, You},
doi = {10.1145/3491418.3535123},
isbn = {9781450391610},
keywords = {HTCondor,Kubernetes,auto scaling,provisioning},
publisher = {Association for Computing Machinery},
series = {PEARC '22},
title = {{Auto-scaling HTCondor pools using Kubernetes compute resources}},
url = {https://doi.org/10.1145/3491418.3535123},
year = {2022}
}
@inproceedings{Lenards2011,
abstract = {The iPlant Collaborative is an NSF-funded cyberinfrastructure (CI) effort directed towards the plant sciences community. This paper enumerates the key concepts, middleware, tools, and extensions that create the unique capabilities of the iPlant Discovery Environment (DE) that provide access to our CI. The DE is a rich web-based application that brings flexible CI capabilities to a wide audience affiliated with the plant sciences, from computational biologists, bioinformaticians, applications developers, to bench biologists. The inherent interdisciplinary nature of plant sciences research produces diverse and complex data products that range from molecular sequences to satellite imagery as part of the discovery life cycle. With the constant creation of novel analysis algorithms, the advent and spread of large data repositories, and the need for collaborative data analysis, marshaling resources to effectively utilize these capabilities necessitates a highly flexible and scalable approach for implementing underlying CI. The iPlant infrastructure simultaneously supports multiple interdisciplinary projects providing essential features found in traditional science gateways as well as highly customized direct access to its underlying frameworks through use of APIs (Application Programming Interfaces). This allows the community to develop de novo applications. This approach allows us to serve broad community needs while providing flexible, secure, and creative utilization of our platform that is based on best practices and that leverages established computational resources. {\textcopyright} 2011 ACM.},
address = {New York, NY, USA},
author = {Lenards, Andrew and Merchant, Nirav and Stanzione, Dan},
booktitle = {GCE'11 - Proceedings of the 2011 ACM Workshop on Gateway Computing Environments, Co-located with SC'11},
doi = {10.1145/2110486.2110494},
isbn = {9781450311236},
keywords = {Bioinformatics,Computational biology,Cyberinfrastructure,Plant biology,Plant sciences,Science gateways},
pages = {51--58},
publisher = {Association for Computing Machinery},
series = {GCE '11},
title = {{Building an environment to facilitate discoveries for plant sciences}},
url = {https://doi.org/10.1145/2110486.2110494},
year = {2011}
}
@inproceedings{Belgin2022,
abstract = {Open Science Grid (OSG) is a consortium that enables many scientific breakthroughs by providing researchers with access to shared High Throughput Computing (HTC) compute clusters in support of large-scale collaborative research. To meet the demand on campus, Georgia Institute of Technology (GT)'s Partnership for an Advanced Computing Environment (PACE) team launched a centralized OSG support project, powered by Buzzard, an NSF-funded OSG cluster. We describe Buzzard's unique multi-tenant architecture, which supports multiple projects on a single CPU/GPU pool, for the benefit of other institutions considering a similar approach to support OSG on their campuses.},
address = {New York, NY, USA},
author = {Belgin, Mehmet and Sarajlic, Semir and Lara, Ruben and Cadonati, Laura and Otte, A. Nepomuk and Taboada, Ignacio J. and Beyer, Gregory L. and Bonner, Norman B. and Brandon, Michael and Buffington, Pam and Coulter, J. Eric and Jezghani, Aaron and Leonard, David and Liu, Fang and Manno, Paul D. and Moseley, Craig A. and Nightingale, Trever C. and Rahaman, Ronald and Suda, Kenneth J. and Wan, Peter and Weiner, Michael D. and Womack, Deirdre and Zhou, Dan and McNeill, Andre C. and Bright, Neil C. and Gardner, Robert W. and Paschos, Pascal and Bryant, Lincoln Andrew and Stephen, Judith Lorraine and Clark, James Alexander and Couvares, Peter F. and Lin, Brian Hua and Tannenbaum, Todd and Thain, Gregory},
booktitle = {PEARC 2022 Conference Series - Practice and Experience in Advanced Research Computing 2022 - Revolutionary: Computing, Connections, You},
doi = {10.1145/3491418.3535135},
isbn = {9781450391610},
keywords = {HTCondor,High Throughput Computing,Open Science Grid,Stash-Cache},
publisher = {Association for Computing Machinery},
series = {PEARC '22},
title = {{Buzzard: Georgia Tech's Foray into the Open Science Grid}},
url = {https://doi.org/10.1145/3491418.3535135},
year = {2022}
}
@article{10.1016/j.jbi.2014.01.005,
abstract = {Due to the upcoming data deluge of genome data, the need for storing and processing large-scale genome data, easy access to biomedical analyses tools, efficient data sharing and retrieval has presented significant challenges. The variability in data volume results in variable computing and storage requirements, therefore biomedical researchers are pursuing more reliable, dynamic and convenient methods for conducting sequencing analyses. This paper proposes a Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses, which enables reliable and highly scalable execution of sequencing analyses workflows in a fully automated manner. Our platform extends the existing Galaxy workflow system by adding data management capabilities for transferring large quantities of data efficiently and reliably (via Globus Transfer), domain-specific analyses tools preconfigured for immediate use by researchers (via user-specific tools integration), automatic deployment on Cloud for on-demand resource allocation and pay-as-you-go pricing (via Globus Provision), a Cloud provisioning tool for auto-scaling (via HTCondor scheduler), and the support for validating the correctness of workflows (via semantic verification tools). Two bioinformatics workflow use cases as well as performance evaluation are presented to validate the feasibility of the proposed approach. {\textcopyright} 2014 Elsevier Inc.},
address = {San Diego, CA, USA},
author = {Liu, Bo and Madduri, Ravi K. and Sotomayor, Borja and Chard, Kyle and Lacinski, Lukasz and Dave, Utpal J. and Li, Jianqiang and Liu, Chunchen and Foster, Ian T.},
doi = {10.1016/j.jbi.2014.01.005},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Bioinformatics,Cloud computing,Galaxy,Scientific workflow,Sequencing analyses},
number = {C},
pages = {119--133},
pmid = {24462600},
publisher = {Elsevier Science},
title = {{Cloud-based bioinformatics workflow platform for large-scale next-generation sequencing analyses}},
url = {https://doi.org/10.1016/j.jbi.2014.01.005},
volume = {49},
year = {2014}
}
@article{10.14778/1453856.1453865,
abstract = {This paper introduces Clustera, an integrated computation and data management system. In contrast to traditional cluster-management systems that target specific types of workloads, Clustera is designed for extensibility, enabling the system to be easily extended to handle a wide variety of job types ranging from computationally-intensive, long-running jobs with minimal I/O requirements to complex SQL queries over massive relational tables. Another unique feature of Clustera is the way in which the system architecture exploits modern software building blocks including application servers and relational database systems in order to realize important performance, scalability, portability and usability benefits. Finally, experimental evaluation suggests that Clustera has good scale-up properties for SQL processing, that Clustera delivers performance comparable to Hadoop for MapReduce processing and that Clustera can support higher job throughput rates than previously published results for the Condor and CondorJ2 batch computing systems. {\textcopyright} 2008 VLDB Endowment.},
annote = {No se encuentran las palabras clave},
author = {DeWitt, David J. and Paulson, Erik and Robinson, Eric and Naughton, Jeffrey and Royalty, Joshua and Shankar, Srinath and Krioukov, Andrew},
doi = {10.14778/1453856.1453865},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
number = {1},
pages = {28--41},
publisher = {VLDB Endowment},
title = {{Clustera: An integrated computation and data management system}},
url = {https://doi.org/10.14778/1453856.1453865},
volume = {1},
year = {2008}
}
@inproceedings{10.5555/3018088.3018092,
abstract = {The majority of university courses which educate students in high performance, parallel, and distributed computing are located within computer science departments. This can potentially be a hurdle to students from other disciplines who need to acquire these critical skills.We discuss a sequence of application-driven courses designed to educate undergraduate and graduate students who do not necessarily have a computer science background on developing scientific research software, with an emphasis on using high performance, parallel, and distributed computational systems.},
author = {Wilson, Lucas A. and Dey, S. Charlie},
booktitle = {Proceedings of EduHPC 2016: Workshop on Education for High-Performance Computing - Held in conjunction with SC 2016: The International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/EduHPC.2016.008},
file = {::},
isbn = {9781509038275},
pages = {19--24},
publisher = {IEEE Press},
series = {EduHPC '16},
title = {{Computational Science Education Focused on Future Domain Scientists}},
year = {2017}
}
@article{Frey2001,
abstract = {In recent years, there has been a dramatic increase in the amount of available computing and storage resources. Yet few have been able to exploit these resources in an aggregated form. We present the Condor-G system, which leverages software from Globus and Condor to allow users to harness multi-domain resources as if they all belong to one personal domain. We describe the structure of Condor-G and how it handles job management, resource selection, security, and fault tolerance.},
author = {Frey, James and Tannenbaum, Todd and Livny, Miron and Foster, Ian and Tuecke, Steven},
doi = {10.1109/HPDC.2001.945176},
issn = {10828907},
journal = {IEEE International Symposium on High Performance Distributed Computing, Proceedings},
keywords = {Condor,Globus,Grid computing,distributed computing},
number = {3},
pages = {55--66},
title = {{Condor-G: A computation management agent for multi-institutional Grids}},
url = {https://doi.org/10.1023/A:1015617019423},
volume = {5},
year = {2001}
}
@article{Zhou2021,
abstract = {Containerisation demonstrates its efficiency in application deployment in Cloud Computing. Containers can encapsulate complex programs with their dependencies in isolated environments making applications more portable, hence are being adopted in High Performance Computing (HPC) clusters. Singularity, initially designed for HPC systems, has become their de facto standard container runtime. Nevertheless, conventional HPC workload managers lack micro-service support and deeply-integrated container management, as opposed to container orchestrators. We introduce a Torque-Operator which serves as a bridge between HPC workload manager (TORQUE) and container orchestrator (Kubernetes). We propose a hybrid architecture that integrates HPC and Cloud clusters seamlessly with little interference to HPC systems where container orchestration is performed on two levels.},
annote = {Tags: HPC, Extension, Containerization, Virtualization, Kubernetes
CVI: 3

Primera iteracion},
author = {Zhou, Naweiluo and Georgiou, Yiannis and Pospieszny, Marcin and Zhong, Li and Zhou, Huan and Niethammer, Christoph and Pejak, Branislav and Marko, Oskar and Hoppe, Dennis},
doi = {10.1186/s13677-021-00231-z},
issn = {2192113X},
journal = {Journal of Cloud Computing},
keywords = {Cloud computing,Container orchestration,HPC workload manager,Kubernetes,Singularity,TORQUE},
number = {1},
pages = {16},
title = {{Container orchestration on HPC systems through Kubernetes}},
url = {https://doi.org/10.1186/s13677-021-00231-z},
volume = {10},
year = {2021}
}
@inproceedings{Belkin2018,
abstract = {Software container solutions have revolutionized application development approaches by enabling lightweight platform abstractions within the so-called “containers.” Several solutions are being actively developed in attempts to bring the benefits of containers to high-performance computing systems with their stringent security demands on the one hand and fundamental resource sharing requirements on the other. In this paper, we discuss the benefits and short-comings of such solutions when deployed on real HPC systems and applied to production scientific applications. We highlight use cases that are either enabled by or significantly benefit from such solutions. We discuss the efforts by HPC system administrators and support staff to support users of these type of workloads on HPC systems not initially designed with these workloads in mind focusing on NCSA's Blue Waters system.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1808.00556},
author = {Belkin, Maxim and Leong, Hon Wai and Haas, Roland and Huerta, Eliu A. and Neubauer, Mark and Arnold, Galen Wesley and Lesny, David},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3219104.3219145},
eprint = {1808.00556},
isbn = {9781450364461},
keywords = {Data Science,Petascale,Reproducibility},
publisher = {Association for Computing Machinery},
series = {PEARC '18},
title = {{Container solutions for HPC systems: A case study of using shifter on blue waters}},
url = {https://doi.org/10.1145/3219104.3219145},
year = {2018}
}
@article{10.1016/j.jss.2017.01.007,
abstract = {eScience demands large-scale computing clusters to support the efficient execution of resource-intensive scientific applications. Virtual Machines (VMs) have introduced the ability to provide customizable execution environments, at the expense of performance loss for applications. However, in recent years, containers have emerged as a light-weight virtualization technology compared to VMs. Indeed, the usage of containers for virtual clusters allows better performance for the applications and fast deployment of additional working nodes, for enhanced elasticity. This paper focuses on the deployment, configuration and management of Virtual Elastic computer Clusters (VEC) dedicated to process scientific workloads. The nodes of the scientific cluster are hosted in containers running on bare-metal machines. The open-source tool Elastic Cluster for Docker (EC4Docker) is introduced, integrated with Docker Swarm to create auto-scaled virtual computer clusters of containers across distributed deployments. We also discuss the benefits and limitations of this solution and analyse the performance of the developed tools under a real scenario by means of a scientific use case that demonstrates the feasibility of the proposed approach.},
address = {USA},
author = {de Alfonso, Carlos and Calatrava, Amanda and Molt{\'{o}}, Germ{\'{a}}n},
doi = {10.1016/j.jss.2017.01.007},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Cluster computing,Computing,Containers,Elasticity},
month = {may},
number = {C},
pages = {1--11},
publisher = {Elsevier Science Inc.},
title = {{Container-based virtual elastic clusters}},
url = {https://doi.org/10.1016/j.jss.2017.01.007},
volume = {127},
year = {2017}
}
@article{Zhou2023a,
abstract = {Containers improve the efficiency in application deployment and thus have been widely utilised on Cloud and lately in High Performance Computing (HPC) environments. Containers encapsulate complex programs with their dependencies in isolated environments making applications more compatible and portable. Often HPC systems have higher security levels compared to Cloud systems, which restrict users' ability to customise environments. Therefore, containers on HPC need to include a heavy package of libraries making their size relatively large. These libraries usually are specifically optimised for the hardware, which compromises portability of containers. Per contra, a Cloud container has smaller volume and is more portable. Furthermore, containers would benefit from orchestrators that facilitate deployment and management of containers at a large scale. Cloud systems in practice usually incorporate sophisticated container orchestration mechanisms as opposed to HPC systems. Nevertheless, some solutions to enable container orchestration on HPC systems have been proposed in state of the art. This paper gives a survey and taxonomy of efforts in both containerisation and its orchestration strategies on HPC systems. It highlights differences thereof between Cloud and HPC. Lastly, challenges are discussed and the potentials for research and engineering are envisioned.},
annote = {Tags: HPC, Research, Containerization
CVI: 3

Segunda iteracion},
archivePrefix = {arXiv},
arxivId = {2212.08717},
author = {Zhou, Naweiluo and Zhou, Huan and Hoppe, Dennis},
doi = {10.1109/TSE.2022.3229221},
eprint = {2212.08717},
issn = {19393520},
journal = {IEEE Transactions on Software Engineering},
keywords = {AI,HPC,cloud computing,container,job scheduling,orchestration,resource management},
number = {4},
pages = {2722--2740},
title = {{Containerization for High Performance Computing Systems: Survey and Prospects}},
volume = {49},
year = {2023}
}
@article{10.1007/s11227-022-04848-y,
abstract = {OS-level virtualization (containers) has become a popular alternative to hypervisor-based virtualization. From a system-administration point-of-view, containers enable support for user-defined software stacks, thus freeing users of restrictions imposed by the host's pre-configured software environment. In high performance computing (HPC), containers inspire special interest due to their potentially low overheads on performance. Moreover, they also bring benefits in portability and scientific reproducibility. Despite the potential advantages, the adoption of containers in HPC has been relatively slow, mainly due to specific requirements of the field. These requirements gave rise to various HPC-focused container implementations. Besides unprivileged container execution, they offer different degrees of automation of system-specific optimizations, which are necessary for optimal performance. When we looked into the scientific literature on containers applied to HPC, we were unable to find an up-to-date overview of the state-of-the-art. For this reason, we developed this extensive survey, including 93 carefully selected works. Overall, based on our survey, we argue that issues related to performance overhead are mostly solved. There is, however, a clear trade-off between performance and portability, since optimal performance often depends on host-specific optimizations. A few works propose solutions to mitigate this issue, but there is still room for improvement. Besides, we found surprisingly few works that deal with portability between dedicated HPC systems and public cloud platforms.},
address = {USA},
annote = {Reviewed},
author = {{Keller Tesser}, Rafael and Borin, Edson},
doi = {10.1007/s11227-022-04848-y},
issn = {15730484},
journal = {Journal of Supercomputing},
keywords = {Containers,HPC,High performance computing,OS-level virtualization,Parallel processing,Survey},
number = {5},
pages = {5759--5827},
publisher = {Kluwer Academic Publishers},
title = {{Containers in HPC: a survey}},
url = {https://doi.org/10.1007/s11227-022-04848-y},
volume = {79},
year = {2023}
}
@inproceedings{10.1145/3311790.3396625,
abstract = {Scientific computing needs are growing dramatically with time and are expanding in science domains that were previously not compute intensive. When compute workflows spike well in excess of the capacity of their local compute resource, capacity should be temporarily provisioned from somewhere else to both meet deadlines and to increase scientific output. Public Clouds have become an attractive option due to their ability to be provisioned with minimal advance notice. The available capacity of cost-effective instances is not well understood. This paper presents expanding the IceCube's production HTCondor pool using cost-effective GPU instances in preemptible mode gathered from the three major Cloud providers, namely Amazon Web Services, Microsoft Azure and the Google Cloud Platform. Using this setup, we sustained for a whole workday about 15k GPUs, corresponding to around 170 PFLOP32s, integrating over one EFLOP32 hour worth of science output for a price tag of about $60k. In this paper, we provide the reasoning behind Cloud instance selection, a description of the setup and an analysis of the provisioned resources, as well as a short description of the actual science output of the exercise.},
address = {New York, NY, USA},
author = {Sfiligoi, Igor and Schultz, David and Riedel, Benedikt and Wuerthwein, Frank and Barnet, Steve and Brik, Vladimir},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3311790.3396625},
isbn = {9781450366892},
keywords = {Cloud,GPU,HTCondor,Hybrid-Cloud,IceCube,Multi-Cloud,astrophysics,cost analysis},
pages = {85--90},
publisher = {Association for Computing Machinery},
series = {PEARC '20},
title = {{Demonstrating a Pre-Exascale, Cost-Effective Multi-Cloud Environment for Scientific Computing: Producing a fp32 ExaFLOP hour worth of IceCube simulation data in a single workday}},
url = {https://doi.org/10.1145/3311790.3396625},
year = {2020}
}
@article{Urbah2009,
abstract = {Desktop Grids, such as XtremWeb and BOINC, and Service Grids, such as EGEE, are two different approaches for science communities to gather computing power from a large number of computing resources. Nevertheless, little work has been done to combine these two Grid technologies in order to establish a seamless and vast Grid resource pool. In this paper we present the EGEE Service Grid, the BOINC and XtremWeb Desktop Grids. Then, we present the EDGeS solution to bridge the EGEE Service Grid with the BOINC and XtremWeb Desktop Grids. {\textcopyright} Springer Science+Business Media B.V. 2009.},
author = {Urbah, Etienne and Kacsuk, Peter and Farkas, Zoltan and Fedak, Gilles and Kecskemeti, Gabor and Lodygensky, Oleg and Marosi, Attila and Balaton, Zoltan and Caillat, Gabriel and Gombas, Gabor and Kornafeld, Adam and Kovacs, Jozsef and He, Haiwu and Lovas, Robert},
doi = {10.1007/s10723-009-9137-0},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {BOINC,Bridge,Desktop grid,EDGeS,EGEE,Interface,Interoperation,OGF,Service grid,XWHEP,Xtrem web},
number = {3},
pages = {335--354},
title = {{EDGeS: Bridging EGEE to BOINC and XtremWeb}},
url = {https://doi.org/10.1007/s10723-009-9137-0},
volume = {7},
year = {2009}
}
@article{Ejarque2022,
abstract = {The evolution of High-Performance Computing (HPC) platforms enables the design and execution of progressively larger and more complex workflow applications in these systems. The complexity comes not only from the number of elements that compose the workflows but also from the type of computations they perform. While traditional HPC workflows target simulations and modelling of physical phenomena, current needs require in addition data analytics (DA) and artificial intelligence (AI) tasks. However, the development of these workflows is hampered by the lack of proper programming models and environments that support the integration of HPC, DA, and AI, as well as the lack of tools to easily deploy and execute the workflows in HPC systems. To progress in this direction, this paper presents use cases where complex workflows are required and investigates the main issues to be addressed for the HPC/DA/AI convergence. Based on this study, the paper identifies the challenges of a new workflow platform to manage complex workflows. Finally, it proposes a development approach for such a workflow platform addressing these challenges in two directions: first, by defining a software stack that provides the functionalities to manage these complex workflows; and second, by proposing the HPC Workflow as a Service (HPCWaaS) paradigm, which leverages the software stack to facilitate the reusability of complex workflows in federated HPC infrastructures. Proposals presented in this work are subject to study and development as part of the EuroHPC eFlows4HPC project.},
annote = {Tags: HPC, Research, Parallel
CVI: 3

Segunda iteracion},
archivePrefix = {arXiv},
arxivId = {2204.09287},
author = {Ejarque, Jorge and Badia, Rosa M. and Albertin, Lo{\"{i}}c and Aloisio, Giovanni and Baglione, Enrico and Becerra, Yolanda and Boschert, Stefan and Berlin, Julian R. and D'Anca, Alessandro and Elia, Donatello and Exertier, Fran{\c{c}}ois and Fiore, Sandro and Flich, Jos{\'{e}} and Folch, Arnau and Gibbons, Steven J. and Koldunov, Nikolay and Lordan, Francesc and Lorito, Stefano and L{\o}vholt, Finn and Mac{\'{i}}as, Jorge and Marozzo, Fabrizio and Michelini, Alberto and Monterrubio-Velasco, Marisol and Pienkowska, Marta and de la Puente, Josep and Queralt, Anna and Quintana-Ort{\'{i}}, Enrique S. and Rodr{\'{i}}guez, Juan E. and Romano, Fabrizio and Rossi, Riccardo and Rybicki, Jedrzej and Kupczyk, Miroslaw and Selva, Jacopo and Talia, Domenico and Tonini, Roberto and Trunfio, Paolo and Volpe, Manuela},
doi = {10.1016/j.future.2022.04.014},
eprint = {2204.09287},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Distributed computing,HPC-DA-AI convergence,High performance computing,Parallel programming,Workflow development,Workflow orchestration},
pages = {414--429},
title = {{Enabling dynamic and intelligent workflows for HPC, data analytics, and AI convergence}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001364},
volume = {134},
year = {2022}
}
@article{10.1016/j.future.2022.01.024,
abstract = {With the growing complexity of computational and experimental facilities, many scientific researchers are turning to machine learning (ML) techniques to analyze large scale ensemble data. With complexities such as multi-component workflows, heterogeneous machine architectures, parallel file systems, and batch scheduling, care must be taken to facilitate this analysis in a high performance computing (HPC) environment. In this paper, we present Merlin, a workflow framework to enable large ML-friendly ensembles of scientific HPC simulations. By augmenting traditional HPC with distributed compute technologies, Merlin aims to lower the barrier for scientific subject matter experts to incorporate ML into their analysis. As a producer–consumer workflow model, Merlin enables multi-machine, cross-batch job, dynamically allocated yet persistent workflows capable of utilizing surge-compute resources. Key features of Merlin are a flexible HPC-centric interface, low per-task overhead, multi-tiered fault recovery, and a hierarchical sampling algorithm that allows for O(N) task execution and O(NlnN) task queuing to ensembles of millions of tasks. In addition to Merlin's design, we test the algorithm's performance in an HPC center and demonstrate the ability to enqueue 40 million simulations in 100 s, with a 30 millisecond per-task overhead that is independent of ensemble size. Finally, we describe some example applications that Merlin has enabled on leadership-class HPC resources, such as the ML-augmented optimization of nuclear fusion experiments and the calibration of infectious disease models to study the progression of and possible mitigation strategies for COVID-19.},
address = {NLD},
annote = {Tags: HPC, HTC, Research, Workflow Management Systems, Machine Learning
CVI: 3

Primera iteracion},
archivePrefix = {arXiv},
arxivId = {1912.02892},
author = {Peterson, J. Luc and Bay, Ben and Koning, Joe and Robinson, Peter and Semler, Jessica and White, Jeremy and Anirudh, Rushil and Athey, Kevin and Bremer, Peer Timo and {Di Natale}, Francesco and Fox, David and Gaffney, Jim A. and Jacobs, Sam A. and Kailkhura, Bhavya and Kustowski, Bogdan and Langer, Steven and Spears, Brian and Thiagarajan, Jayaraman and {Van Essen}, Brian and Yeom, Jae Seung},
doi = {10.1016/j.future.2022.01.024},
eprint = {1912.02892},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Distributed computing,High performance computing,High throughput computing,Machine learning,Scientific computing,Workflow management},
number = {C},
pages = {255--268},
publisher = {Elsevier Science Publishers B. V.},
title = {{Enabling machine learning-ready HPC ensembles with Merlin}},
url = {https://doi.org/10.1016/j.future.2022.01.024},
volume = {131},
year = {2022}
}
@article{Gibson2024,
abstract = {This paper presents a massively parallel, cloud-computing framework for the ad hoc evaluation of discrete-event simulation (DES) models to enable broad exploration of the design space for model parameters. Parallel evaluation is enabled through use of a serverless computing environment allowing thousands of simultaneous experiments, on demand, without the need to explicitly provision or manages hardware. A standard Simulation Evaluation application programming interface (API) was designed for evaluating simulation functions that enables language independence between client application and simulation model, encouraging reuse of simulation models for multiple purposes (what-if analysis, ranking and selection, sensitivity analysis, or optimization). Extensions to the Java Simulation Library (JSL)27 enable rapid deployment of models built with the JSL as parameterized serverless functions implementing the Simulation Evaluation API. New Java packages facilitate the calling of any serverless functions that implement the Simulation Evaluation API.},
annote = {Tags: Parallel, Research, Java
CVI: 3

Primera iteracion},
author = {Gibson, Andrew and Rossetti, Manuel D.},
doi = {10.1177/00375497241233284},
issn = {17413133},
journal = {Simulation},
keywords = {Discrete-event simulation,parallel computation,serverless computing,simulation evaluation},
month = {mar},
number = {8},
pages = {789--811},
publisher = {SAGE Publications Ltd STM},
title = {{Enabling massively parallel, ad hoc exploration of the design space for simulation models within a serverless environment}},
url = {https://doi-org.crai.referencistas.com/https://doi.org/10.1177/00375497241233284},
volume = {100},
year = {2024}
}
@inproceedings{10.1145/3603166.3632142,
abstract = {Distributed cyberinfrastructures (CI) pose opportunities and challenges for the execution of scientific workflows, especially in the context of Earth science applications. They provide heterogeneous resources that can meet the needs of the applications that are part of the scientific workflows and provide the necessary performance and scalability to achieve scientific goals. However, the challenge with distributed CI is that it is difficult to find the right resources for the applications and to orchestrate the workflow execution from resource provisioning to job execution to delivering the final results. In some cases, poor choice of resources may result in slow execution or outright failure. In this paper, we present Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) Pegasus, a CI solution built as part of the U.S. National Science Foundation ACCESS program that provides automated execution of scientific applications. We demonstrate Pegasus's capabilities with SOil MOisture SPatial Inference Engine (SOMOSPIE), an earth science multi-component application for fine-grained soil moisture predictions. We identify a roadmap to migrate applications such as SOMOSPIE on ACCESS resources with the support of ACCESS Pegasus, outlining both strengths and weaknesses of this approach.},
address = {New York, NY, USA},
author = {Roa, Camila and Rynge, Mats and Olaya, Paula and Vahi, Karan and Miller, Todd and Griffioen, James and Knuth, Shelley and Goodhue, John and Hudak, David and Romanella, Alana and Llamas, Ricardo and Vargas, Rodrigo and Livny, Miron and Deelman, Ewa and Taufer, Michela},
booktitle = {16th IEEE/ACM International Conference on Utility and Cloud Computing, UCC 2023},
doi = {10.1145/3603166.3632142},
isbn = {9798400702341},
keywords = {containers,high throughput computing,machine learning,soil moisture,workflows},
publisher = {Association for Computing Machinery},
series = {UCC '23},
title = {{End-to-end Integration of Scientific Workflows on Distributed Cyberinfrastructures: Challenges and Lessons Learned with an Earth Science Application}},
url = {https://doi.org/10.1145/3603166.3632142},
year = {2023}
}
@article{Papadimitriou2021,
abstract = {With the increased prevalence of employing workflows for scientific computing and a push towards exascale computing, it has become paramount that we are able to analyze characteristics of scientific applications to better understand their impact on the underlying infrastructure and vice-versa. Such analysis can help drive the design, development, and optimization of these next generation systems and solutions. In this paper, we present the architecture, integrated with existing well-established and newly developed tools, to collect online performance statistics of workflow executions from various, heterogeneous sources and publish them in a distributed database (Elasticsearch). Using this architecture, we are able to correlate online workflow performance data, with data from the underlying infrastructure, and present them in a useful and intuitive way via an online dashboard. We have validated our approach by executing two classes of real-world workflows, both under normal and anomalous conditions. The first is an I/O-intensive genome analysis workflow; the second, a CPU- and memory-intensive material science workflow. Based on the data collected in Elasticsearch, we are able to demonstrate that we can correctly identify anomalies that we injected. The resulting end-to-end data collection of workflow performance data is an important resource of training data for automated machine learning analysis.},
author = {Papadimitriou, George and Wang, Cong and Vahi, Karan and da Silva, Rafael Ferreira and Mandal, Anirban and Liu, Zhengchun and Mayani, Rajiv and Rynge, Mats and Kiran, Mariam and Lynch, Vickie E. and Kettimuthu, Rajkumar and Deelman, Ewa and Vetter, Jeffrey S. and Foster, Ian},
doi = {10.1016/j.future.2020.11.024},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Extreme scale,Online performance monitoring,Scientific workflows},
pages = {387--400},
title = {{End-to-end online performance data capture and analysis for scientific workflows}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330570},
volume = {117},
year = {2021}
}
@article{Krasovec2019,
abstract = {Scientific computing has evolved considerably in recent years. Scientific applications have become more complex and require an increasing number of computing resources to perform on a large scale. Grid computing has become widely used and is the chosen infrastructure for many scientific calculations and projects, even though it demands a steep learning curve. The computing and storage resources in the Grid are limited, heterogeneous and often overloaded. This heterogeneity is not only present in the hardware setups, but also in the software composition, where configuration permissions are limited. It also has a negative effect on the portability of scientific applications. The use of Cloud resources could eliminate those constraints. In the Cloud, resources are provisioned on demand and can be scaled up and down, while scientists can easily customize their execution environments in the form of virtual machines. Extending the Grid with Cloud resources would improve the utilization of shared resources and would enable the use of additional resources when the Grid resources are overloaded – known as Cloud bursting. We propose an integration model of the Grid and the Cloud using the HTCondor batch system and the NorduGrid ARC middleware. This model enables batch job execution in any public or private Cloud by deploying a virtualized Grid cluster using the ARC middleware - PaaS model for running Grid applications. An evaluation of the virtual Grid cluster was made and compared with the physical one by running NAMD simulations.},
author = {Kra{\v{s}}ovec, Barbara and Filip{\v{c}}i{\v{c}}, Andrej},
doi = {10.1007/s10723-018-09472-w},
issn = {15729184},
journal = {Journal of Grid Computing},
keywords = {ARC,Cloud computing,Distributed computing,Grid computing,HPC,Interoperability,NAMD,Parallel applications},
number = {1},
pages = {119--135},
title = {{Enhancing the Grid with Cloud Computing: ARC-CC: ARC Cluster in the Cloud}},
url = {https://doi.org/10.1007/s10723-018-09472-w},
volume = {17},
year = {2019}
}
@inproceedings{Pavlovikj2014,
abstract = {Complex and large-scale applications in different scientific disciplines are often represented as a set of independent tasks, known as workflows. Many scientific workflows have intensive resource requirements. Therefore, different distributed platforms, including campus clusters, grids and clouds are used for efficient execution of these workflows. In this paper we examine the performance and the cost of running the Pegasus Workflow Management System (Pegasus WMS) implementation of blast2cap3, the protein-guided assembly approach, on three different execution platforms: Sandhills, the University of Nebraska Campus Cluster, the academic grid Open Science Gird (OSG), and the commercial cloud Amazon EC2. Furthermore, the behavior of the blast2cap3 workflow was tested with different number of tasks. For the used workflows and execution platforms, we perform multiple runs in order to compare the total workflow running time, as well as the different resource availability over time. Additionally, for the most interesting runs, the number of running versus the number of idle jobs over time was analyzed for each platform. The performed experiments show that using the Pegasus WMS implementation of blast2cap3 with more than 100 tasks significantly reduces the running time for all execution platforms. In general, for our workflow, better performance and resource usage were achieved when Amazon EC2 was used as an execution platform. However, due to the Amazon EC2 cost, the academic distributed systems can sometimes be a good alternative and have excellent performance, especially when there are plenty of resources available. Copyright 2014 ACM.},
address = {New York, NY, USA},
author = {Pavlovikj, Natasha and Begcy, Kevin and Behera, Sairam and Campbell, Malachy and Walia, Harkamal and Deogun, Jitender S.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2616498.2616551},
isbn = {9781450328937},
keywords = {Amazon EC2,Blast2cap3,Campus cluster,Open science grid,Pegasus workflow management system,Scientific workflow},
publisher = {Association for Computing Machinery},
series = {XSEDE '14},
title = {{Evaluating distributed platforms for protein-guided scientific workflow}},
url = {https://doi.org/10.1145/2616498.2616551},
year = {2014}
}
@inproceedings{Mandal2013,
abstract = {This paper presents a performance evaluation of scientific workflows on networked cloud systems with particular emphasis on evaluating the effect of provisioned network bandwidth on application I/O performance. The experiments were run on ExoGENI, a widely distributed networked infrastructure as a service (NIaaS) testbed. ExoGENI orchestrates a federation of independent cloud sites located around the world along with backbone circuit providers. The evaluation used a representative data-intensive scientific work-flow application called Montage. The application was deployed on a virtualized HTCondor environment provisioned dynamically from the ExoGENI networked cloud testbed, and managed by the Pegasus workflow manager. The results of our experiments show the effect of modifying provisioned network bandwidth on disk I/O throughput and workflow execution time. The marginal benefit as perceived by the workflow reduces as the network bandwidth allocation increases to a point where disk I/O saturates. There is little or no benefit from increasing network bandwidth beyond this inflection point. The results also underline the importance of network and I/O performance isolation for predictable application performance, and are applicable for general data-intensive workloads. Insights from this work will also be useful for real-time monitoring, application steering and infrastructure planning for data-intensive workloads on networked cloud platforms. Copyright 2013 ACM.},
address = {New York, NY, USA},
author = {Mandal, Anirban and Ruth, Paul and Baldin, Ilya and Xin, Yufeng and Castillo, Claris and Rynge, Mats and Deelman, Ewa},
booktitle = {Proc. of NDM 2013: 3rd Int. Workshop on Network-Aware Data Management - Held in Conjunction with SC 2013: The Int. Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1145/2534695.2534698},
isbn = {9781450325226},
keywords = {I/O performance,Networked clouds,Performance evaluation,Performance monitoring,Scientific workflows},
publisher = {Association for Computing Machinery},
series = {NDM '13},
title = {{Evaluating I/O aware network management for scientific workflows on networked clouds}},
url = {https://doi.org/10.1145/2534695.2534698},
year = {2013}
}
@inproceedings{McGough2018,
abstract = {High Throughput Computing allows workloads of many thousands of tasks to be performed efficiently over many distributed resources and frees the user from the laborious process of managing task deployment, execution and result collection. However, in many cases the High Throughput Computing system is comprised from volunteer computational resources where tasks may be evicted by the owner of the resource. This has two main disadvantages. First, tasks may take longer to run as they may require multiple deployments before finally obtaining enough time on a resource to complete. Second, the wasted computation time will lead to wasted energy. We may be able to reduce the effect of the first disadvantage here by submitting multiple replicas of the task and take the results from the first one to complete. This, though, could lead to a significant increase in energy consumption. Thus we desire to only ever submit the minimum number of replicas required to run the task in the allocated time whilst simultaneously minimising energy. In this work we evaluate the use of fixed replica counts and Reinforcement Learning on the proportion of task which fail to finish in a given time-frame and the energy consumed by the system.},
address = {New York, NY, USA},
author = {McGough, A. Stephen and Forshaw, Matthew},
booktitle = {ICPE 2018 - Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
doi = {10.1145/3185768.3186313},
isbn = {9781450356299},
keywords = {Machine Learning, Energy,Simulation,Trace-Driven},
pages = {85--90},
publisher = {Association for Computing Machinery},
series = {ICPE '18},
title = {{Evaluation of energy consumption of replicated tasks in a volunteer computing environment}},
url = {https://doi.org/10.1145/3185768.3186313},
volume = {2018-Janua},
year = {2018}
}
@inproceedings{Madduri2013,
abstract = {We describe Globus Genomics, a system that we have developed for rapid analysis of large quantities of next-generation sequencing (NGS) genomic data. This system is notable for its high degree of end-to-end automation, which encompasses every stage of the data analysis pipeline from initial data access (from remote sequencing center or database, by the Globus Online file transfer system) to on-demand resource acquisition (on Amazon EC2, via the Globus Provision cloud manager); specification, configuration, and reuse of multi-step processing pipelines (via the Galaxy workflow system); and efficient scheduling of these pipelines over many processors (via the Condor scheduler). The system allows biomedical researchers to perform rapid analysis of large NGS datasets using just a web browser in a fully automated manner, without software installation. {\textcopyright} 2013 by the Association for Computing Machinery, Inc.},
address = {New York, NY, USA},
author = {Madduri, Ravi K. and Dave, Paul and Sulakhe, Dinanath and Lacinski, Lukasz and Liu, Bo and Foster, Ian T.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2484762.2484827},
isbn = {9781450321709},
keywords = {Cloud,HPC,HTC,NGS,Workflows},
publisher = {Association for Computing Machinery},
series = {XSEDE '13},
title = {{Experiences in building a next-generation sequencing analysis service using galaxy, globus online and amazon web service}},
url = {https://doi.org/10.1145/2484762.2484827},
year = {2013}
}
@inproceedings{Vockler2011,
abstract = {Clouds are rapidly becoming an important platform for scientific applications. In this paper we describe our experiences running a scientific workflow application in the cloud. The application was developed to process astronomy data released by the Kepler project, a NASA mission to search for Earth-like planets orbiting other stars. This workflow was deployed across multiple clouds using the Pegasus Workflow Management System. The clouds used include several sites within the FutureGrid, NERSC's Magellan cloud, and Amazon EC2. We describe how the application was deployed, evaluate its performance executing in different clouds (based on Nimbus, Eucalyptus, and EC2), and discuss the challenges of deploying and executing workflows in a cloud environment. We also demonstrate how Pegasus was able to support sky computing by executing a single workflow across multiple cloud infrastructures simultaneously. {\textcopyright} 2011 ACM.},
address = {New York, NY, USA},
author = {V{\"{o}}ckler, Jens S{\"{o}}nke and Juve, Gideon and Deelman, Ewa and Rynge, Mats and Berriman, Bruce},
booktitle = {ScienceCloud'11 - Proceedings of the 2nd International Workshop on Scientific Cloud Computing},
doi = {10.1145/1996109.1996114},
isbn = {9781450306997},
keywords = {EC2,cross cloud computing,eucalyptus,experience,futuregrid,magellan,nimbus,pegasus,periodigrams,sky computing,workflow},
pages = {15--24},
publisher = {Association for Computing Machinery},
series = {ScienceCloud '11},
title = {{Experiences using cloud computing for a scientific workflow application}},
url = {https://doi.org/10.1145/1996109.1996114},
year = {2011}
}
@article{Maimour2004,
abstract = {The ever growing needs for computation power and accesses to critical resources have launched in a very short time a large number of grid projects and many realizations have been done on dedicated network infrastructures. On Internet-based infrastructures, however, there are very few distributed or interactive applications (MPI, DIS, HLA, remote visualization) because of insufficient end-to-end performances (bandwidth, latency, for example) to support such an interactivity. For the moment, computing resources and network resources are viewed separately in the Grid architecture and we believe this is the main bottleneck for achieving end-to-end performances. In this paper, we promote the idea of a Grid infrastructure able to adapt to the application's needs and thus define the idea of application-aware Grid infrastructures where the network infrastructure is tightly involved in both the communication and processing process. We report on our early experiences in building application-aware components based on active networking technologies for providing a low latency and a low overhead multicast framework for applications running on a computational Grid. Performance results from both simulations and implementation prototypes confirm that introducing application-aware components at specific location in the network infrastructure can succeed in providing not only performances for the end-users but also new perspectives in building a communication framework for computational Grids. {\textcopyright} 2004 Kluwer Academic Publishers.},
author = {Maimour, M. and Pham, C.},
doi = {10.1007/s10723-004-2075-y},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Active network,Programmable grid,Reliable multicast},
number = {1},
pages = {71--83},
title = {{Experimenting active reliable multicast on application-aware grids}},
url = {https://doi.org/10.1007/s10723-004-2075-y},
volume = {2},
year = {2004}
}
@inproceedings{Shakil2018,
abstract = {Cloud computing is a cost-effective way for start-up life sciences laboratories to store and manage their data. However, in many instances the data stored over the cloud could be redundant which makes cloud-based data management inefficient and costly because one has to pay for every byte of data stored over the cloud. Here, we tested efficient management of data generated by an electron cryo-microscopy (cryoEM) lab on a cloud-based environment. The test data was obtained from cryoEM repository EMPIAR. All the images were subjected to an in-house parallelized version of principal component analysis. An efficient cloud-based MapReduce modality was used for parallelization. We showed that large data in order of terabytes could be efficiently reduced to its minimal essential self in a cost-effective scalable manner. Furthermore, on-spot instance on Amazon EC2 was shown to reduce costs by a margin of about 27 percent. This approach could be scaled to data of any large volume and type.},
address = {New York, NY, USA},
author = {Shakil, Kashish Ara and Alam, Mansaf and Shakeel, Shabih and Ora, Ari and Khan, Samiya},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3232174.3232177},
isbn = {9781450364232},
keywords = {Big Data,Cloud Computing,Cryo-image data,Data reduction,PCA},
pages = {61--66},
publisher = {Association for Computing Machinery},
series = {ICCMB '18},
title = {{Exploiting data reduction principles in cloud-based data management for cryo-image data}},
url = {https://doi.org/10.1145/3232174.3232177},
year = {2018}
}
@article{10.1016/j.jpdc.2019.08.002,
abstract = {Many bioinformatic applications require to exploit the capabilities of several computational resources to effectively access and process large and distributed datasets. In this context, Grid computing has been largely used to face unprecedented challenges in Computational Biology, at the cost of complex workarounds needed to make applications successfully running. The Grid computing paradigm, in fact, has always suffered from a lack of flexibility. Although this has been partially solved by Cloud computing, the on-demand approach is way distant from the original idea of volunteering computing that boosted the Grid paradigm. A solution to outpace the impossibility of creating custom environments for running applications in Grid is represented by the containerization technology. In this paper, we describe our experience in exploiting a Docker-based approach to run in a Grid environment a novel, computationally intensive, bioinformatic application, which models the DNA spatial conformation inside the nucleus of eukaryotic cells. Results assess the feasibility of this approach in terms of performance and efforts to run large experiments.},
address = {USA},
author = {Merelli, Ivan and Fornari, Federico and Tordini, Fabio and D'Agostino, Daniele and Aldinucci, Marco and Cesini, Daniele},
doi = {10.1016/j.jpdc.2019.08.002},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Chromatin conformation,Computational Biology,Data modelling,Docker containers,Grid computing},
number = {C},
pages = {116--127},
publisher = {Academic Press, Inc.},
title = {{Exploiting Docker containers over Grid computing for a comprehensive study of chromatin conformation in different cell types}},
url = {https://doi.org/10.1016/j.jpdc.2019.08.002},
volume = {134},
year = {2019}
}
@inproceedings{Kupsch2010,
abstract = {Clouds and Grids offer significant challenges to providing secure infrastructure software. As part of a our effort to secure such middleware, we present First Principles Vulnerability Assessment (FPVA), a new analyst-centric (manual) technique that aims to focus the analyst's attention on the parts of the software system and its resources that are most likely to contain vulnerabilities that would provide access to high-value assets. FPVA finds new threats to a system and is not dependent on a list of known threats. Manual assessment is labor-intensive, making the use of automated assessment tools quite attractive. We compared the results of FPVA to those of the top commercial tools, providing the first significant evaluation of these tools against a real-world known collection of serious vulnerabilities. While these tools can find common problems in a program's source code, they miss a significant number of serious vulnerabilities found by FPVA. We are now using the results of this comparison study to guide our future research into improving automated software assessment. {\textcopyright} 2010 ACM.},
address = {New York, NY, USA},
author = {Kupsch, James A. and Miller, Barton P. and Heymann, Elisa and C{\'{e}}sar, Eduardo},
booktitle = {Proceedings of the ACM Conference on Computer and Communications Security},
doi = {10.1145/1866835.1866852},
isbn = {9781450300896},
issn = {15437221},
keywords = {Auditing,Tiger team},
pages = {87--92},
publisher = {Association for Computing Machinery},
series = {CCSW '10},
title = {{First principles vulnerability assessment}},
url = {https://doi.org/10.1145/1866835.1866852},
year = {2010}
}
@article{Viil2018,
abstract = {Scientific workflows have become a standardized way for scientists to represent a set of tasks to overcome/solve a certain scientific problem. Usually these workflows consist of numerous CPU and I/O-intensive jobs that are executed using workflow management systems (WfMS), on clouds, grids, supercomputers, etc. Previously, it was shown that using k-way partitioning to distribute a workflow's tasks between multiple machines in the cloud reduces the overall data communication and therefore lowers the cost of the bandwidth usage. A framework was built to automate this process of partitioning and execution of any workflow submitted by a scientist that is meant to be run on Pegasus WfMS, in the cloud, with ease. The framework provisions the instances in the cloud using CloudML, configures and installs all the software needed for the execution, partitions and runs the provided scientific workflow, also showing the estimated makespan and cost.},
author = {Viil, Jaagup and Srirama, Satish Narayana},
doi = {10.1007/s11227-018-2296-7},
issn = {15730484},
journal = {Journal of Supercomputing},
keywords = {Cloud computing,CloudML,Framework,METIS,Partitioning,Scientific workflows},
number = {6},
pages = {2656--2683},
title = {{Framework for automated partitioning and execution of scientific workflows in the cloud}},
url = {https://doi.org/10.1007/s11227-018-2296-7},
volume = {74},
year = {2018}
}
@article{Zhang2014,
abstract = {Scalability and fault-tolerance are two fundamental challenges for all distributed computing at Internet scale. Despite many recent advances from both academia and industry, these two problems are still far from settled. In this paper, we present Fuxi, a resource management and job scheduling system that is capable of handling the kind of workload at Alibaba where hundreds of terabytes of data are generated and analyzed everyday to help optimize the company's business operations and user experiences. We employ several novel techniques to enable Fuxi to perform efficient scheduling of hundreds of thousands of concurrent tasks over large clusters with thousands of nodes: 1) an incremental resource management protocol that supports multi-dimensional resource allocation and data locality; 2) user-transparent failure recovery where failures of any Fuxi components will not impact the execution of user jobs; and 3) an effective detection mechanism and a multi-level blacklisting scheme that prevents them from affecting job execution. Our evaluation results demonstrate that 95% and 91% scheduled CPU/memory utilization can be fulfilled under synthetic workloads, and Fuxi is capable of achieving 2.36T-B/minute throughput in GraySort. Additionally, the same Fuxi job only experiences approximately 16% slowdown under a 5% fault-injection rate. The slowdown only grows to 20% when we double the fault-injection rate to 10%. Fuxi has been deployed in our production environment since 2009, and it now manages hundreds of thousands of server nodes. {\textcopyright} 2014 VLDB Endowment 2150-8097/14/08.},
annote = {No se encuentran las palabras clave},
author = {Zhang, Zhuo and Li, Chao and Tao, Yangyu and Yangy, Renyu and Tang, Hong and Xu, Jie},
doi = {10.14778/2733004.2733012},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
month = {aug},
number = {13},
pages = {1393--1404},
publisher = {VLDB Endowment},
title = {{Fuxi: A fault-tolerant resource management and job scheduling system at internet scale}},
url = {https://doi.org/10.14778/2733004.2733012},
volume = {7},
year = {2014}
}
@inproceedings{10.1145/1851476.1851588,
abstract = {Voluntary Computing systems or Desktop Grids (DGs) en- able sharing of commodity computing resources across the globe and have gained tremendous popularity among sci- entific research communities. Data management is one of the major challenges of adopting the Voluntary Comput- ing paradigm for large data-intensive applications. To date, middleware for supporting such applications either lacks an eficient cooperative data distribution scheme or cannot eas- ily accommodate existing data-intensive applications due to the requirement for using middleware-specific APIs. To address this challenge, in this paper we introduce Gator- Share, a data management framework that ofiers a file sys- tem interface and an extensible architecture designed to sup- port multiple data transfer protocols, including BitTorrent, based on which we implement a cooperative data distribu- tion service for DGs. It eases the integration with Desktop Grids and enables high-throughput data management for unmodified data-intensive applications. To improve the per- formance of BitTorrent in Desktop Grids, we have enhanced BitTorrent by making it fully decentralized and capable of supporting partial file downloading in an on-demand fash- ion. To justify this approach we present a quantitative evalua- tion of the framework in terms of data distribution eficiency. Experimental results show that the framework significantly improves the data dissemination performance for unmod- ified data-intensive applications compared to a traditional client/server architecture. Copyright 2010 ACM.},
address = {New York, NY, USA},
author = {Xu, Jiangyan and Figueiredo, Renato},
booktitle = {HPDC 2010 - Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
doi = {10.1145/1851476.1851588},
isbn = {9781605589428},
keywords = {BitTorrent,DataMan- agement,Desktop grids,High-throughput computing,Voluntary computing},
pages = {776--786},
publisher = {Association for Computing Machinery},
series = {HPDC '10},
title = {{GatorShare: A file system framework for high-throughput data management}},
url = {https://doi.org/10.1145/1851476.1851588},
year = {2010}
}
@inproceedings{10.1145/3086567.3086570,
abstract = {While some properties of SaaS have long been leveraged in science, particularly in science gateways, there is yet to be widespread adoption of SaaS models. For example, few scientific SaaS providers are publicly available, few leverage elastic cloud platforms, and none-with the exception of Globus-implement subscription-based models to recoup operations costs. Globus has employed the SaaS model for seven years and is fast approaching subscription levels that will support long-term sustainability. In this paper we discuss the SaaS paradigm and explore its suitability to scientific domains. We then describe how production Globus SaaS services are implemented, deployed, and operated.},
address = {New York, NY, USA},
author = {Allen, Bryce and Ananthakrishnan, Rachana and Chard, Kyle and Foster, Ian and Madduri, Ravi and Pruyne, Jim and Rosen, Stephen and Tuecke, Steve},
booktitle = {ScienceCloud 2017 - Proceedings of the 8th Workshop on Scientific Cloud Computing, co-located with HPDC 2017},
doi = {10.1145/3086567.3086570},
isbn = {9781450350211},
keywords = {Globus,Research data management,Science as a service,research data management,science as a service},
pages = {25--32},
publisher = {Association for Computing Machinery},
series = {ScienceCloud '17},
title = {{Globus: A case study in software as a service for scientists}},
url = {https://doi.org/10.1145/3086567.3086570},
year = {2017}
}
@article{Rood2009,
abstract = {The frequent and volatile unavailability of volunteer-based Grid computing resources challenges Grid schedulers to make effective job placements. The manner in which host resources become unavailable will have different effects on different jobs, depending on their runtime and their ability to be checkpointed or replicated. A multi-state availability model can help improve scheduling performance by capturing the various ways a resource may be available or unavailable to the Grid. This paper uses a multi-state model and analyzes a machine availability trace in terms of that model. Several prediction techniques then forecast resource transitions into the model's states. We analyze the accuracy of our predictors, which outperform existing approaches. We also propose and study several classes of schedulers that utilize the predictions, and a method for combining scheduling factors. We characterize the inherent tradeoff between job makespan and the number of evictions due to failure, and demonstrate how our schedulers can navigate this tradeoff under various scenarios. Lastly, we propose job replication techniques, which our schedulers utilize to replicate those jobs that are most likely to fail. Our replication strategies outperform others, as measured by improved makespan and fewer redundant operations. In particular, we define a new metric for replication efficiency, and demonstrate that our multi-state availability predictor can provide information that allows our schedulers to be more efficient than others that blindly replicate all jobs or some static percentage of jobs. {\textcopyright} Springer Science + Business Media B.V. 2009.},
author = {Rood, Brent and Lewis, Michael J.},
doi = {10.1007/s10723-009-9135-2},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Availability,Characterization,Multi-state,Prediction,Replication,Scheduling},
number = {4},
pages = {479--500},
title = {{Grid resource availability prediction-based scheduling and task replication}},
url = {https://doi.org/10.1007/s10723-009-9135-2},
volume = {7},
year = {2009}
}
@inproceedings{Mustafee2010,
abstract = {Collaborative research has facilitated the development of distributed systems that provide users nontrivial access to geographically dispersed resources that are administered in multiple computer domains. The term grid computing is popularly used to refer to such distributed systems. Scientific simulations have traditionally been the primary benefactor of grid computing. The application of this technology to simulation in industry has, however, been negligible. This research investigates grid technology in the context of Commercial Simulation Packages (CSPs). Towards this end, the paper identifies (a) six CSPspecific grid services, (b) identifies grid middleware that could be used to provide the CSP-specific grid services, and (c) list CSPs that include vendor-specific solutions for these grid services. The authors hope that this research will lead to an increased awareness of the potential of grid computing among simulation end users and CSP vendors. {\textcopyright}2010 IEEE.},
author = {Mustafee, Navonil and Taylor, Simon J.E.},
booktitle = {Proceedings - Winter Simulation Conference},
doi = {10.1109/WSC.2010.5679122},
isbn = {9781424498666},
issn = {08917736},
keywords = {Biological system modeling,Collaboration,Computational modeling,Grid computing,Industries,Middleware,Software},
pages = {665--676},
publisher = {Winter Simulation Conference},
series = {WSC '10},
title = {{Grid services for commercial simulation packages}},
year = {2010}
}
@inproceedings{10.1145/2608029.2608035,
abstract = {This paper describes the use of a distributed cloud computing system for high energy physics (HEP) applications. The system is composed of IaaS clouds integrated into a unified infrastructure that has been in production for over two years. It continues to expand in scale and sites, encompassing more than twenty clouds on three continents. We are prototyping a new context-aware architecture that enables the virtual machines to make connections to both software and data repositories based on geolocation information. The new design will significantly enhance the ability of the system to scale to higher workloads and run data-intensive applications. We review the operation of the production system and describe our work towards a context-aware cloud system. Copyright 2014 ACM.},
address = {New York, NY, USA},
author = {Berghaus, Frank and Desmarais, Ron and Gable, Ian and Leavett-Brown, Colin and Paterson, Michael and Sobie, Randall J. and Taylor, Ryan and Charbonneau, Andre},
booktitle = {ScienceCloud 2014 - Proceedings of the 2014 ACM International Workshop on Scientific Cloud Computing, Co-located with HPDC 2014},
doi = {10.1145/2608029.2608035},
isbn = {9781450329118},
keywords = {Cloud computing,Grid of clouds,Scientific applications},
pages = {41--45},
publisher = {Association for Computing Machinery},
series = {ScienceCloud '14},
title = {{HEP computing in a context-aware cloud environment}},
url = {https://doi.org/10.1145/2608029.2608035},
year = {2014}
}
@inproceedings{Thapa2018,
abstract = {We present a system that allows individual researchers and virtual organizations (VOs) to access allocations on Stampede2 and Bridges through the Open Science Grid (OSG), a national grid infrastructure for running high throughput computing (HTC) tasks. Using this system, VOs and researchers are able to run larger workflows than can be done with OSG resources alone. This system allows a VO or user to run on XSEDE resources (with their allocation) using the same framework used with OSG resources. The system consists of two parts: the compute element (CE) that routes workloads to the appropriate user accounts and allocation on XSEDE resources, and simulated access to the CernVM Filesystem (CVMFS) servers used by OSG and VOs to distribute software and data. This allows jobs submitted through this system to work on a homogeneous environment regardless of whether they run on XSEDE HPC resources (like Stampede2 and Bridges) or OSG.},
address = {New York, NY, USA},
author = {Thapa, Suchandra and Hufnagel, Dirk and Gardner, Robert W. and Lesny, David and Herner, Kenneth and Rynge, Mats},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3219104.3219157},
isbn = {9781450364461},
keywords = {ATLAS,Bridges,CMS,CVMFS,Distributed data access,OSG,PSC,Stampede2,TACC,XSEDE},
publisher = {Association for Computing Machinery},
series = {PEARC '18},
title = {{Homogenizing OSG and XSEDE: Providing access to XSEDE allocations through OSG infrastructure}},
url = {https://doi.org/10.1145/3219104.3219157},
year = {2018}
}
@inproceedings{10.1145/2465848.2465850,
abstract = {This paper describes the use of a distributed cloud computing system for high-throughput computing (HTC) scientific applications. The distributed cloud computing system is composed of a number of separate Infrastructure-as-a-Service (IaaS) clouds that are utilized in a unified infrastructure. The distributed cloud has been in production-quality operation for two years with approximately 500,000 completed jobs where a typical workload has 500 simultaneous embarrassingly-parallel jobs that run for approximately 12 hours. We review the design and implementation of the system which is based on pre-existing components and a number of custom components. We discuss the operation of the system, and describe our plans for the expansion to more sites and increased computing capacity. {\textcopyright} 2013 ACM.},
address = {New York, NY, USA},
author = {Sobie, Randall and Agarwal, Ashok and Gable, Ian and Leavett-Brown, Colin and Paterson, Michael and Taylor, Ryan and Charbonneau, Andre and Impey, Roger and Podiama, Wayne},
booktitle = {ScienceCloud 2013 - Proceedings of the 4th ACM Workshop on Scientific Cloud Computing},
doi = {10.1145/2465848.2465850},
isbn = {9781450319799},
keywords = {cloud computing},
pages = {45--51},
publisher = {Association for Computing Machinery},
series = {Science Cloud '13},
title = {{HTC scientific computing in a distributed cloud environment}},
url = {https://doi.org/10.1145/2465848.2465850},
year = {2013}
}
@article{Mateescu2011,
abstract = {We introduce a hybrid High Performance Computing (HPC) infrastructure architecture that provides predictable execution of scientific applications, and scales from a single resource to multiple resources, with different ownership, policy, and geographic locations. We identify three paradigms in the evolution of HPC and high-throughput computing: owner-centric HPC (traditional), Grid computing, and Cloud computing. After analyzing the synergies among HPC, Grid and Cloud computing, we argue for an architecture that combines the benefits of these technologies. We call the building block of this architecture, Elastic Cluster. We describe the concept of Elastic Cluster and show how it can be used to achieve effective and predictable execution of HPC workloads. Then we discuss implementation aspects, and propose a new distributed information system design that combines features of distributed hash tables and relational databases. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Mateescu, Gabriel and Gentzsch, Wolfgang and Ribbens, Calvin J.},
doi = {10.1016/j.future.2010.11.003},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud bursting,Cloud computing,Distributed hash table,Distributed systems,HPC,Scheduling,Statistical reservation,Virtualization},
number = {5},
pages = {440--453},
title = {{Hybrid Computing-Where HPC meets grid and Cloud Computing}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1000213X},
volume = {27},
year = {2011}
}
@article{10.1016/j.jss.2016.05.027,
abstract = {High performance computing (HPC) has crossed the Petaflop mark and is reaching the Exaflop range quickly. The exascale system is projected to have millions of nodes, with thousands of cores for each node. At such an extreme scale, the substantial amount of concurrency can cause a critical contention issue for I/O system. This study proposes a dynamically coordinated I/O architecture for addressing some of the limitations that current parallel file systems and storage architectures are facing with very large-scale systems. The fundamental idea is to coordinate I/O accesses according to the topology/profile of the infrastructure, the load metrics, and the I/O demands of each application. The measurements have shown that by using IKAROS approach we can fully utilize the provided I/O and network resources, minimize disk and network contention, and achieve better performance.},
address = {USA},
author = {Filippidis, Christos and Tsanakas, Panayiotis and Cotronis, Yiannis},
doi = {10.1016/j.jss.2016.05.027},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Data management,Distributed systems,Exascale systems,GPFS,Grid computing,High performance computing,Parallel file systems,Soho-NAS,Storage},
number = {C},
pages = {277--287},
publisher = {Elsevier Science Inc.},
title = {{IKAROS: A scalable I/O framework for high-performance computing systems}},
url = {https://doi.org/10.1016/j.jss.2016.05.027},
volume = {118},
year = {2016}
}
@inproceedings{10.1145/1383422.1383439,
abstract = {Self-configuring virtual networks rely on structured P2P routing to provide seamless connectivity among nodes through overlay routing of virtual IP packets, support decentralized hole-punching to establish bi-directional communication links among nodes behind network address translators, and dynamic configuration of virtual IP addresses. Our experiences with deployments of virtual networks in support of wide-area overlays of virtual workstations (WOWs) reveal that connectivity constraints imposed by symmetric NATs and by Internet route outages often hinder P2P overlay structure maintenance and routability, subsequently limiting the ability of WOWs to deliver high-throughput computing through aggregation of resources in different domains. In this paper, we describe and evaluate two novel approaches which are generally applicable and fully decentralized, and show that they improve routability of structured P2P networks in such connectivity constrained environments: (1) a fault-tolerant routing algorithm based on simulated annealing from optimization theory, and (2) tunneling of connections between adjacent nodes (in the P2P identifier space) over common neighbors when direct communication is not possible. Simulation-based analyses show that when pairs of nodes only have 70% chance of being able to communicate directly, the described approaches improve all-to-all routability of the network horn 90% to 99%. We have implemented these techniques in the IP-over-P2P (IPOP) virtual network and have conducted experiments with a 80-node WOW Condor pool, demonstrating that, at 81% probability of establishing a pair-wise connection, annealing and tunneling combined allow all nodes to be connected to the pool, compared to only 160 nodes in the absence of these techniques. Copyright 2008 ACM.},
address = {New York, NY, USA},
author = {Ganguly, Arijit and {Oscar Boykin}, P. and Wolinksky, David I. and Figueiredo, Renato J.},
booktitle = {Proceedings of the 17th International Symposium on High Performance Distributed Computing 2008, HPDC'08},
doi = {10.1145/1383422.1383439},
isbn = {9781595939975},
keywords = {DHT,Overlay,P2P,Virtual network},
pages = {129--140},
publisher = {Association for Computing Machinery},
series = {HPDC '08},
title = {{Improving peer Connectivity in wide-area overlays of virtual workstations}},
url = {https://doi.org/10.1145/1383422.1383439},
year = {2008}
}
@article{LePiane2024,
abstract = {This perspective article explores the convergence of advanced digital technologies, including high-performance computing (HPC), artificial intelligence, machine learning, and sophisticated data management workflows. The primary objective is to enhance the accessibility of multiscale simulations and their integration with other computational techniques, thereby advancing the field of nanomaterials technologies. The proposed approach relies on key strategies and digital technologies employed to achieve efficient and innovative materials discovery, emphasizing a fully digital, data-centric methodology. The integration of methodologies rooted in knowledge and structured information management serves as a foundational element, establishing a framework for representing materials-related information and ensuring interoperability across a diverse range of tools. The paper explores the distinctive features of digital and data-centric approaches and technologies for materials development. It highlights the role of digital twins in research, particularly in the realm of nanomaterials development and examines the impact of knowledge engineering in establishing data and information standards to facilitate interoperability. Furthermore, the paper explores the role of deployment technologies in managing HPC infrastructures. It also addresses the pairing of these technologies with user-friendly development tools to support the adoption of digital methodologies in advanced research.},
annote = {Tags: HPC, Extension, Machine Learning
CVI: 3

Primera iteracion},
author = {{Le Piane}, Fabio and Vozza, Mario and Baldoni, Matteo and Mercuri, Francesco},
doi = {10.3762/BJNANO.15.119},
issn = {21904286},
journal = {Beilstein Journal of Nanotechnology},
keywords = {HPC,artificial intelligence,high-performance computing,machine learning,materials modelling,multiscale modelling,nanomaterials,semantic data management},
pages = {1498--1521},
publisher = {Beilstein-Institut},
title = {{Integrating high-performance computing, machine learning, data management workflows, and infrastructures for multiscale simulations and nanomaterials technologies}},
url = {https://doi.org/10.3762/bjnano.15.119},
volume = {15},
year = {2024}
}
@article{Pruyne1996,
abstract = {A continuing challenge to the scientific research and engineering communities is how to fully utilize computational hardware. In particular, the proliferation of clusters of high performance workstations has become an increasingly attractive source of compute power. Developments to take advantage of this environment have previously focused primarily on managing the resources, or on providing interfaces so that a number of machines can be used in parallel to solve large problems. Both approaches are desirable, and indeed should be complementary. Unfortunately, the resource management and parallel processing systems are usually developed by independent groups, and they usually do not interact well together. To bridge this gap, we have developed a framework for interfacing these two sorts of systems. Using this framework, we have interfaced PVM, a popular system for parallel programming with Condor, a powerful resource management system. This combined system is operational, and we have made further developments to provide a single coherent environment.},
author = {Pruyne, Jim and Livny, Miron},
doi = {10.1016/0167-739X(95)00036-R},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Distributed processing,PVM,Parallel programming,Resource management},
month = {may},
number = {1},
pages = {67--85},
title = {{Interfacing Condor and PVM to harness the cycles of workstation clusters}},
url = {https://www.sciencedirect.com/science/article/pii/0167739X9500036R https://linkinghub.elsevier.com/retrieve/pii/0167739X9500036R},
volume = {12},
year = {1996}
}
@article{Kepner2000,
abstract = {The advent of Massively Parallel Network of Workstations (MP-NOW) represents an important trend in high performance computing. The rise of interpreted languages (e.g., Visual Basic, MATLAB, IDL, Maple and Mathematica) for algorithm development, prototyping, data analysis and graphical user interfaces (GUIs) represents an important trend in software engineering. However, using interpreted languages on a MP-NOW is a significant challenge. We present a specific example of a very simple, but generic solution to this problem. Our example uses an interpreted language to set up a calculation and then interfaces with a computational kernel written in a compiled language (e.g., C, C++, Fortran). The interpreted language calls the computational kernel as an external library. We have added to the computational kernel an additional layer, which manages multiple copies of the kernel running on a MP-NOW and returns the results back to the interpreted layer. Our implementation uses The Next generation Taskbag (TNT) library developed at Sarnoff to provide an efficient means for implementing task parallelism. A test problem (taken from Astronomy) has been implemented on the Sarnoff Cyclone computer which consists of 160 heterogeneous nodes connected by a fat tree 100 Mb/s switched Ethernet running the RedHat Linux and FreeBSD operating systems. Our first results in this ongoing project have demonstrated the feasibility of this approach and produced speedups of greater than 50 on 60 processors.},
author = {Kepner, Jeremy and Gokhale, Maya and Minnich, Ron and Marks, Aaron and DeGood, John},
doi = {10.1023/A:1019011716367},
issn = {1386-7857},
journal = {Cluster Computing},
keywords = {Computer Science},
number = {1},
pages = {35--44},
title = {{Interfacing interpreted and compiled languages to support applications on a massively parallel network of workstations (MP-NOW)}},
url = {http://www.springerlink.com/content/v684h5m6370k424n/},
volume = {3},
year = {2000}
}
@inproceedings{10.1145/1362622.1362640,
abstract = {The grid vision of a single computing utility has yet to materialize: while many grids with thousands of processors each exist, most work in isolation. An important obstacle for the effective and efficient inter-operation of grids is the problem of resource selection. In this paper we propose a solution to this problem that combines the hierarchical and decentralized approaches for interconnecting grids. In our solution, a hierarchy of grid sites is augmented with peer-to-peer connections between sites under the same administrative control. To operate this architecture, we employ the key concept of delegated matchmaking, which temporarily binds resources from remote sites to the local environment. With trace-based simulations we evaluate our solution under various infrastructural and load conditions, and we show that it outperforms other approaches to inter-operating grids. Specifically, we show that delegated matchmaking achieves up to 60% more goodput and completes 26% more jobs than its best alternative. (c) 2007 ACM.},
address = {New York, NY, USA},
author = {Losup, Alexandru and Epema, Dick H.J. and Tannenbaum, Todd and Farrellee, Matthew and Livny, Miron},
booktitle = {Proceedings of the 2007 ACM/IEEE Conference on Supercomputing, SC'07},
doi = {10.1145/1362622.1362640},
isbn = {9781595937643},
keywords = {Computer architecture,Computer science education,Computer vision,Grid computing,Isolation technology,Laboratories,Load management,Permission,Production,Resource management},
publisher = {Association for Computing Machinery},
series = {SC '07},
title = {{Inter-operating grids through delegated matchmaking}},
url = {https://doi.org/10.1145/1362622.1362640},
year = {2007}
}
@inproceedings{Maheshwari2012,
abstract = {In this paper, we address the challenges of reducing the time-to-solution of the data intensive earthquake simulation workflow "CyberShake" by supplementing the high-performance parallel computing (HPC) resources on which it typically runs with distributed, heterogeneous resources that can be obtained opportunistically from grids and clouds. We seek to minimize time to solution by maximizing the amount of work that can be efficiently done on the distributed resources. We identify data movement as the main bottleneck in effectively utilizing the combined local and distributed resources. We address this by analyzing the I/O characteristics of the application, processor acquisition rate (from a pilot-job service), and the data movement throughput of the infrastructure. With these factors in mind, we explore a combination of strategies including partitioning of computation (over HPC and distributed resources) and job clustering. We validate our approach with a theoretical study and with preliminary measurements on the Ranger HPC system and distributed Open Science Grid resources. More complete performance results will be presented in the final submission of this paper. Copyright 2012 ACM.},
address = {New York, NY, USA},
author = {Maheshwari, Ketan and Espinosa, Allan and Katz, Daniel S. and Wilde, Michael and Zhang, Zhao and Foster, Ian and Callaghan, Scott and Maechling, Phillip},
booktitle = {DIDC'12 - 5th International Workshop on Data-Intensive Distributed Computing},
doi = {10.1145/2286996.2287000},
isbn = {9781450313414},
keywords = {Hpc,Parallel,Scec,Swift},
pages = {3--11},
publisher = {Association for Computing Machinery},
series = {DIDC '12},
title = {{Job and data clustering for aggregate use of multiple production cyberinfrastructures}},
url = {https://doi.org/10.1145/2286996.2287000},
year = {2012}
}
@inproceedings{Milligan2018,
abstract = {The Minnesota Supercomputing Institute has implemented Jupyterhub and the Jupyter notebook server as a general-purpose point-of-entry to interactive high performance computing services. This mode of operation runs counter to traditional job-oriented HPC operations, but offers significant advantages for ease-of-use, data exploration, prototyping, and workflow development. From the user perspective, these features bring the computing cluster nearer to parity with emerging cloud computing options. On the other hand, retreating from fully-scheduled, job-based resource allocation poses challenges for resource availability and utilization efficiency, and can involve tools and technologies outside the typical core competencies of a supercomputing center's operations staff. MSI has attempted to mitigate these challenges by adopting Jupyter as a common technology platform for interactive services, capable of providing command-line, graphical, and workflow-oriented access to HPC resources while still integrating with job scheduling systems and using existing compute resources. This paper will describe the mechanisms that MSI has put in place, advantages for research and instructional uses, and lessons learned.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1807.09929},
author = {Milligan, Michael B.},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3219104.3219162},
eprint = {1807.09929},
isbn = {9781450364461},
keywords = {High performance computing,Interfaces,Jupyter,Management},
publisher = {Association for Computing Machinery},
series = {PEARC '18},
title = {{Jupyter as common technology platform for interactive HPC services}},
url = {https://doi.org/10.1145/3219104.3219162},
year = {2018}
}
@article{Fernandez-Quiruelas2015,
abstract = {The current availability of a variety of computing infrastructures including HPC, Grid and Cloud resources provides great computer power for many fields of science, but their common profit to accomplish large scientific experiments is still a challenge. In this work, we use the paradigm of climate modeling to present the key problems found by standard applications to be run in hybrid distributed computing infrastructures and propose a framework to allow a climate model to take advantage of these resources in a transparent and user-friendly way. Furthermore, an implementation of this framework, using the Weather Research and Forecasting system, is presented as a working example. In order to illustrate the usefulness of this framework, a realistic climate experiment leveraging Cluster, Grid and Cloud resources simultaneously has been performed. This test experiment saved more than 75% of the execution time, compared to local resources. The framework and tools introduced in this work can be easily ported to other models and are probably useful in other scientific areas employing data- and CPU-intensive applications.},
author = {Fern{\'{a}}ndez-Quiruelas, V and Blanco, C and Cofi{\~{n}}o, A.S. and Fern{\'{a}}ndez, J},
doi = {10.1016/j.future.2015.04.009},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud computing,Grid computing,HPC,Hybrid distributed computing infrastructures,Regional climate model,WRF},
month = {oct},
pages = {36--44},
title = {{Large-scale climate simulations harnessing clusters, grid and cloud infrastructures}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15001016 https://linkinghub.elsevier.com/retrieve/pii/S0167739X15001016},
volume = {51},
year = {2015}
}
@article{Zhao2013,
abstract = {The solution of complex global challenges in the land system, such as food and energy security, requires information on the management of agricultural systems at a high spatial and temporal resolution over continental or global extents. However, computing capacity remains a barrier to large-scale, high-resolution agricultural modeling. To model wheat production, soil carbon, and nitrogen dynamics in Australia's cropping regions at a high resolution, we developed a hybrid computing approach combining parallel processing and grid computing. The hybrid approach distributes tasks across a heterogeneous grid computing pool and fully utilizes all the resources of computers within the pool. We simulated 325 management scenarios (nitrogen application rates and stubble management) at a daily time step over 122 years, for 12,707 climate-soil zones using the Windows-based Agricultural Production Systems SIMulator (APSIM). These simulations would have taken over 30 years on a single computer. Our hybrid high performance computing (HPC) approach completed the modeling within 10.5 days-a speed-up of over 1000 times-with most jobs finishing within the first few days. The approach utilizes existing idle organization-wide computing resources and eliminates the need to translate Windows-based models to other operating systems for implementation on computing clusters. There are however, numerous computing challenges that need to be addressed for the effective use of these techniques and there remain several potential areas for further performance improvement. The results demonstrate the effectiveness of the approach in making high-resolution modeling of agricultural systems possible over continental and global scales. {\textcopyright} 2012 Elsevier Ltd.},
author = {Zhao, Gang and Bryan, Brett A. and King, Darran and Luo, Zhongkui and Wang, Enli and Bende-Michl, Ulrike and Song, Xiaodong and Yu, Qiang},
doi = {10.1016/j.envsoft.2012.08.007},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {APSIM,Agricultural modeling,Crop modeling,Food security,Grid computing,High-performance computing,Parallel programming},
pages = {231--238},
title = {{Large-scale, high-resolution agricultural systems modeling using a hybrid approach combining grid computing and parallel processing}},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212002277},
volume = {41},
year = {2013}
}
@inproceedings{Zhang2015a,
abstract = {High throughput computing (HTC) systems are widely adopted in scientific discovery and engineering research. They are responsible for scheduling submitted batch jobs to utilize the cluster resources. Current systems mostly focus on managing computing resources like CPU and memory, however, they lack flexible and fine-grained management mechanisms for network resources. This has increasingly been an urgent need as current batch systems may be distributed among dozens of sites around the globe like Open Science Grid. The Lark project was motivated by this need to re-examine how the HTC layer interacts with the network layer. In this paper, we present the system architecture of Lark and its implementation as a plugin of HTCondor which is a popular HTC software project. Lark achieves lightweight network virtualization at per-job granularity for HTCondor by utilizing Linux container and virtual Ethernet devices, this provides each batch job with a unique network address in a private network namespace. We extended HTCondor's description language, Class Ads, so users can specify networking requirements in the job submission script. HTCondor can perform matchmaking to make sure user-specified network requirements and resource-specific policies are fulfilled. We also extended the job agent, condor starter, so that it can manage and configure the job's network environment. Given this important building block as the core, we implement bandwidth management functionality at both the host and network levels utilizing software-defined networking (SDN). Our experiments and evaluations show that Lark can effectively manage network resources within the cluster with low overhead. It provides the users with better predictability of their job execution and the administrators more flexibility in network resource consumption policies.},
author = {Zhang, Zhe and Bockelman, Brian and Carder, Dale W. and Tannenbaum, Todd},
booktitle = {Proceedings - 2015 IEEE/ACM 15th International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2015},
doi = {10.1109/CCGrid.2015.116},
isbn = {9781479980062},
keywords = {Bandwidth management,HTCondor,High throughput computing,Network-aware scheduling,Software-defined networking},
pages = {382--391},
publisher = {IEEE Press},
series = {CCGRID '15},
title = {{Lark: Bringing network awareness to high throughput computing}},
url = {https://doi.org/10.1109/CCGrid.2015.116},
year = {2015}
}
@article{Raicu2010,
abstract = {Many-task computing aims to bridge the gap between two computing paradigms, high throughput computing and high performance computing. Many-task computing denotes high-performance computations comprising multiple distinct activities, coupled via file system operations. The aggregate number of tasks, quantity of computing, and volumes of data may be extremely large. Traditional techniques found in production systems in the scientific community to support many-task computing do not scale to today's largest systems, due to issues in local resource manager scalability and granularity, efficient utilization of the raw hardware, long wait queue times, and shared/parallel file system contention and scalability. To address these limitations, we adopted a "top-down" approach to building a middleware called Falkon, to support the most demanding many-task computing applications at the largest scales. Falkon (Fast and Light-weight tasK executiON framework) integrates (1) multi-level scheduling to enable dynamic resource provisioning and minimize wait queue times, (2) a streamlined task dispatcher able to achieve orders-of-magnitude higher task dispatch rates than conventional schedulers, and (3) data diffusion which performs data caching and uses a data-aware scheduler to co-locate computational and storage resources. Micro-benchmarks have shown Falkon to achieve over 15K+ tasks/s throughputs, scale to hundreds of thousands of processors and to millions of queued tasks, and execute billions of tasks per day. Data diffusion has also shown to improve applications scalability and performance, with its ability to achieve hundreds of Gb/s I/O rates on modest sized clusters, with Tb/s I/O rates on the horizon. Falkon has shown orders of magnitude improvements in performance and scalability than traditional approaches to resource management across many diverse workloads and applications at scales of billions of tasks on hundreds of thousands of processors across clusters, specialized systems, Grids, and supercomputers. Falkon's performance and scalability have enabled a new class of applications called Many-Task Computing to operate at previously so-believed impossible scales with high efficiency. {\textcopyright} 2010 Springer Science+Business Media, LLC.},
author = {Raicu, Ioan and Foster, Ian and Wilde, Mike and Zhang, Zhao and Iskra, Kamil and Beckman, Peter and Zhao, Yong and Szalay, Alex and Choudhary, Alok and Little, Philip and Moretti, Christopher and Chaudhary, Amitabh and Thain, Douglas},
doi = {10.1007/s10586-010-0132-9},
issn = {13867857},
journal = {Cluster Computing},
keywords = {Computing,Data-intensive distributed computing,Falkon,High-performance computing,High-throughput,Loosely-coupled applications,Many-task computing,Petascale,Swift},
number = {3},
pages = {291--314},
title = {{Middleware support for many-task computing}},
url = {https://doi.org/10.1007/s10586-010-0132-9},
volume = {13},
year = {2010}
}
@inproceedings{10.1145/1646468.1646478,
abstract = {The availability of large quantities of processors is a crucial enabler of many-task computing. Voluntary computing systems have proven that it is possible to build computing platforms with millions of nodes to support the execution of embarrassingly parallel applications. These systems, however, lack the flexibility of more traditional grid infrastructures. On the other hand, flexible infrastructures currently available can gather only dozens of thousands nodes. We propose a novel architecture for generic Distributed Computing Infrastructures (DCI) that can be instantiated on demand to be, at the same time, flexible and highly-scalable. Bringing the scalability from voluntary computing, the flexibility from grid computing and the elasticity from cloud computing in a single arrangement, our proposal allows for fast setup, fast initialization and fast dismantle of customized DCI supported by both dedicated and shared underlying infrastructures. Our approach leverages broadcast communication as an efficient mechanism to enable aggregation of geographically distributed computing resources, including millions of non-traditional processing devices such as PDA, mobile phones and Digital TV receivers, using both opportunistic and non-opportunistic models. We show the feasibility of the proposed architecture by implementing it atop a digital television system. We also assess the performance of such system and show that it can be used to execute several classes of many-tasks computing applications with very high efficiency, substantially decreasing their response time. Copyright {\textcopyright} 2009 ACM.},
address = {New York, NY, USA},
author = {Costa, Rostand and Brasileiro, Francisco and Filho, Guido Lemos and Sousa, D{\^{e}}nio Mariz},
booktitle = {Proceedings of the 2nd ACM Workshop on Many-Task Computing on Grids and Supercomputers 2009, MTAGS '09},
doi = {10.1145/1646468.1646478},
isbn = {9781605587141},
keywords = {Broadcast,Cloud computing,Digital TV,Distributed computing infrastructure,Grid computing,High-throughput computing,Many tasks computing,On-demand instantiation},
publisher = {Association for Computing Machinery},
series = {MTAGS '09},
title = {{OddCI: On-demand distributed computing infrastructure}},
url = {https://doi.org/10.1145/1646468.1646478},
year = {2009}
}
@article{Lopez2017,
abstract = {Computer clusters are today a cost-effective way of providing either high-performance and/or high-availability. The flexibility of their configuration aims to fit the needs of multiple environments, from small servers to SME and large Internet servers. For these reasons, their usage has expanded not only in academia but also in many companies. However, each environment needs a different “cluster flavour”. High-performance and high-throughput computing are required in universities and research centres while high-performance service and high-availability are usually reserved to use in companies. Despite this fact, most university cluster computing courses continue to cover only high-performance computing, usually ignoring other possibilities. In this paper, a master-level course which attempts to fill this gap is discussed. It explores the different types of cluster computing as well as their functional basis, from a very practical point of view. As part of the teaching methodology, each student builds from scratch a computer cluster based on a virtualization tool. The entire process is designed to be scalable. The goal is to be able to apply it to an actual computer cluster with a larger number of nodes, such as those the students may subsequently encounter in their professional life.},
author = {L{\'{o}}pez, Pedro and Baydal, Elvira},
doi = {10.1016/j.jpdc.2017.01.009},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Computer cluster configuration and administration,Computer engineering education,Lab project},
pages = {127--137},
title = {{On a course on computer cluster configuration and administration}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517300151},
volume = {105},
year = {2017}
}
@article{Freyermuth2021a,
abstract = {High performance and high throughput computing (HPC/HTC) is challenged by ever increasing demands on the software stacks and more and more diverging requirements by different research communities. This led to a reassessment of the operational concept of HPC/HTC clusters at the Physikalisches Institut at the University of Bonn. As a result, the present HPC/HTC cluster (named BAF2) introduced various conceptual changes compared to conventional clusters. All jobs are now run in containers and a container-aware resource management system is used which allowed us to switch to a model without login/head nodes. Furthermore, a modern, feature-rich storage system with powerful interfaces has been deployed. We describe the design considerations, the implemented functionality and the operational experience gained with this new-generation setup which turned out to be very successful and well-accepted by its users.},
annote = {Tags: HPC, HTC, Condor, Research, Containerization
CVI: 4

Primera iteracion},
author = {Freyermuth, Oliver and Wienemann, Peter and Bechtle, Philip and Desch, Klaus},
doi = {10.1007/s41781-020-00050-y},
file = {::;::},
issn = {25102044},
journal = {Computing and Software for Big Science},
keywords = {Batch processing,Containers,Distributed file systems,Host management,Scientific computing},
month = {dec},
number = {1},
publisher = {Springer Nature},
title = {{Operating an HPC/HTC Cluster with Fully Containerized Jobs Using HTCondor, Singularity, CephFS and CVMFS}},
volume = {5},
year = {2021}
}
@article{Zhou2023,
abstract = {In the era of big data, materials science workflows need to handle large-scale data distribution, storage, and computation. Any of these areas can become a performance bottleneck. We present a framework for analyzing internal material structures (e.g., cracks) to mitigate these bottlenecks. We demonstrate the effectiveness of our framework for a workflow performing synchrotron X-ray computed tomography reconstruction and segmentation of a silica-based structure. Our framework provides a cloud-based, cutting-edge solution to challenges such as growing intermediate and output data and heavy resource demands during image reconstruction and segmentation. Specifically, our framework efficiently manages data storage, scaling up compute resources on the cloud. The multi-layer software structure of our framework includes three layers. A top layer uses Jupyter notebooks and serves as the user interface. A middle layer uses Ansible for resource deployment and managing the execution environment. A low layer is dedicated to resource management and provides resource management and job scheduling on heterogeneous nodes (i.e., GPU and CPU). At the core of this layer, Kubernetes supports resource management, and Dask enables large-scale job scheduling for heterogeneous resources. The broader impact of our work is four-fold: through our framework, we hide the complexity of the cloud's software stack to the user who otherwise is required to have expertise in cloud technologies; we manage job scheduling efficiently and in a scalable manner; we enable resource elasticity and workflow orchestration at a large scale; and we facilitate moving the study of nonporous structures, which has wide applications in engineering and scientific fields, to the cloud. While we demonstrate the capability of our framework for a specific materials science application, it can be adapted for other applications and domains because of its modular, multi-layer architecture.},
annote = {Tags: Heterogenenous Resources, Research, Containerization, Virtualization, Kubernetes
CVI: 3

Segunda iteracion},
author = {Zhou, Naweiluo and Scorzelli, Giorgio and Luettgau, Jakob and Kancharla, Rahul R. and Kane, Joshua J. and Wheeler, Robert and Croom, Brendan P. and Newell, Pania and Pascucci, Valerio and Taufer, Michela},
doi = {10.1177/10943420231167800},
issn = {17412846},
journal = {International Journal of High Performance Computing Applications},
keywords = {Scientific workflow,cloud computing,job scheduling,materials science,orchestration,resource management},
month = {apr},
number = {3-4},
pages = {260--271},
publisher = {SAGE Publications Ltd STM},
title = {{Orchestration of materials science workflows for heterogeneous resources at large scale}},
url = {https://doi.org/10.1177/10943420231167800},
volume = {37},
year = {2023}
}
@inproceedings{Babuji2019,
abstract = {High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250 000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1905.02158},
author = {Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle},
booktitle = {HPDC 2019- Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
doi = {10.1145/3307681.3325400},
eprint = {1905.02158},
isbn = {9781450366700},
keywords = {Parallel programming,Parsl,Python},
pages = {25--36},
publisher = {Association for Computing Machinery},
series = {HPDC '19},
title = {{Parsl: Pervasive parallel programming in Python}},
url = {https://doi.org/10.1145/3307681.3325400},
year = {2019}
}
@inproceedings{Chard2015,
abstract = {Accessing and analyzing data from cosmological simulations is a major challenge due to the prohibitive size of cosmological datasets and the diversity of the associated large-scale analysis tasks. Analysis of the simulated models requires direct access to the datasets, considerable compute infrastructure, and storage capacity for the results. Resource limitations can become serious obstacles to performing research on the most advanced cosmological simulations. The Portal for Data Analysis services for Cosmological Simulations (PDACS) is a web-based workflow service and scientific gateway for cosmology. The PDACS platform provides access to shared repositories for datasets, analytical tools, cosmological workflows, and the infrastructure required to perform a wide variety of analyses. PDACS is a repurposed implementation of the Galaxy workflow engine and supports a rich collection of cosmology-specific datatypes and tools. The platform leverages high-performance computing infrastructure at the National Energy Research Scientific Computing Center (NERSC) and Argonne National Laboratory (ANL), enabling researchers to deploy computationally intensive workflows. In this paper we present PDACS and discuss the process and challenges of developing a research platform for cosmological research.},
author = {Chard, Ryan and Sehrish, Saba and Rodriguez, Alex and Madduri, Ravi and Uram, Thomas D. and Paterno, Marc and Heitmann, Katrin and Cholia, Shreyas and Kowalkowski, Jim and Habib, Salman},
booktitle = {Proceedings of GCE 2014: 9th Gateway Computing Environments Workshop, held in conjunction with SC 2014: The International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1109/GCE.2014.7},
isbn = {9781479970308},
keywords = {Analytical models,Communities,Computational modeling,Data models,Genomics,Handheld computers,Logic gates},
pages = {30--33},
publisher = {IEEE Press},
series = {GCE '14},
title = {{PDACS-A portal for data analysis services for cosmological simulations}},
url = {https://doi.org/10.1109/GCE.2014.7},
year = {2015}
}
@article{Liu2022,
abstract = {Containerization technology offers an appealing alternative for encapsulating and operating applications (and all their dependencies) without being constrained by the performance penalties of using Virtual Machines and, as a result, has got the interest of the High-Performance Computing (HPC) community to obtain fast, customized, portable, flexible, and reproducible deployments of their workloads. Previous work on this area has demonstrated that containerized HPC applications can exploit InfiniBand networks, but has ignored the potential of multi-container deployments which partition the processes that belong to each application into multiple containers in each host. Partitioning HPC applications has demonstrated to be useful when using virtual machines by constraining them to a single NUMA (Non-Uniform Memory Access) domain. This paper conducts a systematical study on the performance of multi-container deployments with different network fabrics and protocols, focusing especially on Infiniband networks. We analyze the impact of container granularity and its potential to exploit processor and memory affinity to improve applications' performance. Our results show that default Singularity can achieve near bare-metal performance but does not support fine-grain multi-container deployments. Docker and Singularity-instance have similar behavior in terms of the performance of deployment schemes with different container granularity and affinity. This behavior differs for the several network fabrics and protocols, and depends as well on the application communication patterns and the message size. Moreover, deployments on Infiniband are also more impacted by the computation and memory allocation, and because of that, they can exploit the affinity better.},
annote = {Tags: HPC, Extension, Containerization, Virtualization
CVI: 3

Primera iteracion},
author = {Liu, Peini and Guitart, Jordi},
doi = {10.1007/s10586-021-03460-8},
issn = {15737543},
journal = {Cluster Computing},
keywords = {Containerization,Docker,InfiniBand,Multi-container,Performance,Singularity},
number = {2},
pages = {847--868},
title = {{Performance characterization of containerization for HPC workloads on InfiniBand clusters: an empirical study}},
url = {https://doi.org/10.1007/s10586-021-03460-8},
volume = {25},
year = {2022}
}
@misc{Decker2022,
abstract = {Serverless computing has grown massively in popularity over the last few years, and has provided developers with a way to deploy function-sized code units without having to take care of the actual servers or deal with logging, monitoring, and scaling of their code. High-performance computing (HPC) clusters can profit from improved serverless resource sharing capabilities compared to reservation-based systems such as Slurm. However, before running self-hosted serverless platforms in HPC becomes a viable option, serverless platforms must be able to deliver a decent level of performance. Other researchers have already pointed out that there is a distinct lack of studies in the area of comparative benchmarks on serverless platforms, especially for open-source self-hosted platforms. This study takes a step towards filling this gap by systematically benchmark-ing two promising self-hosted Kubernetes-based serverless platforms in comparison. While the resulting benchmarks signal potential, they demonstrate that many opportunities for performance improvements in serverless computing are being left on the table.},
annote = {Tags: HPC, Research, Containerization, Kubernetes
CVI: 3

Segunda iteracion},
author = {Decker, Jonathan and Kasprzak, Piotr and Kunkel, Julian Martin},
booktitle = {Algorithms},
doi = {10.3390/a15070234},
isbn = {1999-4893},
issn = {19994893},
keywords = {HPC,Kubernetes,benchmark,open source,performance,self-hosted,serverless},
number = {7},
title = {{Performance Evaluation of Open-Source Serverless Platforms for Kubernetes}},
volume = {15},
year = {2022}
}
@article{Walker2004,
abstract = {We describe a system for creating personal clusters in user-space to support the submission and management of thousands of compute-intensive serial jobs to the network-connected compute resources on the NSF Tera- Grid. The system implements a robust infrastructure that submits and manages job proxies across a distributed computing environment. These job proxies contribute resources to personal clusters created dynamically for a user ondemand. The personal clusters then adapt to the prevailing job load conditions at the distributed sites by migrating job proxies to sites expected to provide resources more quickly. Furthermore, the system allows multiple instances of these personal clusters to be created as containers for individual scientific experiments, allowing the submission environment to be customized for each instance. The version of the system described in this paper allows users to build large personal Condor and Sun Grid Engine clusters on the TeraGrid. Users then manage their scientific jobs, within each personal cluster, with a single uniform interface using the feature-rich functionality found in these job management environments. {\textcopyright} Springer Science+Business Media, LLC 2007.},
author = {Walker, Edward and Gardner, Jeffrey P. and Litvin, Vladimir and Turner, Evan L.},
doi = {10.1007/s10586-007-0028-5},
issn = {15731936},
journal = {Tertiary Education and Management},
keywords = {Cooperative systems,Distributed computing,Resource management},
number = {3},
pages = {339--350},
title = {{Personal adaptive clusters as containers for scientific jobs}},
url = {https://doi.org/10.1007/s10586-007-0028-5},
volume = {10},
year = {2004}
}
@inproceedings{10.1145/2287076.2287105,
abstract = {High Throughput Computing (HTC) platforms aggregate heterogeneous resources to provide vast amounts of computing power over a long period of time. Typical HTC systems, such as Condor and BOINC, rely on central managers for resource discovery and scheduling. While this approach simplifies deployment, it requires careful system configuration and management to ensure high availability and scalability. In this paper, we present a novel approach that integrates a self-organizing P2P overlay for scalable and timely discovery of resources with unmodified client/server job scheduling middleware in order to create HTC virtual resource Pools on Demand (PonD). This approach decouples resource discovery and scheduling from job execution/monitoring - a job submission dynamically generates an HTC platform based upon resources discovered through match-making from a large "sea" of resources in the P2P overlay and forms a "PonD" capable of leveraging unmodified HTC middleware for job execution and monitoring. We show that job scheduling time of our approach scales with O(log N), where N is the number of resources in a pool, through first-order analytical models and large-scale simulation results. To verify the practicality of PonD, we have implemented a prototype using Condor (called C-PonD), a structured P2P overlay, and a PonD creation module. Experimental results with the prototype in two WAN environments (PlanetLab and the FutureGrid cloud computing testbed) demonstrates the utility of C-PonD as a HTC approach without relying on a central repository for maintaining all resource information. Though the prototype is based on Condor, the decoupled nature of the system components - decentralized resource discovery, PonD creation, job execution/monitoring - is generally applicable to other grid computing middleware systems. Copyright {\textcopyright} 2012 ACM.},
address = {New York, NY, USA},
author = {Lee, Kyungyong and Wolinsky, David and Figueiredo, Renato},
booktitle = {HPDC '12 - Proceedings of the 21st ACM Symposium on High-Performance Parallel and Distributed Computing},
doi = {10.1145/2287076.2287105},
isbn = {9781450308052},
keywords = {High-throughput computing,P2P,Resource discovery,Self-configuration,Virtual resources},
pages = {161--172},
publisher = {Association for Computing Machinery},
series = {HPDC '12},
title = {{PonD: Dynamic creation of HTC pool on demand using a decentralized resource discovery system}},
url = {https://doi.org/10.1145/2287076.2287105},
year = {2012}
}
@inproceedings{Herzfeld2010,
abstract = {A campus grid is a critical component of research cyberin- frastructure. A grid facilitates resource sharing, improving collaboration and increasing the capability to address large scale scientific and engineering problems. Resources comprising a grid include research clusters, centrally managed clusters, opportunistic use of desktop computers, campus clouds, and commercial clouds. Advances in virtual machine technology and hypervisor availability has made the use of virtual machines an attractive tool for building campus grids. Pools of Virtual Boxes (POVB) is an open-source, dedicated virtual machine environment for rapidly deploying a campus grid. POVB is targeted at institutions where financial and administrative constraints prevent large scale changes in computational infrastructure. The POVB distribution includes: services that manage the virtual machine hypervisor, services that communicate select host information with the Linux guest for debugging and resource utilization policies, a bootstrapping framework for building virtual images, and a third-party package deployment framework that integrates with Condor to advertise available software services. We report on the design and implementation of POVB and examine a deployment of several hundred POVB instances, which has been operational for over a year. POVB has been released under the GNU Public License Version 3, and is available at http:// poolsofvirtualb.sourceforge.net. Copyright 2010 ACM.},
address = {New York, NY, USA},
author = {Herzfeld, David J. and Olson, Lars E. and Struble, Craig A.},
booktitle = {HPDC 2010 - Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
doi = {10.1145/1851476.1851575},
isbn = {9781605589428},
keywords = {Campus grid,Deployment,High throughput Computing,Virtual machines},
pages = {667--675},
publisher = {Association for Computing Machinery},
series = {HPDC '10},
title = {{Pools of virtual boxes: Building campus grids with virtual machines}},
url = {https://doi.org/10.1145/1851476.1851575},
year = {2010}
}
@inproceedings{10.1145/1731740.1731843,
abstract = {The paper presents the authors' research on the possibilities of Grid technologies for realization of compute intensive problems. An experimental Grid environment, in which a Monte Carlo simulation for evaluating the market risk of a financial position is proposed. Comparison data, obtained by performing the simulation in an environment, controlled by a traditional high throughput computing system are discussed. {\textcopyright} 2009 ACM.},
address = {New York, NY, USA},
author = {Penev, Ivaylo and Antonov, Anatoliy},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1731740.1731843},
isbn = {9781605589862},
keywords = {Distributed computing system,Grid computing,High throughput computing,Monte Carlo simulation},
publisher = {Association for Computing Machinery},
series = {CompSysTech '09},
title = {{Possibilities of grid computing for realization of simulation problems}},
url = {https://doi.org/10.1145/1731740.1731843},
volume = {433},
year = {2009}
}
@article{Bockelman2021,
abstract = {Mechanisms for remote execution of computational tasks enable a distributed system to effectively utilize all available resources. This ability is essential to attaining the objectives of high availability, system reliability, and graceful degradation and directly contribute to flexibility, adaptability, and incremental growth. As part of a national fabric of Distributed High Throughput Computing (dHTC) services, remote execution is a cornerstone of the Open Science Grid (OSG) Compute Federation. Most of the organizations that harness the computing capacity provided by the OSG also deploy HTCondor pools on resources acquired from the OSG. The HTCondor Compute Entrypoint (CE) facilitates the remote acquisition of resources by all organizations. The HTCondor-CE is the product of a most recent translational cycle that is part of a multidecade translational process. The process is rooted in a partnership, between members of the High Energy Physics community and computer scientists, that evolved over three decades and involved testing and evaluation with active users and production infrastructures. Through several translational cycles that involved researchers from different organizations and continents, principles, ideas, frameworks and technologies were translated into a widely adopted software artifact that isresponsible for provisioning of approximately 9 million core hours per day across 170 endpoints.},
annote = {Reviewed},
author = {Bockelman, Brian and Livny, Miron and Lin, Brian and Prelz, Francesco},
doi = {10.1016/j.jocs.2020.101213},
file = {::},
issn = {18777503},
journal = {Journal of Computational Science},
keywords = {Distributed computing,Distributed high throughput computing,High throughput computing,Translational computing},
pages = {101213},
title = {{Principles, technologies, and time: The translational journey of the HTCondor-CE}},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320305147},
volume = {52},
year = {2021}
}
@inproceedings{10.1145/1551609.1551634,
abstract = {Advances in the development of large scale distributed computing systems such as Grids and Computing Clouds have intensified the need for developing scheduling algorithms capable of allocating multiple resources simultaneously. In principle, the required resources may be allocated by sequentially scheduling each resource individually. However, such a solution can be computationally expensive, hence inappropriate for time-sensitive applications, and may lead to deadlocks. In this work we present an efficient online algorithm for co-allocating resources that also provides support for advance reservations. The algorithm utilizes data structures specifically designed to organize the temporal availability of resources, and implements co-allocation through efficient range searches that identify all available resources simultaneously. We use simulations driven by real workloads to show that the co-allocation algorithm scales to systems with large numbers of users and resources, and we perform an in-depth comparative analysis against existing batch scheduling mechanisms. Our findings indicate that the online scheduling algorithms may achieve higher utilization while providing smaller delays and better QoS guarantees without adding much complexity. Copyright 2009 ACM.},
address = {New York, NY, USA},
author = {Castillo, Claris and Rouskas, George N. and Harfoush, Khaled},
booktitle = {Proc. 18th ACM International Symposium on High Performance Distributed Computing, HPDC 09, Co-located with the 2009 International Symposium on High Performance Distributed Computing Conf., HPDC'09},
doi = {10.1145/1551609.1551634},
isbn = {9781605585871},
keywords = {Advance reservation,Resource co-allocation,Scheduling,resource co-allocation,scheduling},
pages = {131--140},
publisher = {Association for Computing Machinery},
series = {HPDC '09},
title = {{Resource co-allocation for large-scale distributed environments}},
url = {https://doi.org/10.1145/1551609.1551634},
year = {2009}
}
@inproceedings{10.1145/1646468.1646469,
abstract = {Scientific workflow tools allow users to specify complex computational experiments and provide a good framework for robust science and engineering. Workflows consist of pipelines of tasks that can be used to explore the behaviour of some system, involving computations that are either performed locally or on remote computers. Robust scientific methods require the exploration of the parameter space of a system (some of which can be run in parallel on distributed resources), and may involve complete state space exploration, experimental design or numerical optimization techniques. Whilst workflow engines provide an overall framework, they have not been developed with these concepts in mind, and in general, don't provide the necessary components to implement robust workflows. In this paper we discuss Nimrod/K - a set of add in components and a new run time machine for a general workflow engine, Kepler. Nimrod/K provides an execution architecture based on the tagged dataflow concepts developed in 1980's for highly parallel machines. This is embodied in a new Kepler 'Director' that orchestrates the execution on clusters, Grids and Clouds using many-task computing. Nimrod/K also provides a set of 'Actors' that facilitate the various modes of parameter exploration discussed above. We demonstrate the power of Nimrod/K to solve real problems in cardiac science. Copyright 2009 ACM.},
address = {New York, NY, USA},
author = {Abramson, David and Bethwaite, Blair and Enticott, Colin and Garic, Slavisa and Peachey, Tom and Michailova, Anushka and Amirriazi, Saleh and Chitters, Ramya},
booktitle = {Proceedings of the 2nd ACM Workshop on Many-Task Computing on Grids and Supercomputers 2009, MTAGS '09},
doi = {10.1145/1646468.1646469},
isbn = {9781605587141},
publisher = {Association for Computing Machinery},
series = {MTAGS '09},
title = {{Robust workflows for science and engineering}},
url = {https://doi.org/10.1145/1646468.1646469},
year = {2009}
}
@inproceedings{Mukherjee2012,
abstract = {The challenges facing biomolecular simulations are many-fold. In addition to long time simulations of a single large system, an important challenge is the ability to run a large number of identical copies (ensembles) of the same system. Ensemble-based simulations are important for effective sampling. Due to the low-level of coupling between them, ensemble-based simulations are good candidates to utilize distributed cyberinfrastructure. The problem for the practitioner is thus effectively marshaling thousands if not millions of high-performance simulations on distributed cyberinfrastructure. Here we assess the ability of an interoperable and extensible pilot-job tool (BigJob), to support high-throughput simulations of high-performance molecular dynamics simulations across distributed supercomputing infrastructure. BigJob provides the capability to run hundreds or thousands of MPI ensembles concurrently. This is advantageous on large machines because it reduces the number of submissions to the queue, thereby reducing the overall waiting time in the queue. The wait time problem is further complicated by scheduling policies on some large XSEDE machines that prioritize large job requests over very small or single core job requests. Using a nucleosome positioning problem as an exemplar, we demonstrate how we have addressed this challenge on the TeraGrid/XSEDE. Specifically, we compute 336 independent trajectories of 20 ns each. Each trajectory is divided into twenty 1 ns long simulation tasks. A single task requires ≈ 42 MB of input, 9 hours of compute time on 32 cores, and generates 3.8 GB of data. In total we have 6,720 tasks (6.7 $\mu$s) and approximately 25 TB to manage. There is natural task-level concurrency, as these 6,720 tasks can be executed with 336-way task concurrency. This project requires approximately 2 million hours of CPU time and could be completed in just over 1 month on a dedicated supercomputer containing 3,000 cores. In practice, even such a modest supercomputer is a shared resource and our experience suggests that a simple scheme to automatically batch queue the tasks, might require several years to complete the project. In order to reduce the total time-to-completion, we need to scale-up, out and across various resources. Our approach is to aggregate many ensemble members into pilot-jobs, distribute pilot-jobs over multiple compute resources concurrently, and dynamically assign tasks across the available resources. Here we report the computational methodology employed in our study and refrain from analyzing the biological aspects of the simulations. {\textcopyright} 2012 ACM.},
address = {New York, NY, USA},
author = {Mukherjee, Rajib and Thota, Abhinav and Fujioka, Hideki and Bishop, Thomas C. and Jha, Shantenu},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2335755.2335787},
isbn = {9781450316026},
keywords = {HPC,MD,NAMD,XSEDE resources,distributed computing,experience,large scale,technology},
publisher = {Association for Computing Machinery},
series = {XSEDE '12},
title = {{Running many molecular dynamics simulations on many supercomputers}},
url = {https://doi.org/10.1145/2335755.2335787},
year = {2012}
}
@inproceedings{Callaghan2017,
abstract = {Computational science researchers running large-scale scientific workflow applications often want to run their workflows on the largest available compute systems to improve time to solution. Workflow tools used in distributed, heterogeneous, high performance computing environments typically rely on either a pushbased or a pull-based approach for resource provisioning from these compute systems. However, many large clusters have moved to two-factor authentication for job submission, making traditional automated push-based job submission impossible. On the other hand, pull-based approaches such as pilot jobs may lead to increased complexity and a reduction in node-hour efficiency. In this paper, we describe a new, efficient approach based on HTCondor-G called reverse GAHP (rvGAHP) that allows us to push jobs using reverse SSH submissions with better efficiency than pull-based methods. We successfully used this approach to perform a large probabilistic seismic hazard analysis study using SCEC's CyberShake workflow in March 2017 on the Titan Cray XK7 hybrid system at Oak Ridge National Laboratory.},
address = {New York, NY, USA},
author = {Callaghan, Scott and Juve, Gideon and Vahi, Karan and Maechling, Philip J. and Jordan, Thomas H. and Deelman, Ewa},
booktitle = {Proceedings of WORKS 2017: 12th Workshop on Workflows in Support of Large-Scale Science - Held in conjunction with SC 2017: The International Conference for High Performance Computing, Networking, Storage and Analysis},
doi = {10.1145/3150994.3151003},
isbn = {9781450351294},
keywords = {Remote job submission,Resource provisioning,Scientific workflows,Seismic hazard analysis},
publisher = {Association for Computing Machinery},
series = {WORKS '17},
title = {{RvGAHP - Push-based job submission using reverse SSH connections}},
url = {https://doi.org/10.1145/3150994.3151003},
year = {2017}
}
@article{Zhang2007,
abstract = {Monitoring and information system (MIS) implementations provide data about available resources and services within a distributed system, or Grid. A comprehensive performance evaluation of an MIS can aid in detecting potential bottlenecks, advise in deployment, and help improve future system development. In this paper, we analyze and compare the performance of three implementations in a quantitative manner: the Globus Toolkit{\textregistered} Monitoring and Discovery Service (MDS2), the European DataGrid Relational Grid Monitoring Architecture (R-GMA), and the Condor project's Hawkeye. We use the NetLogger toolkit to instrument the main service components of each MIS and conduct four sets of experiments to benchmark their scalability with respect to the number of users, the number of resources, and the amount of data collected. Our study provides quantitative measurements comparable across all systems. We also find performance bottlenecks and identify how they relate to the design goals, underlying architectures, and implementation technologies of the corresponding MIS, and we present guidelines for deploying MISs in practice.},
author = {Zhang, Xuehai and Freschl, Jeffrey L and Schopf, Jennifer M},
doi = {10.1016/j.jpdc.2007.03.006},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Distributed systems,Grid performance study,Monitoring and information systems,Performance analysis},
month = {aug},
number = {8},
pages = {883--902},
title = {{Scalability analysis of three monitoring and information systems: MDS2, R-GMA, and Hawkeye}},
url = {https://www.sciencedirect.com/science/article/pii/S0743731507000408 https://linkinghub.elsevier.com/retrieve/pii/S0743731507000408},
volume = {67},
year = {2007}
}
@article{10.1145/3363554,
abstract = {Deep Learning (DL) has had an immense success in the recent past, leading to state-of-the-art results in various domains, such as image recognition and natural language processing. One of the reasons for this success is the increasing size of DL models and the proliferation of vast amounts of training data being available. To keep on improving the performance of DL, increasing the scalability of DL systems is necessary. In this survey, we perform a broad and thorough investigation on challenges, techniques and tools for scalable DL on distributed infrastructures. This incorporates infrastructures for DL, methods for parallel DL training, multi-tenant resource scheduling, and the management of training and model data. Further, we analyze and compare 11 current open-source DL frameworks and tools and investigate which of the techniques are commonly implemented in practice. Finally, we highlight future research trends in DL systems that deserve further research.},
address = {New York, NY, USA},
annote = {Tags: Deep Learning, Research, Parallel
CVI: 3

Segunda iteracion},
archivePrefix = {arXiv},
arxivId = {1903.11314},
author = {Mayer, Ruben and Jacobsen, Hans Arno},
doi = {10.1145/3363554},
eprint = {1903.11314},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Deep-learning systems},
number = {1},
publisher = {Association for Computing Machinery},
title = {{Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools}},
url = {https://doi.org/10.1145/3363554},
volume = {53},
year = {2020}
}
@inproceedings{Alrajeh2018,
abstract = {Virtual Machine (VM) live migration is one strategic approach that can be employed to reduce energy consumption and increase the utilisation of a single computer in large computing infrastructure. However, virtualisation in High Throughput Computing (HTC) has received limited attention in the literature. In this paper, we present an extension of an existing trace-driven simulation to incorporate virtualisation. Furthermore, we implement the pre-copy live migration algorithm to provide a test environment for job live migration in HTC system. Our simulation provides the total number of migrations and their overall time of migrations as well as calculates the energy consumption of migrations during its runtime. In this paper, we propose two methods to perform the live migration in the HTC system. We demonstrate that our responsive migration could save up to 75% of the system wasted energy.},
annote = {No se encuentran las palabras clave},
author = {Alrajeh, Osama and Forshaw, Matthew and {Stephen McGough}, Andrew and Thomas, Nigel},
booktitle = {Proceedings of the 2018 IEEE/ACM 22nd International Symposium on Distributed Simulation and Real Time Applications, DS-RT 2018},
doi = {10.1109/DISTRA.2018.8601013},
isbn = {9781538650486},
keywords = {Cloud computing,Computational modeling,Energy consumption,History,Task analysis,Throughput,Tools},
pages = {47--54},
publisher = {IEEE Press},
series = {DS-RT '18},
title = {{Simulation of Virtual Machine Live Migration in High Throughput Computing Environments}},
year = {2018}
}
@inproceedings{10.5555/1516744.1516935,
abstract = {An increased need for collaborative research, together with continuing advances in communication technology and computer hardware, has facilitated the development of distributed systems that can provide users access to geographically dispersed computing resources that are administered in multiple computer domains. The term grid computing, or grids, is popularly used to refer to such distributed systems. Simulation is characterized by the need to run multiple sets of computationally intensive experiments. Large scale scientific simulations have traditionally been the primary benefactor of grid computing. The application of this technology to simulation in industry has, however, been negligible. This research investigates how grid technology can be effectively exploited by users to model simulations in industry. It introduces our desktop grid, WinGrid, and presents a case study conducted at a leading European investment bank. Results indicate that grid computing does indeed hold promise for simulation in industry. {\textcopyright}2008 IEEE.},
annote = {No se encuentran las palabras clave},
author = {Mustafee, Navonil and Taylor, Simon J.E.},
booktitle = {Proceedings - Winter Simulation Conference},
doi = {10.1109/WSC.2008.4736176},
isbn = {9781424427086},
issn = {08917736},
keywords = {Application software,Collaboration,Communications technology,Computational modeling,Computer industry,Distributed computing,Grid computing,Hardware,Investments,Large-scale systems},
pages = {1077--1085},
publisher = {Winter Simulation Conference},
series = {WSC '08},
title = {{Supporting simulation in industry through the application of grid computing}},
year = {2008}
}
@inproceedings{Sly-Delgado2023,
abstract = {Many scientific applications are expressed as high-throughput workflows that consist of large graphs of data assets and tasks to be executed on large parallel and distributed systems. A challenge in executing these workflows is managing data: both datasets and software must be efficiently distributed to cluster nodes; intermediate data must be conveyed between tasks; output data must be delivered to its destination. Scaling problems result when these actions are performed in an uncoordinated manner on a shared filesystem. To address this problem, we introduce TaskVine: a system for exploiting the aggregate local storage and network capacity of a large cluster. TaskVine tracks the lifetime of data in a workflow -from archival sources to final outputs- making use of local storage to distribute, and re-use data wherever possible. We describe the architecture and novel capabilities of TaskVine, and demonstrate its use with applications in genomics, high energy physics, molecular dynamics, and machine learning.},
address = {New York, NY, USA},
annote = {No se encuentran las palabras clave},
author = {Sly-Delgado, Barry and Phung, Thanh Son and Thomas, Colin and Simonetti, David and Hennessee, Andrew and Tovar, Ben and Thain, Douglas},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3624062.3624277},
isbn = {9798400707858},
pages = {1978--1988},
publisher = {Association for Computing Machinery},
series = {SC-W '23},
title = {{TaskVine: Managing In-Cluster Storage for High-Throughput Data Intensive Workflows}},
url = {https://doi.org/10.1145/3624062.3624277},
year = {2023}
}
@inproceedings{10.1145/1383422.1383435,
abstract = {Ever more scientists are employing large-scale distributed systems such as grids for their computational work, instead of tightly coupled high-performance computing systems. However, while these distributed systems are more cost-effective, their heterogeneity in terms of hardware, software, and systems administration, and the lack of accurate resource information leads to inefficient scheduling. In addition, and in contrast to the workloads of tightly coupled high-performance computing systems, a large part of the workloads submitted to these distributed systems consists of large sets (bags) of sequential tasks. Therefore, a realistic performance analysis of scheduling bags-of-tasks in large-scale distributed systems is important. Towards this end, we introduce in this paper a realistic workload model for bags-of-tasks, and we explore through trace-based simulations the design space of scheduling bags-of-tasks. Finally, we identify three new scheduling policies that use only inaccurate information when scheduling, and we compare them against known classes of proposed scheduling policies. Copyright 2008 ACM.},
address = {New York, NY, USA},
author = {Losup, Alexandru and Sonmez, Ozan and Anoep, Shanny and Epema, Dick},
booktitle = {Proceedings of the 17th International Symposium on High Performance Distributed Computing 2008, HPDC'08},
doi = {10.1145/1383422.1383435},
isbn = {9781595939975},
keywords = {Bags-of-tasks,Batches of jobs,Large-scale distributed systems,Multi-cluster systems,Performance evaluation,Prediction,Scheduling,Trace-based simulation,Workload modeling,batches of jobs,large-scale distributed systems,multi-cluster systems,performance evaluation,prediction,scheduling,trace-based simulation,workload modeling},
pages = {97--108},
publisher = {Association for Computing Machinery},
series = {HPDC '08},
title = {{The performance of bags-of-tasks in large-scale distributed systems}},
url = {https://doi.org/10.1145/1383422.1383435},
year = {2008}
}
@article{Hey2002,
abstract = {This paper describes the {\pounds}120M UK ‘e-Science' (http://www.research-councils.ac.uk/ and http://www.escience-grid.org.uk) initiative and begins by defining what is meant by the term e-Science. The majority of the {\pounds}120M, some {\pounds}75M, is funding large-scale e-Science pilot projects in many areas of science and engineering. The infrastructure needed to support such projects must permit routine sharing of distributed and heterogeneous computational and data resources as well as supporting effective collaboration between groups of scientists. Such an infrastructure is commonly referred to as the Grid. Apart from {\pounds}10M towards a Teraflop computer, the remaining funds, some {\pounds}35M, constitute the e-Science ‘Core Programme'. The goal of this Core Programme is to advance the development of robust and generic Grid middleware in collaboration with industry. The key elements of the Core Programme will be outlined including details of a UK e-Science Grid testbed. The pilot e-Science projects that have so far been announced are then briefly described. These projects span a range of disciplines from particle physics and astronomy to engineering and healthcare, and illustrate the breadth of the UK e-Science Programme. In addition to these major e-Science projects, the Core Programme is funding a series of short-term e-Science demonstrators across a number of disciplines as well as projects in network traffic engineering and some international collaborative activities. We conclude with some remarks about the need to develop a data architecture for the Grid that will allow federated access to relational databases as well as flat files.},
author = {Hey, Tony and Trefethen, Anne E},
doi = {10.1016/S0167-739X(02)00082-1},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Core Programme,Grid,e-Science},
month = {oct},
number = {8},
pages = {1017--1031},
title = {{The UK e-Science Core Programme and the Grid}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X02000821 https://linkinghub.elsevier.com/retrieve/pii/S0167739X02000821},
volume = {18},
year = {2002}
}
@inproceedings{Liu2021,
abstract = {As many scientific computation tasks focus on solving large-scale and computationally intensive problems, a wide range of problems involving High-Throughput Computing (HTC) paradigms and data-oriented algorithms emerge. To solve these problems, we review and recommend some simple workflows for users to bundle their serial or parallel jobs in this paper, including the native way with the Linux scheduler, the numactl method for processes or shared memory management, and the job array feature of job scheduler. We also introduce several convenient job bundling tools that TACC develops, such as ibrun, Launcher, Pylauncher, and Launcher-GPU. Some basic practice guidelines are added for users to choose appropriate workflows and tools when job bundling is required.},
address = {New York, NY, USA},
annote = {No se encuentran las palabras clave},
author = {Liu, Si and Eijkhout, Victor and Cazes, John},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3437359.3465569},
isbn = {9781450382922},
publisher = {Association for Computing Machinery},
series = {PEARC '21},
title = {{Tools and Guidelines for Job Bundling on Modern Supercomputers}},
url = {https://doi.org/10.1145/3437359.3465569},
year = {2021}
}
@inproceedings{10.5555/1413370.1413393,
abstract = {We have extended the Falkon lightweight task execution framework to make loosely coupled programming on petascale systems a practical and useful programming model. This work studies and measures the performance factors involved in applying this approach to enable the use of petascale systems by a broader user community, and with greater ease. Our work enables the execution of highly parallel computations composed of loosely coupled serial jobs with no modifications to the respective applications. This approach allows a new-and potentially far larger-class of applications to leverage petascale systems, such as the IBM Blue Gene/P supercomputer. We present the challenges of I/O performance encountered in making this model practical, and show results using both microbenchmarks and real applications from two domains: economic energy modeling and molecular dynamics. Our benchmarks show that we can scale up to 160K processor-cores with high efficiency, and can achieve sustained execution rates of thousands of tasks per second. {\textcopyright} 2008 IEEE.},
author = {Raicu, Ioan and Zhang, Zhao and Wilde, Mike and Foster, Ian and Beckman, Pete and Iskra, Kamil and Clifford, Ben},
booktitle = {2008 SC - International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2008},
doi = {10.1109/SC.2008.5219768},
isbn = {9781424428359},
keywords = {Blue gene,Falkon,High throughput computing,Loosely coupled applications,Many task computing,Petascale,Swift},
publisher = {IEEE Press},
series = {SC '08},
title = {{Toward loosely coupled programming petascale systems}},
year = {2008}
}
@inproceedings{Maassen2011,
abstract = {The scientific computing landscape is becoming more and more complex. Besides traditional supercomputers and clusters, scientists can also apply grid and cloud infrastructures. Moreover, the current integration of many-core technologies such as GPUs with such infrastructures adds to the complexity. To make matters worse, data distribution, hardware availability, software heterogeneity, and increasing data sizes, commonly force scientists to use multiple computing platforms simultaneously: a true computing jungle. In this paper we introduce Ibis/Constellation, a software platform specifically designed for distributed, heterogeneous and hierarchical computing environments. In Ibis/Constellation we assume that applications consist of several distinct (but somehow related) activities. These activities can be implemented independently using existing, well understood tools (e.g. MPI, CUDA, etc.). Ibis/Constellation is then used to construct the overall application by coupling the distinct activities. Using application defined labels in combination with context-aware work stealing, Ibis/Constellation provides a simple and efficient mechanism for automatically mapping the activities to the appropriate resources, taking data locality and heterogeneity into account. We show that an existing supernova detection application can be ported to Ibis/Constellation with little effort. By making small changes to the application defined labels, this example application can run efficiently in three very different HPC computing environments: a distributed set of clusters, a large 48-core machine, and a GPU cluster. {\textcopyright} Copyright 2011 ACM.},
address = {New York, NY, USA},
author = {Maassen, Jason and Drost, Niels and Bal, Henri E. and Seinstra, Frank J.},
booktitle = {3DAPAS'11 - Proceedings of the Workshop on Dynamic Distributed Data-Intensive Applications, Programming Abstractions and Systems},
doi = {10.1145/1996010.1996013},
isbn = {9781450307055},
keywords = {Design,Experimentation,Performance},
pages = {7--18},
publisher = {Association for Computing Machinery},
series = {3DAPAS '11},
title = {{Towards jungle computing with Ibis/Constellation}},
url = {https://doi.org/10.1145/1996010.1996013},
year = {2011}
}
@inproceedings{Zhang2010,
abstract = {Cluster computing, Cloud computing and GPU computing play overlapping and complementary roles in parallel processing of geospatial data within the general HPC framework. The fast increasing hardware capacities of modern personal computers equipped with chip multiprocessor CPUs and massively parallel GPUs have made high performance computing of large-scale geospatial data in a personal computing environment possible. We discuss the framework of Personal HPC-G and compare it with traditional Cluster computing and the newly emerging Cloud computing. We consider Personal HPC-G possesses many favorable features: low initial and operational costs, good support for data management and excellent support for both numeric modeling and interactive visualization. A case study on developing a parallel spatial statistics module for visual explorations on top of Personal HPC-G is subsequently presented. Copyright 2010 ACM.},
address = {New York, NY, USA},
annote = {No se encuentran las palabras clave},
author = {Zhang, Jianting},
booktitle = {Proceedings of the ACM SIGSPATIAL International Workshop on High Performance and Distributed Geographic Information Systems, ACM SIGSPATIAL HPDGIS 2010},
doi = {10.1145/1869692.1869694},
isbn = {9781450304320},
pages = {3--10},
publisher = {Association for Computing Machinery},
series = {HPDGIS '10},
title = {{Towards personal high-performance geospatial computing (HPC-G): Perspectives and a case study}},
url = {https://doi.org/10.1145/1869692.1869694},
year = {2010}
}
@inproceedings{10.5555/3571885.3571916,
abstract = {Today's supercomputers offer massive computation resources to execute a large number of user jobs. Effectively managing such large-scale hardware parallelism and workloads is essential for supercomputers. However, existing HPC resource management (RM) systems fail to capitalize on the hardware parallelism by following a centralized design used decades ago. They give poor scalability and inefficient performance on today's supercomputers, which will worsen in exascale computing. We present ESlurm, a better RM for supercomputers. As a departure from existing HPC RMs, ESlurm implements a distributed communication structure. It employs a new communication tree strategy and uses job runtime estimation to improve communications and job scheduling efficiency. ESlurm is deployed into production in a real supercomputer. We evaluate ESlurm on up to 20K nodes. Compared to state-of-the-art RM solutions, ESlurm exhibits better scalability, significantly reducing the resource usage of master nodes and improving data transfer and job scheduling efficiency by a large margin.},
author = {Dai, Yiqin and Dong, Yong and Lu, Kai and Wang, Ruibo and Zhang, Wei and Chen, Juan and Shao, Mingtian and Wang, Zheng},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
doi = {10.1109/SC41404.2022.00029},
isbn = {9781665454445},
issn = {21674337},
keywords = {Exascale computing,Resource management,Scheduling},
publisher = {IEEE Press},
series = {SC '22},
title = {{Towards Scalable Resource Management for Supercomputers}},
volume = {2022-Novem},
year = {2022}
}
@article{Bittencourt2010,
abstract = {The workflow paradigm has become the standard to represent processes and their execution flows. With the evolution of e-Science, workflows are becoming larger and more computational demanding. Such e-Science necessities match with what computational Grids have to offer. Grids are shared distributed platforms which will eventually receive multiple requisitions to execute workflows. With this, there is a demand for a scheduler which deals with multiple workflows in the same set of resources, thus the development of multiple workflow scheduling algorithms is necessary. In this paper we describe four different initial strategies for scheduling multiple workflows on Grids and evaluate them in terms of schedule length and fairness. We present results for the initial schedule and for the makespan after the execution with external load. From the results we conclude that interleaving the workflows on the Grid leads to good average makespan and provides fairness when multiple workflows share the same set of resources. {\textcopyright} Springer Science+Business Media B.V. 2009.},
author = {Bittencourt, Luiz Fernando and Madeira, Edmundo R.M.},
doi = {10.1007/s10723-009-9144-1},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Grid computing,Scheduling,Workflow},
number = {3},
pages = {419--441},
title = {{Towards the scheduling of multiple workflows on computational Grids}},
url = {https://doi.org/10.1007/s10723-009-9144-1},
volume = {8},
year = {2010}
}
@inproceedings{Meng2015,
abstract = {Environment configuration is a significant challenge in large scale computing. An application that runs correctly on one carefullyprepared machine may fail completely on another machine, creating wasted effort and serious concerns about long-term reproducibility. Virtual machines and system containers provide a partial solution to this problem, in that they allow for the accurate reconstruction of an entire computing environment. However, when used directly, they have the dual problems of significant overhead and a lack of portability. To avoid this problem, we present Umbrella, a tool for specifying and materializing comprehensive execution environments from the hardware all the way up to software and data. A user simply invokes Umbrella with the desired task, and Umbrella determines the minimum mechanism necessary to run the task - direct execution, a system container, a local virtual machine, or submission to a cloud or grid environment. We present the overall design of Umbrella and demonstrate its use to precisely execute a high energy physics application across many platforms using combinations of chroot, Docker, Parrot, Condor, and Amazon EC2.},
address = {New York, NY, USA},
author = {Meng, Haiyan and Thain, Douglas},
booktitle = {VTDC 2015 - Proceedings of the 8th International Workshop on Virtualization Technologies in Distributed Computing, Part of HPDC 2015},
doi = {10.1145/2755979.2755982},
isbn = {9781450335737},
keywords = {Containers,Execution environment,Reproducible computing,Virtualization},
pages = {23--30},
publisher = {Association for Computing Machinery},
series = {VTDC '15},
title = {{Umbrella: A portable environment creator for reproducible computing on clusters, clouds, and grids}},
url = {https://doi.org/10.1145/2755979.2755982},
year = {2015}
}
@article{Freyermuth2021,
abstract = {In recent years Jupyter notebooks have conquered class rooms and some scientists also enjoy their convenience to quickly evaluate ideas and check whether a more detailed study is justified. To lower the threshold for getting started with Jupyter notebooks and to ease sharing and collaborative use, offering a JupyterHub service is tempting. However, offering such a service for a larger science class also requires a compute backend with sufficient resources such that hundreds of notebooks can be run simultaneously. Since resource usage for teaching activities typically fluctuates significantly over the year, dedicated compute resources seem inefficient. In this paper we present an alternative by exploiting an existing high throughput computing cluster (BAF2) at the University of Bonn, which comes with the additional advantage that scientific users may use the very same software and data environment they also select for their batch jobs. To implement this, we used a novel approach which allowed us to integrate BAF2 execute nodes although they do not have inbound network connectivity. Therefore, it does not touch the security concept of the cluster. The very same technique can be used to integrate any compute resources without inbound network connectivity and thus allows one to overcome usual firewall restrictions. This design also simplifies exploiting remote resources e.g. offered by resource federations or cloud providers.},
annote = {Tags: Condor, Vanilla Universe, Teaching, Jupyter
CVI: 3

Segunda iteracion},
author = {Freyermuth, Oliver and Kohl, Katrin and Wienemann, Peter},
doi = {10.1007/s41781-021-00063-1},
issn = {25102044},
journal = {Computing and Software for Big Science},
keywords = {Batch systems,Interactive services,JupyterLab},
number = {1},
pages = {24},
title = {{Unleashing JupyterHub: Exploiting Resources Without Inbound Network Connectivity Using HTCondor}},
url = {https://doi.org/10.1007/s41781-021-00063-1},
volume = {5},
year = {2021}
}
@inproceedings{10.1145/3053600.3053612,
abstract = {When performing a trace-driven simulation of a High Through- put Computing system we are limited to the knowledge which should be available to the system at the current point within the simulation. However, the trace-log contains information we would not be privy to during the simulation. Through the use of Machine Learning we can extract the latent patterns within the trace-log allowing us to accurately predict characteristics of tasks based only on the information we would know. These characteristics will allow us to make better decisions within simulations allowing us to derive better policies for saving energy. We demonstrate that we can accurately predict (up-To 99% accuracy), using oversampling and deep learning, those tasks which will complete while at the same time provide accurate predictions for the task execution time and memory footprint using Random Forest Regression.},
address = {New York, NY, USA},
author = {McGgough, A. Stephen and Moubayed, Noura Al and Forshaw, Matthew},
booktitle = {ICPE 2017 - Companion of the 2017 ACM/SPEC International Conference on Performance Engineering},
doi = {10.1145/3053600.3053612},
isbn = {9781450348997},
keywords = {Machine learning,Simulation,Trace-driven,machine learning,simulation},
pages = {55--60},
publisher = {Association for Computing Machinery},
series = {ICPE '17 Companion},
title = {{Using machine learning in trace-driven energy-Aware simulations of high-Throughput computing systems}},
url = {https://doi.org/10.1145/3053600.3053612},
year = {2017}
}
@inproceedings{Shieh2014,
abstract = {RNA aptamers are small oligonucleotide molecules whose composition and resulting folded structure enable them to bind with high affinity and high selectivity to target ligands and therefore hold great promise as potential therapeutic drugs. Functional aptamers are selected from a large, randomized initial library in a process known as SELEX (systematic evolution of ligands by exponential enrichment). This is an iterative process involving numerous rounds of binding, elution, and amplification against a specific target substrate. During each iteration-or round of selection-we enrich for the species with the highest binding affinity to the target. After multiple rounds, we ideally have an enriched aptamer library suitable for subsequent investigation. Modern techniques employ massively parallel sequencing, enabling the generation of large libraries ($\sim$106 sequences) in a matter of hours for each round of selection. As RNA is singlestranded, covariance models (CMs) are ideal for representing motifs in their secondary structures, allowing us to discover patterns within functional aptamer populations following each round. CMs have been implemented in Infernal, a program that infers RNA alignments based on RNA sequence and structure. Calibrating a single CM in Infernal can take several hours and is a significant performance bottleneck for our work. However, as each CM calculation is itself independently determined and requires defined processing and memory resources, their computation in parallel offers a potential solution to this problem. In this paper, we describe using the Open Science Grid (OSG) to facilitate the identification of aptamer motifs by running CM calibrations and refinements in parallel across up to ten OSG clients. We use the Simple API for Grid Applications (SAGA) to interface with OSG and manage job submissions and file transfers. When run in parallel, our results show a significant speed up, constrained by typical latencies and QoS associated with nominal OSG usage. Our work demonstrates the ability of SAGA and the OSG to assist in parallelizing solutions to complex sequencing-based biomedical challenges. Copyright 2014 ACM.},
address = {New York, NY, USA},
author = {Shieh, Kevin and Broin, Pilib {\'{O}} and Rhee, David and Levy, Matthew and Golden, Aaron},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/2616498.2616517},
isbn = {9781450328937},
keywords = {Aptamers,HTCondor,High-throughput computing,Open science grid,Parallel computing,SAGA},
publisher = {Association for Computing Machinery},
series = {XSEDE '14},
title = {{Using SAGA and the open science grid to search for aptamers}},
url = {https://doi.org/10.1145/2616498.2616517},
year = {2014}
}
@article{Alam2023,
abstract = {Supercomputers have been driving innovations for performance and scaling benefiting several scientific applications for the past few decades. Yet their ecosystems remain virtually unchanged when it comes to integrating distributed data-driven workflows, primarily due to rather rigid access methods and restricted configuration management options. X-as-a-Service model of cloud has introduced, among other features, a developer-centric DevOps approach empowering developers of infrastructure, platform to software artefacts, which, unfortunately contemporary supercomputers still lack. We introduce vClusters (versatile software-defined clusters), which is based on Infrastructure-as-code (IaC) technology. vClusters approach is a unique fusion of HPC and cloud technologies resulting in a software-defined, multi-tenant cluster on a supercomputing ecosystem, that, together with software-defined storage, enable DevOps for complex, data-driven workflows like grid middleware, alongside a classic HPC platform. IaC has been a commonplace in cloud computing, however, it lacked adoption within multi-Petascale ecosystems due to concerns related to performance and interoperability with classic HPC data centres' ecosystems. We present an overview of the Swiss National Supercomputing Centre's flagship Alps ecosystem as an implementation target for vClusters for HPC and data-driven workflows. Alps is based on the Cray-HPE Shasta EX supercomputing platform that includes an IaC compliant, microservices architecture (MSA) management system, which we leverage for demonstrating vClusters usage for our diverse operational workflows. We provide implementation details of two operational vClusters platforms: a classic HPC platform that is used predominantly by hundreds of users running thousands of large-scale numerical simulations batch jobs; and a widely used, data-intensive, Grid computing middleware platform used for CERN Worldwide LHC Computing Grid (WLCG) operations. The resulting solution showcases reuse and reduction of common configuration recipes across vCluster implementations, minimising operational change management overheads while introducing flexibility for managing artefacts for DevOps required by diverse workflows.},
annote = {Tags: HPC, Research, Grid Computing
CVI: 3

Segunda iteracion},
author = {Alam, Sadaf R. and Gila, Miguel and Klein, Mark and Martinasso, Maxime and Schulthess, Thomas C.},
doi = {10.1177/10943420231167811},
issn = {17412846},
journal = {International Journal of High Performance Computing Applications},
keywords = {HPC,cloud,co-design,devOps,grid middleware,infrastructure-as-code},
month = {apr},
number = {3-4},
pages = {288--305},
publisher = {SAGE Publications Ltd STM},
title = {{Versatile software-defined HPC and cloud clusters on Alps supercomputer for diverse workflows}},
url = {https://doi.org/10.1177/10943420231167811},
volume = {37},
year = {2023}
}
@inproceedings{Benton2011,
abstract = {Job schedulers for grids and clouds can offer great generality and configurability, but they typically do so at the cost of increased administrator complexity. In this paper, we present Wallaby, an open-source, scalable configuration service for compute resources managed by the Condor high-throughput computing system. Wallaby offers several notable advantages over similar systems: it lets administrators write declarative specifications of user-visible functionality on groups of nodes instead of low-level configuration file fragments; it presents a high-level semantic model of Condor features and their interactions and dependencies; it validates configurations before pushing them to nodes; it supports version control, "undo," and configuration differencing; and it includes a networked API that enables extensions and advanced functionality. Wallaby allows administrators to extend pools to include more physical, virtual, or cloud nodes with minimal explicit configuration. Finally, it is scalable, supporting pools consisting of thousands of nodes with hundreds of configuration parameters each. Copyright 2011 ACM.},
address = {New York, NY, USA},
annote = {No se encuentran las palabras clave},
author = {Benton, William C. and Rati, Robert H. and Erlandson, Erik J.},
booktitle = {State of the Practice Reports, SC'11},
doi = {10.1145/2063348.2063362},
isbn = {9781450311397},
keywords = {Computational modeling,Documentation,Logic gates,Manuals,Merging,Object oriented modeling,Semantics},
publisher = {Association for Computing Machinery},
series = {SC '11},
title = {{Wallaby: A scalable semantic configuration service for grids and clouds}},
url = {https://doi.org/10.1145/2063348.2063362},
year = {2011}
}
@article{Deelman2009,
abstract = {Scientific workflow systems have become a necessary tool for many applications, enabling the composition and execution of complex analysis on distributed resources. Today there are many workflow systems, often with overlapping functionality. A key issue for potential users of workflow systems is the need to be able to compare the capabilities of the various available tools. There can be confusion about system functionality and the tools are often selected without a proper functional analysis. In this paper we extract a taxonomy of features from the way scientists make use of existing workflow systems and we illustrate this feature set by providing some examples taken from existing workflow systems. The taxonomy provides end users with a mechanism by which they can assess the suitability of workflow in general and how they might use these features to make an informed choice about which workflow system would be a good choice for their particular application. Crown Copyright {\textcopyright} 2008.},
author = {Deelman, Ewa and Gannon, Dennis and Shields, Matthew and Taylor, Ian},
doi = {10.1016/j.future.2008.06.012},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Automation of scientific processes,Computation,Cyberinfrastructure,Distributed computing,Distributed systems,Grid computing,Scientific workflow,Web services,e-Science},
number = {5},
pages = {528--540},
title = {{Workflows and e-Science: An overview of workflow system features and capabilities}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X08000861},
volume = {25},
year = {2009}
}

% ---------------------------------------------------------------
% --------------------AQUI TERMINAN LOS SPSs---------------------
% ---------------------------------------------------------------
%
% ---------------------------------------------------------------
% ---------------------Link del contenedor --------------------
% ---------------------------------------------------------------
